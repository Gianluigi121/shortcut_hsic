[
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "call",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "shared.train_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "restrict_GPU_tf",
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "isExtraImport": true,
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "config_hasher",
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "isExtraImport": true,
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "tried_config_file",
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "isExtraImport": true,
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "configurator",
        "importPath": "chexpert_support_device",
        "description": "chexpert_support_device",
        "isExtraImport": true,
        "detail": "chexpert_support_device",
        "documentation": {}
    },
    {
        "label": "configurator",
        "importPath": "chexpert_support_device",
        "description": "chexpert_support_device",
        "isExtraImport": true,
        "detail": "chexpert_support_device",
        "documentation": {}
    },
    {
        "label": "weighting",
        "importPath": "chexpert_support_device",
        "description": "chexpert_support_device",
        "isExtraImport": true,
        "detail": "chexpert_support_device",
        "documentation": {}
    },
    {
        "label": "data_builder",
        "importPath": "chexpert_support_device",
        "description": "chexpert_support_device",
        "isExtraImport": true,
        "detail": "chexpert_support_device",
        "documentation": {}
    },
    {
        "label": "shared.cross_validation",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "keras",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "keras",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "shutil,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil.",
        "description": "shutil.",
        "detail": "shutil.",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "flags",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "train",
        "importPath": "shared",
        "description": "shared",
        "isExtraImport": true,
        "detail": "shared",
        "documentation": {}
    },
    {
        "label": "get_sigma",
        "importPath": "shared",
        "description": "shared",
        "isExtraImport": true,
        "detail": "shared",
        "documentation": {}
    },
    {
        "label": "evaluation",
        "importPath": "shared",
        "description": "shared",
        "isExtraImport": true,
        "detail": "shared",
        "documentation": {}
    },
    {
        "label": "architectures",
        "importPath": "shared",
        "description": "shared",
        "isExtraImport": true,
        "detail": "shared",
        "documentation": {}
    },
    {
        "label": "evaluation",
        "importPath": "shared",
        "description": "shared",
        "isExtraImport": true,
        "detail": "shared",
        "documentation": {}
    },
    {
        "label": "train_utils",
        "importPath": "shared",
        "description": "shared",
        "isExtraImport": true,
        "detail": "shared",
        "documentation": {}
    },
    {
        "label": "DenseNet121",
        "importPath": "tensorflow.keras.applications.densenet",
        "description": "tensorflow.keras.applications.densenet",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications.densenet",
        "documentation": {}
    },
    {
        "label": "tensorflow_probability",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow_probability",
        "description": "tensorflow_probability",
        "detail": "tensorflow_probability",
        "documentation": {}
    },
    {
        "label": "sample",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "stats",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "chexpert_support_device.data_builder",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "layers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "layers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "ResNet50",
        "importPath": "tensorflow.keras.applications.resnet50",
        "description": "tensorflow.keras.applications.resnet50",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications.resnet50",
        "documentation": {}
    },
    {
        "label": "ResNet50",
        "importPath": "tensorflow.keras.applications.resnet50",
        "description": "tensorflow.keras.applications.resnet50",
        "isExtraImport": true,
        "detail": "tensorflow.keras.applications.resnet50",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "configure_hsic_model",
        "kind": 2,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "def configure_hsic_model(skew_train, weighted, batch_size):\n\t\"\"\"Creates hyperparameters for correlations experiment for SLABS model.\n\tReturns:\n\t\tIterator with all hyperparameter combinations\n\t\"\"\"\n\tparam_dict = {\n\t\t'random_seed': [0],\n\t\t'pixel': [128],\n\t\t'l2_penalty': [0.0],\n\t\t'embedding_dim': [-1],",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "configure_baseline",
        "kind": 2,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "def configure_baseline(skew_train, weighted, batch_size):\n\t\"\"\"Creates hyperparameters for correlations experiment for SLABS model.\n\tReturns:\n\t\tIterator with all hyperparameter combinations\n\t\"\"\"\n\tparam_dict = {\n\t\t'random_seed': [0],\n\t\t'pixel': [128],\n\t\t'l2_penalty': [0.0, 0.0001, 0.001],\n\t\t'embedding_dim': [-1],",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "get_sweep",
        "kind": 2,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "def get_sweep(experiment, model, batch_size):\n\t\"\"\"Wrapper function, creates configurations based on experiment and model.\n\tArgs:\n\t\texperiment: string with experiment name\n\t\tmodel: string, which model to create the configs for\n\t\taug_prop: float, proportion of augmentation relative to training data.\n\t\t\tOnly relevant for augmentation based baselines\n\tReturns:\n\t\tIterator with all hyperparameter combinations\n\t\"\"\"",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "\tparam_dict",
        "kind": 5,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "\tparam_dict = {\n\t\t'random_seed': [0],\n\t\t'pixel': [128],\n\t\t'l2_penalty': [0.0],\n\t\t'embedding_dim': [-1],\n\t\t'sigma': [10.0, 100.0, 1000.0],\n\t\t'alpha': [100.0, 1000.0, 10000.0],\n\t\t\"architecture\": [\"pretrained_densenet\"],\n\t\t\"batch_size\": [batch_size],\n\t\t'weighted': [weighted],",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "\tparam_dict_ordered",
        "kind": 5,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "\tparam_dict_ordered = collections.OrderedDict(sorted(param_dict.items()))\n\tkeys, values = zip(*param_dict_ordered.items())\n\tsweep = [dict(zip(keys, v)) for v in itertools.product(*values)]\n\treturn sweep\ndef configure_baseline(skew_train, weighted, batch_size):\n\t\"\"\"Creates hyperparameters for correlations experiment for SLABS model.\n\tReturns:\n\t\tIterator with all hyperparameter combinations\n\t\"\"\"\n\tparam_dict = {",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "\tsweep",
        "kind": 5,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "\tsweep = [dict(zip(keys, v)) for v in itertools.product(*values)]\n\treturn sweep\ndef configure_baseline(skew_train, weighted, batch_size):\n\t\"\"\"Creates hyperparameters for correlations experiment for SLABS model.\n\tReturns:\n\t\tIterator with all hyperparameter combinations\n\t\"\"\"\n\tparam_dict = {\n\t\t'random_seed': [0],\n\t\t'pixel': [128],",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "\tparam_dict",
        "kind": 5,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "\tparam_dict = {\n\t\t'random_seed': [0],\n\t\t'pixel': [128],\n\t\t'l2_penalty': [0.0, 0.0001, 0.001],\n\t\t'embedding_dim': [-1],\n\t\t'sigma': [10.0],\n\t\t'alpha': [0.0],\n\t\t\"architecture\": [\"pretrained_densenet\"],\n\t\t\"batch_size\": [batch_size],\n\t\t'weighted': [weighted],",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "\tparam_dict_ordered",
        "kind": 5,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "\tparam_dict_ordered = collections.OrderedDict(sorted(param_dict.items()))\n\tkeys, values = zip(*param_dict_ordered.items())\n\tsweep = [dict(zip(keys, v)) for v in itertools.product(*values)]\n\treturn sweep\ndef get_sweep(experiment, model, batch_size):\n\t\"\"\"Wrapper function, creates configurations based on experiment and model.\n\tArgs:\n\t\texperiment: string with experiment name\n\t\tmodel: string, which model to create the configs for\n\t\taug_prop: float, proportion of augmentation relative to training data.",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "\tsweep",
        "kind": 5,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "\tsweep = [dict(zip(keys, v)) for v in itertools.product(*values)]\n\treturn sweep\ndef get_sweep(experiment, model, batch_size):\n\t\"\"\"Wrapper function, creates configurations based on experiment and model.\n\tArgs:\n\t\texperiment: string with experiment name\n\t\tmodel: string, which model to create the configs for\n\t\taug_prop: float, proportion of augmentation relative to training data.\n\t\t\tOnly relevant for augmentation based baselines\n\tReturns:",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "\timplemented_models",
        "kind": 5,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "\timplemented_models = open(\n\t\tf'{Path(__file__).resolve().parent}/implemented_models.txt',\n\t\t\"r\").read().split(\"\\n\")\n\timplemented_experiments = ['skew_train', 'unskew_train']\n\tif experiment not in implemented_experiments:\n\t\traise NotImplementedError((f'Experiment {experiment} parameter'\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t' configuration not implemented'))\n\tif model not in implemented_models:\n\t\traise NotImplementedError((f'Model {model} parameter configuration'\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t' not implemented'))",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "\timplemented_experiments",
        "kind": 5,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "\timplemented_experiments = ['skew_train', 'unskew_train']\n\tif experiment not in implemented_experiments:\n\t\traise NotImplementedError((f'Experiment {experiment} parameter'\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t' configuration not implemented'))\n\tif model not in implemented_models:\n\t\traise NotImplementedError((f'Model {model} parameter configuration'\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t' not implemented'))\n\tskew_train = 'True' if experiment == 'skew_train' else 'False'\n\tif model == 'unweighted_baseline':\n\t\treturn configure_baseline(skew_train=skew_train, weighted='False',",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "\tskew_train",
        "kind": 5,
        "importPath": "chexpert_support_device.configurator",
        "description": "chexpert_support_device.configurator",
        "peekOfCode": "\tskew_train = 'True' if experiment == 'skew_train' else 'False'\n\tif model == 'unweighted_baseline':\n\t\treturn configure_baseline(skew_train=skew_train, weighted='False',\n\t\t\tbatch_size=batch_size)\n\tif model == 'weighted_baseline':\n\t\treturn configure_baseline(skew_train=skew_train, weighted='True',\n\t\t\tbatch_size=batch_size)\n\tif model == 'unweighted_hsic':\n\t\treturn configure_hsic_model(skew_train=skew_train, weighted='False',\n\t\t\tbatch_size=batch_size)",
        "detail": "chexpert_support_device.configurator",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "def main(save_directory):\n\t\"\"\" Function that creates a csv with the cohort\"\"\"\n\t# -- import the data\n\ttrdf = pd.read_csv('/nfs/turbo/coe-rbg/CheXpert-v1.0/train.csv')\n\tvdf = pd.read_csv('/nfs/turbo/coe-rbg/CheXpert-v1.0/valid.csv')\n\tdf = trdf.append(vdf)\n\tdel trdf, vdf\n\t# -- keep only healthy, pneumonia and support device\n\tdf = df[(\n\t\t(df['No Finding'] == 1) | (df['Pneumonia'] == 1) | (df['Support Devices'] == 1)",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\ttrdf",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\ttrdf = pd.read_csv('/nfs/turbo/coe-rbg/CheXpert-v1.0/train.csv')\n\tvdf = pd.read_csv('/nfs/turbo/coe-rbg/CheXpert-v1.0/valid.csv')\n\tdf = trdf.append(vdf)\n\tdel trdf, vdf\n\t# -- keep only healthy, pneumonia and support device\n\tdf = df[(\n\t\t(df['No Finding'] == 1) | (df['Pneumonia'] == 1) | (df['Support Devices'] == 1)\n\t)]\n\t# print(df.Pneumonia.value_counts(dropna=False, normalize=True))\n\t# print(df['Support Devices'].value_counts(dropna=False))",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tvdf",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tvdf = pd.read_csv('/nfs/turbo/coe-rbg/CheXpert-v1.0/valid.csv')\n\tdf = trdf.append(vdf)\n\tdel trdf, vdf\n\t# -- keep only healthy, pneumonia and support device\n\tdf = df[(\n\t\t(df['No Finding'] == 1) | (df['Pneumonia'] == 1) | (df['Support Devices'] == 1)\n\t)]\n\t# print(df.Pneumonia.value_counts(dropna=False, normalize=True))\n\t# print(df['Support Devices'].value_counts(dropna=False))\n\t# -- clean up a bit",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf = trdf.append(vdf)\n\tdel trdf, vdf\n\t# -- keep only healthy, pneumonia and support device\n\tdf = df[(\n\t\t(df['No Finding'] == 1) | (df['Pneumonia'] == 1) | (df['Support Devices'] == 1)\n\t)]\n\t# print(df.Pneumonia.value_counts(dropna=False, normalize=True))\n\t# print(df['Support Devices'].value_counts(dropna=False))\n\t# -- clean up a bit\n\tdf['patient'] = df.Path.str.extract(r'(patient)(\\d+)')[1]",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf = df[(\n\t\t(df['No Finding'] == 1) | (df['Pneumonia'] == 1) | (df['Support Devices'] == 1)\n\t)]\n\t# print(df.Pneumonia.value_counts(dropna=False, normalize=True))\n\t# print(df['Support Devices'].value_counts(dropna=False))\n\t# -- clean up a bit\n\tdf['patient'] = df.Path.str.extract(r'(patient)(\\d+)')[1]\n\tdf['study'] = df.Path.str.extract(r'(study)(\\d+)')[1].astype(int)\n\tdf['uid'] = df['patient'] + \"_\" + df['study'].astype(str)\n\tdf = df[",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf['patient']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf['patient'] = df.Path.str.extract(r'(patient)(\\d+)')[1]\n\tdf['study'] = df.Path.str.extract(r'(study)(\\d+)')[1].astype(int)\n\tdf['uid'] = df['patient'] + \"_\" + df['study'].astype(str)\n\tdf = df[\n\t\t['uid', 'patient', 'study', 'Sex', 'Frontal/Lateral',\n\t\t'Pneumonia', 'Support Devices', 'Path']\n\t]\n\t# current_uid = len(df.uid.unique())\n\t# print(f'Total uids {current_uid}')\n\t# get the main outcome",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf['study']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf['study'] = df.Path.str.extract(r'(study)(\\d+)')[1].astype(int)\n\tdf['uid'] = df['patient'] + \"_\" + df['study'].astype(str)\n\tdf = df[\n\t\t['uid', 'patient', 'study', 'Sex', 'Frontal/Lateral',\n\t\t'Pneumonia', 'Support Devices', 'Path']\n\t]\n\t# current_uid = len(df.uid.unique())\n\t# print(f'Total uids {current_uid}')\n\t# get the main outcome\n\tdf['y0'] = df['Pneumonia'].copy()",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf['uid']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf['uid'] = df['patient'] + \"_\" + df['study'].astype(str)\n\tdf = df[\n\t\t['uid', 'patient', 'study', 'Sex', 'Frontal/Lateral',\n\t\t'Pneumonia', 'Support Devices', 'Path']\n\t]\n\t# current_uid = len(df.uid.unique())\n\t# print(f'Total uids {current_uid}')\n\t# get the main outcome\n\tdf['y0'] = df['Pneumonia'].copy()\n\tdf.y0.fillna(0, inplace=True)",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf = df[\n\t\t['uid', 'patient', 'study', 'Sex', 'Frontal/Lateral',\n\t\t'Pneumonia', 'Support Devices', 'Path']\n\t]\n\t# current_uid = len(df.uid.unique())\n\t# print(f'Total uids {current_uid}')\n\t# get the main outcome\n\tdf['y0'] = df['Pneumonia'].copy()\n\tdf.y0.fillna(0, inplace=True)\n\tdf.y0[(df.y0 == -1)] = 1",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf['y0']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf['y0'] = df['Pneumonia'].copy()\n\tdf.y0.fillna(0, inplace=True)\n\tdf.y0[(df.y0 == -1)] = 1\n\t# print(df.y0.value_counts(dropna=False, normalize=True))\n\t# get the first auxiliary label\n\tdf = df[(df.Sex != 'Unknown')]\n\tdf['y1'] = (df.Sex == 'Male').astype(int)\n\tdf.drop('Sex', axis=1, inplace=True)\n\t# print(f'Lost {100*(current_uid - len(df.uid.unique()))/current_uid:.3f}% because of unknown sex')\n\t# current_uid = len(df.uid.unique())",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf = df[(df.Sex != 'Unknown')]\n\tdf['y1'] = (df.Sex == 'Male').astype(int)\n\tdf.drop('Sex', axis=1, inplace=True)\n\t# print(f'Lost {100*(current_uid - len(df.uid.unique()))/current_uid:.3f}% because of unknown sex')\n\t# current_uid = len(df.uid.unique())\n\t# get the second auxiliary label\n\tdf['y2'] = df['Support Devices'].copy()\n\tdf.y2.fillna(0, inplace=True)\n\tdf.y2[(df.y2 == -1)] = 1\n\t# print(df.y2.value_counts(dropna = False, normalize = True))",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf['y1']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf['y1'] = (df.Sex == 'Male').astype(int)\n\tdf.drop('Sex', axis=1, inplace=True)\n\t# print(f'Lost {100*(current_uid - len(df.uid.unique()))/current_uid:.3f}% because of unknown sex')\n\t# current_uid = len(df.uid.unique())\n\t# get the second auxiliary label\n\tdf['y2'] = df['Support Devices'].copy()\n\tdf.y2.fillna(0, inplace=True)\n\tdf.y2[(df.y2 == -1)] = 1\n\t# print(df.y2.value_counts(dropna = False, normalize = True))\n\t# keep only studies with frontal views",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf['y2']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf['y2'] = df['Support Devices'].copy()\n\tdf.y2.fillna(0, inplace=True)\n\tdf.y2[(df.y2 == -1)] = 1\n\t# print(df.y2.value_counts(dropna = False, normalize = True))\n\t# keep only studies with frontal views\n\tdf['frontal'] = (df['Frontal/Lateral'] == 'Frontal').astype(int)\n\tdf = df[(df.frontal == 1)]\n\t# more clean ups\n\t# print(f'Lost {100*(current_uid - len(df.uid.unique()))/current_uid:.3f}% because they dont have frontal views')\n\t# current_uid = len(df.uid.unique())",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf['frontal']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf['frontal'] = (df['Frontal/Lateral'] == 'Frontal').astype(int)\n\tdf = df[(df.frontal == 1)]\n\t# more clean ups\n\t# print(f'Lost {100*(current_uid - len(df.uid.unique()))/current_uid:.3f}% because they dont have frontal views')\n\t# current_uid = len(df.uid.unique())\n\tdf.drop_duplicates(subset=['uid'], inplace=True)\n\t# print(f'Lost {100*(current_uid - df.shape[0])/current_uid:.3f}% because they have duplicates')\n\t# current_uid = len(df.uid.unique())\n\tdf.drop(\n\t\t['Frontal/Lateral', 'frontal', 'Pneumonia', 'Support Devices'],",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tdf",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tdf = df[(df.frontal == 1)]\n\t# more clean ups\n\t# print(f'Lost {100*(current_uid - len(df.uid.unique()))/current_uid:.3f}% because they dont have frontal views')\n\t# current_uid = len(df.uid.unique())\n\tdf.drop_duplicates(subset=['uid'], inplace=True)\n\t# print(f'Lost {100*(current_uid - df.shape[0])/current_uid:.3f}% because they have duplicates')\n\t# current_uid = len(df.uid.unique())\n\tdf.drop(\n\t\t['Frontal/Lateral', 'frontal', 'Pneumonia', 'Support Devices'],\n\t\taxis=1, inplace=True)",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\tparser",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--save_directory', '-save_directory',\n\t\thelp=\"Directory where the final cohort will be saved\",\n\t\ttype=str)\n\targs = vars(parser.parse_args())\n\tmain(**args)",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "\targs",
        "kind": 5,
        "importPath": "chexpert_support_device.create_data",
        "description": "chexpert_support_device.create_data",
        "peekOfCode": "\targs = vars(parser.parse_args())\n\tmain(**args)",
        "detail": "chexpert_support_device.create_data",
        "documentation": {}
    },
    {
        "label": "runner",
        "kind": 2,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "def runner(config, base_dir, checkpoint_dir, slurm_save_dir, overwrite,\n\tsubmit):\n\t\"\"\"Trains model in config if not trained before.\n\tArgs:\n\t\tconfig: dict with config\n\tReturns:\n\t\tNothing\n\t\"\"\"\n\t# check system status\n\thash_string = utils.config_hasher(config)",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "def main(experiment_name,\n\t\t\t\t\tbase_dir,\n\t\t\t\t\tcheckpoint_dir,\n\t\t\t\t\tslurm_save_dir,\n\t\t\t\t\tmodel_to_tune,\n\t\t\t\t\tbatch_size,\n\t\t\t\t\toverwrite,\n\t\t\t\t\tsubmit,\n\t\t\t\t\tnum_workers,\n\t\t\t\t\tclean_directories):",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "ARMIS_USER",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "ARMIS_USER = 'mmakar'\nARMIS_MAIN_DIR = '/nfs/turbo/coe-rbg'\nif ARMIS_USER == 'precisionhealth':\n\tARMIS_SCRATCH_DIR = '/scratch/precisionhealth_owned_root/precisionhealth_owned1'\n\tACCOUNT = 'precisionhealth_owned1'\n\tPARTITION = 'precisionhealth'\nif ARMIS_USER == 'mmakar':\n\tARMIS_SCRATCH_DIR = '/scratch/mmakar_root/mmakar0/'\n\tACCOUNT = 'mmakar0'\n\tPARTITION = 'gpu'",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "ARMIS_MAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "ARMIS_MAIN_DIR = '/nfs/turbo/coe-rbg'\nif ARMIS_USER == 'precisionhealth':\n\tARMIS_SCRATCH_DIR = '/scratch/precisionhealth_owned_root/precisionhealth_owned1'\n\tACCOUNT = 'precisionhealth_owned1'\n\tPARTITION = 'precisionhealth'\nif ARMIS_USER == 'mmakar':\n\tARMIS_SCRATCH_DIR = '/scratch/mmakar_root/mmakar0/'\n\tACCOUNT = 'mmakar0'\n\tPARTITION = 'gpu'\nMIT_MAIN_DIR = '/data/ddmg/scate/'",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tARMIS_SCRATCH_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tARMIS_SCRATCH_DIR = '/scratch/precisionhealth_owned_root/precisionhealth_owned1'\n\tACCOUNT = 'precisionhealth_owned1'\n\tPARTITION = 'precisionhealth'\nif ARMIS_USER == 'mmakar':\n\tARMIS_SCRATCH_DIR = '/scratch/mmakar_root/mmakar0/'\n\tACCOUNT = 'mmakar0'\n\tPARTITION = 'gpu'\nMIT_MAIN_DIR = '/data/ddmg/scate/'\nMIT_SCRATCH_DIR = '/data/ddmg/scate/scratch'\nif os.path.isdir(ARMIS_MAIN_DIR):",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tACCOUNT",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tACCOUNT = 'precisionhealth_owned1'\n\tPARTITION = 'precisionhealth'\nif ARMIS_USER == 'mmakar':\n\tARMIS_SCRATCH_DIR = '/scratch/mmakar_root/mmakar0/'\n\tACCOUNT = 'mmakar0'\n\tPARTITION = 'gpu'\nMIT_MAIN_DIR = '/data/ddmg/scate/'\nMIT_SCRATCH_DIR = '/data/ddmg/scate/scratch'\nif os.path.isdir(ARMIS_MAIN_DIR):\n\tMAIN_DIR = ARMIS_MAIN_DIR",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tPARTITION",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tPARTITION = 'precisionhealth'\nif ARMIS_USER == 'mmakar':\n\tARMIS_SCRATCH_DIR = '/scratch/mmakar_root/mmakar0/'\n\tACCOUNT = 'mmakar0'\n\tPARTITION = 'gpu'\nMIT_MAIN_DIR = '/data/ddmg/scate/'\nMIT_SCRATCH_DIR = '/data/ddmg/scate/scratch'\nif os.path.isdir(ARMIS_MAIN_DIR):\n\tMAIN_DIR = ARMIS_MAIN_DIR\n\tSCRATCH_DIR = ARMIS_SCRATCH_DIR",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tARMIS_SCRATCH_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tARMIS_SCRATCH_DIR = '/scratch/mmakar_root/mmakar0/'\n\tACCOUNT = 'mmakar0'\n\tPARTITION = 'gpu'\nMIT_MAIN_DIR = '/data/ddmg/scate/'\nMIT_SCRATCH_DIR = '/data/ddmg/scate/scratch'\nif os.path.isdir(ARMIS_MAIN_DIR):\n\tMAIN_DIR = ARMIS_MAIN_DIR\n\tSCRATCH_DIR = ARMIS_SCRATCH_DIR\n\tHOST = 'ARMIS'\nelif os.path.isdir(MIT_MAIN_DIR):",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tACCOUNT",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tACCOUNT = 'mmakar0'\n\tPARTITION = 'gpu'\nMIT_MAIN_DIR = '/data/ddmg/scate/'\nMIT_SCRATCH_DIR = '/data/ddmg/scate/scratch'\nif os.path.isdir(ARMIS_MAIN_DIR):\n\tMAIN_DIR = ARMIS_MAIN_DIR\n\tSCRATCH_DIR = ARMIS_SCRATCH_DIR\n\tHOST = 'ARMIS'\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tPARTITION",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tPARTITION = 'gpu'\nMIT_MAIN_DIR = '/data/ddmg/scate/'\nMIT_SCRATCH_DIR = '/data/ddmg/scate/scratch'\nif os.path.isdir(ARMIS_MAIN_DIR):\n\tMAIN_DIR = ARMIS_MAIN_DIR\n\tSCRATCH_DIR = ARMIS_SCRATCH_DIR\n\tHOST = 'ARMIS'\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\n\tSCRATCH_DIR = MIT_SCRATCH_DIR",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "MIT_MAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "MIT_MAIN_DIR = '/data/ddmg/scate/'\nMIT_SCRATCH_DIR = '/data/ddmg/scate/scratch'\nif os.path.isdir(ARMIS_MAIN_DIR):\n\tMAIN_DIR = ARMIS_MAIN_DIR\n\tSCRATCH_DIR = ARMIS_SCRATCH_DIR\n\tHOST = 'ARMIS'\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\n\tSCRATCH_DIR = MIT_SCRATCH_DIR\n\tHOST = 'TIG'",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "MIT_SCRATCH_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "MIT_SCRATCH_DIR = '/data/ddmg/scate/scratch'\nif os.path.isdir(ARMIS_MAIN_DIR):\n\tMAIN_DIR = ARMIS_MAIN_DIR\n\tSCRATCH_DIR = ARMIS_SCRATCH_DIR\n\tHOST = 'ARMIS'\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\n\tSCRATCH_DIR = MIT_SCRATCH_DIR\n\tHOST = 'TIG'\nelse:",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tMAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tMAIN_DIR = ARMIS_MAIN_DIR\n\tSCRATCH_DIR = ARMIS_SCRATCH_DIR\n\tHOST = 'ARMIS'\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\n\tSCRATCH_DIR = MIT_SCRATCH_DIR\n\tHOST = 'TIG'\nelse:\n\traise ValueError((\"Can't locate data resources, please point to\"\n\t\" the right directory that has the data\"))",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tSCRATCH_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tSCRATCH_DIR = ARMIS_SCRATCH_DIR\n\tHOST = 'ARMIS'\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\n\tSCRATCH_DIR = MIT_SCRATCH_DIR\n\tHOST = 'TIG'\nelse:\n\traise ValueError((\"Can't locate data resources, please point to\"\n\t\" the right directory that has the data\"))\ndef runner(config, base_dir, checkpoint_dir, slurm_save_dir, overwrite,",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tHOST",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tHOST = 'ARMIS'\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\n\tSCRATCH_DIR = MIT_SCRATCH_DIR\n\tHOST = 'TIG'\nelse:\n\traise ValueError((\"Can't locate data resources, please point to\"\n\t\" the right directory that has the data\"))\ndef runner(config, base_dir, checkpoint_dir, slurm_save_dir, overwrite,\n\tsubmit):",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tMAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tMAIN_DIR = MIT_MAIN_DIR\n\tSCRATCH_DIR = MIT_SCRATCH_DIR\n\tHOST = 'TIG'\nelse:\n\traise ValueError((\"Can't locate data resources, please point to\"\n\t\" the right directory that has the data\"))\ndef runner(config, base_dir, checkpoint_dir, slurm_save_dir, overwrite,\n\tsubmit):\n\t\"\"\"Trains model in config if not trained before.\n\tArgs:",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tSCRATCH_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tSCRATCH_DIR = MIT_SCRATCH_DIR\n\tHOST = 'TIG'\nelse:\n\traise ValueError((\"Can't locate data resources, please point to\"\n\t\" the right directory that has the data\"))\ndef runner(config, base_dir, checkpoint_dir, slurm_save_dir, overwrite,\n\tsubmit):\n\t\"\"\"Trains model in config if not trained before.\n\tArgs:\n\t\tconfig: dict with config",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tHOST",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tHOST = 'TIG'\nelse:\n\traise ValueError((\"Can't locate data resources, please point to\"\n\t\" the right directory that has the data\"))\ndef runner(config, base_dir, checkpoint_dir, slurm_save_dir, overwrite,\n\tsubmit):\n\t\"\"\"Trains model in config if not trained before.\n\tArgs:\n\t\tconfig: dict with config\n\tReturns:",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\thash_string",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\thash_string = utils.config_hasher(config)\n\tprint(hash_string)\n\tmodel_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tif not os.path.exists(model_dir):\n\t\tos.system(f'mkdir -p {model_dir}')\n\tcheckpoint_dir = os.path.join(checkpoint_dir, 'tuning', hash_string)\n\tif not os.path.exists(checkpoint_dir):\n\t\tos.system(f'mkdir -p {checkpoint_dir}')\n\tpickle.dump(config, open(f'{model_dir}/config.pkl', 'wb'))\n\tconfig['data_dir'] = base_dir",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tmodel_dir",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tmodel_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tif not os.path.exists(model_dir):\n\t\tos.system(f'mkdir -p {model_dir}')\n\tcheckpoint_dir = os.path.join(checkpoint_dir, 'tuning', hash_string)\n\tif not os.path.exists(checkpoint_dir):\n\t\tos.system(f'mkdir -p {checkpoint_dir}')\n\tpickle.dump(config, open(f'{model_dir}/config.pkl', 'wb'))\n\tconfig['data_dir'] = base_dir\n\tconfig['exp_dir'] = model_dir\n\tconfig['checkpoint_dir'] = checkpoint_dir",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tcheckpoint_dir",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tcheckpoint_dir = os.path.join(checkpoint_dir, 'tuning', hash_string)\n\tif not os.path.exists(checkpoint_dir):\n\t\tos.system(f'mkdir -p {checkpoint_dir}')\n\tpickle.dump(config, open(f'{model_dir}/config.pkl', 'wb'))\n\tconfig['data_dir'] = base_dir\n\tconfig['exp_dir'] = model_dir\n\tconfig['checkpoint_dir'] = checkpoint_dir\n\tconfig['gpuid'] = '$CUDA_VISIBLE_DEVICES'\n\tflags = ' '.join('--%s %s \\\\\\n' % (k, str(v)) for k, v in config.items())\n\tif os.path.exists(f'{slurm_save_dir}/{hash_string}.sbatch'):",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tconfig['data_dir']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tconfig['data_dir'] = base_dir\n\tconfig['exp_dir'] = model_dir\n\tconfig['checkpoint_dir'] = checkpoint_dir\n\tconfig['gpuid'] = '$CUDA_VISIBLE_DEVICES'\n\tflags = ' '.join('--%s %s \\\\\\n' % (k, str(v)) for k, v in config.items())\n\tif os.path.exists(f'{slurm_save_dir}/{hash_string}.sbatch'):\n\t\tos.remove(f'{slurm_save_dir}/{hash_string}.sbatch')\n\tf = open(f'{slurm_save_dir}/{hash_string}.sbatch', 'x')\n\tf.write('#!/bin/bash\\n')\n\tf.write('#SBATCH --time=10:00:00\\n')",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tconfig['exp_dir']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tconfig['exp_dir'] = model_dir\n\tconfig['checkpoint_dir'] = checkpoint_dir\n\tconfig['gpuid'] = '$CUDA_VISIBLE_DEVICES'\n\tflags = ' '.join('--%s %s \\\\\\n' % (k, str(v)) for k, v in config.items())\n\tif os.path.exists(f'{slurm_save_dir}/{hash_string}.sbatch'):\n\t\tos.remove(f'{slurm_save_dir}/{hash_string}.sbatch')\n\tf = open(f'{slurm_save_dir}/{hash_string}.sbatch', 'x')\n\tf.write('#!/bin/bash\\n')\n\tf.write('#SBATCH --time=10:00:00\\n')\n\tf.write('#SBATCH --cpus-per-task=10\\n')",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tconfig['checkpoint_dir']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tconfig['checkpoint_dir'] = checkpoint_dir\n\tconfig['gpuid'] = '$CUDA_VISIBLE_DEVICES'\n\tflags = ' '.join('--%s %s \\\\\\n' % (k, str(v)) for k, v in config.items())\n\tif os.path.exists(f'{slurm_save_dir}/{hash_string}.sbatch'):\n\t\tos.remove(f'{slurm_save_dir}/{hash_string}.sbatch')\n\tf = open(f'{slurm_save_dir}/{hash_string}.sbatch', 'x')\n\tf.write('#!/bin/bash\\n')\n\tf.write('#SBATCH --time=10:00:00\\n')\n\tf.write('#SBATCH --cpus-per-task=10\\n')\n\tf.write('#SBATCH --nodes=1\\n')",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tconfig['gpuid']",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tconfig['gpuid'] = '$CUDA_VISIBLE_DEVICES'\n\tflags = ' '.join('--%s %s \\\\\\n' % (k, str(v)) for k, v in config.items())\n\tif os.path.exists(f'{slurm_save_dir}/{hash_string}.sbatch'):\n\t\tos.remove(f'{slurm_save_dir}/{hash_string}.sbatch')\n\tf = open(f'{slurm_save_dir}/{hash_string}.sbatch', 'x')\n\tf.write('#!/bin/bash\\n')\n\tf.write('#SBATCH --time=10:00:00\\n')\n\tf.write('#SBATCH --cpus-per-task=10\\n')\n\tf.write('#SBATCH --nodes=1\\n')\n\tf.write('#SBATCH --output=gpu.out\\n')",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tflags",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tflags = ' '.join('--%s %s \\\\\\n' % (k, str(v)) for k, v in config.items())\n\tif os.path.exists(f'{slurm_save_dir}/{hash_string}.sbatch'):\n\t\tos.remove(f'{slurm_save_dir}/{hash_string}.sbatch')\n\tf = open(f'{slurm_save_dir}/{hash_string}.sbatch', 'x')\n\tf.write('#!/bin/bash\\n')\n\tf.write('#SBATCH --time=10:00:00\\n')\n\tf.write('#SBATCH --cpus-per-task=10\\n')\n\tf.write('#SBATCH --nodes=1\\n')\n\tf.write('#SBATCH --output=gpu.out\\n')\n\tf.write('#SBATCH --tasks-per-node=1\\n')",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tf",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tf = open(f'{slurm_save_dir}/{hash_string}.sbatch', 'x')\n\tf.write('#!/bin/bash\\n')\n\tf.write('#SBATCH --time=10:00:00\\n')\n\tf.write('#SBATCH --cpus-per-task=10\\n')\n\tf.write('#SBATCH --nodes=1\\n')\n\tf.write('#SBATCH --output=gpu.out\\n')\n\tf.write('#SBATCH --tasks-per-node=1\\n')\n\tf.write('#SBATCH --gres=gpu:1\\n')\n\t# f.write('#SBATCH --gpus-per-task=1\\n')\n\tif HOST == 'ARMIS':",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tall_config",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tall_config = configurator.get_sweep(experiment_name, model_to_tune,\n\t\tbatch_size)\n\tprint(f'All configs are {len(all_config)}')\n\tif not overwrite:\n\t\tconfigs_to_consider = [\n\t\t\tnot utils.tried_config(config, base_dir=base_dir) for config in all_config\n\t\t]\n\t\tall_config = list(itertools.compress(all_config, configs_to_consider))\n\tprint(f'Remaining configs are {len(all_config)}')\n\t# all_config = all_config[:1]",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\t\tconfigs_to_consider",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\t\tconfigs_to_consider = [\n\t\t\tnot utils.tried_config(config, base_dir=base_dir) for config in all_config\n\t\t]\n\t\tall_config = list(itertools.compress(all_config, configs_to_consider))\n\tprint(f'Remaining configs are {len(all_config)}')\n\t# all_config = all_config[:1]\n\trunner_wrapper = functools.partial(runner, base_dir=base_dir,\n\t\tcheckpoint_dir=checkpoint_dir, slurm_save_dir=slurm_save_dir,\n\t\toverwrite=overwrite, submit=submit)\n\tif num_workers > 0:",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\t\tall_config",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\t\tall_config = list(itertools.compress(all_config, configs_to_consider))\n\tprint(f'Remaining configs are {len(all_config)}')\n\t# all_config = all_config[:1]\n\trunner_wrapper = functools.partial(runner, base_dir=base_dir,\n\t\tcheckpoint_dir=checkpoint_dir, slurm_save_dir=slurm_save_dir,\n\t\toverwrite=overwrite, submit=submit)\n\tif num_workers > 0:\n\t\tpool = multiprocessing.Pool(num_workers)\n\t\tfor _ in tqdm.tqdm(pool.imap_unordered(runner_wrapper, all_config),\n\t\t\ttotal=len(all_config)):",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\trunner_wrapper",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\trunner_wrapper = functools.partial(runner, base_dir=base_dir,\n\t\tcheckpoint_dir=checkpoint_dir, slurm_save_dir=slurm_save_dir,\n\t\toverwrite=overwrite, submit=submit)\n\tif num_workers > 0:\n\t\tpool = multiprocessing.Pool(num_workers)\n\t\tfor _ in tqdm.tqdm(pool.imap_unordered(runner_wrapper, all_config),\n\t\t\ttotal=len(all_config)):\n\t\t\tpass\n\telse:\n\t\tfor config in all_config:",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\t\tpool",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\t\tpool = multiprocessing.Pool(num_workers)\n\t\tfor _ in tqdm.tqdm(pool.imap_unordered(runner_wrapper, all_config),\n\t\t\ttotal=len(all_config)):\n\t\t\tpass\n\telse:\n\t\tfor config in all_config:\n\t\t\trunner_wrapper(config)\n\tif clean_directories:\n\t\traise NotImplementedError(\"havent implemented cleaning up yet\")\nif __name__ == \"__main__\":",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\timplemented_models",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\timplemented_models = open(\n\t\tf'{Path(__file__).resolve().parent}/implemented_models.txt',\n\t\t\"r\").read().split(\"\\n\")\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--base_dir', '-base_dir',\n\t\thelp=\"Base directory where the final model will be saved\",\n\t\ttype=str)\n\tparser.add_argument('--checkpoint_dir', '-checkpoint_dir',\n\t\thelp=\"Checkpoint directory where the model checkpoints will be saved\",\n\t\ttype=str)",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\tparser",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--base_dir', '-base_dir',\n\t\thelp=\"Base directory where the final model will be saved\",\n\t\ttype=str)\n\tparser.add_argument('--checkpoint_dir', '-checkpoint_dir',\n\t\thelp=\"Checkpoint directory where the model checkpoints will be saved\",\n\t\ttype=str)\n\tparser.add_argument('--slurm_save_dir', '-slurm_save_dir',\n\t\thelp=\"Directory where the slurm scripts will be saved\",\n\t\ttype=str)",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "\targs",
        "kind": 5,
        "importPath": "chexpert_support_device.create_submit_slurm",
        "description": "chexpert_support_device.create_submit_slurm",
        "peekOfCode": "\targs = vars(parser.parse_args())\n\tmain(**args)",
        "detail": "chexpert_support_device.create_submit_slurm",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "chexpert_support_device.cross_validation",
        "description": "chexpert_support_device.cross_validation",
        "peekOfCode": "def main(base_dir, experiment_name, model_to_tune,\n\txv_method, batch_size, num_workers, pval):\n\tif not os.path.exists(f'{base_dir}/final_models/'):\n\t\tos.mkdir(f'{base_dir}/final_models/')\n\tall_config = configurator.get_sweep(experiment_name, model_to_tune, batch_size)\n\tprint(f'All configs are {len(all_config)}')\n\toriginal_configs = len(all_config)\n\t# -- Get all the configs that are available\n\tconfigs_available = [\n\t\tutils.tried_config(config, base_dir=base_dir) for config in all_config",
        "detail": "chexpert_support_device.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tall_config",
        "kind": 5,
        "importPath": "chexpert_support_device.cross_validation",
        "description": "chexpert_support_device.cross_validation",
        "peekOfCode": "\tall_config = configurator.get_sweep(experiment_name, model_to_tune, batch_size)\n\tprint(f'All configs are {len(all_config)}')\n\toriginal_configs = len(all_config)\n\t# -- Get all the configs that are available\n\tconfigs_available = [\n\t\tutils.tried_config(config, base_dir=base_dir) for config in all_config\n\t]\n\tall_config = list(itertools.compress(all_config, configs_available))\n\tfound_configs = len(all_config)\n\tprint(f'------ FOUND {found_configs} / {original_configs}---------')",
        "detail": "chexpert_support_device.cross_validation",
        "documentation": {}
    },
    {
        "label": "\toriginal_configs",
        "kind": 5,
        "importPath": "chexpert_support_device.cross_validation",
        "description": "chexpert_support_device.cross_validation",
        "peekOfCode": "\toriginal_configs = len(all_config)\n\t# -- Get all the configs that are available\n\tconfigs_available = [\n\t\tutils.tried_config(config, base_dir=base_dir) for config in all_config\n\t]\n\tall_config = list(itertools.compress(all_config, configs_available))\n\tfound_configs = len(all_config)\n\tprint(f'------ FOUND {found_configs} / {original_configs}---------')\n\tbest_model_results, best_model_configs = cv.get_optimal_model_results(\n\t\tmode=xv_method,",
        "detail": "chexpert_support_device.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tconfigs_available",
        "kind": 5,
        "importPath": "chexpert_support_device.cross_validation",
        "description": "chexpert_support_device.cross_validation",
        "peekOfCode": "\tconfigs_available = [\n\t\tutils.tried_config(config, base_dir=base_dir) for config in all_config\n\t]\n\tall_config = list(itertools.compress(all_config, configs_available))\n\tfound_configs = len(all_config)\n\tprint(f'------ FOUND {found_configs} / {original_configs}---------')\n\tbest_model_results, best_model_configs = cv.get_optimal_model_results(\n\t\tmode=xv_method,\n\t\tconfigs=all_config,\n\t\tbase_dir=base_dir,",
        "detail": "chexpert_support_device.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tall_config",
        "kind": 5,
        "importPath": "chexpert_support_device.cross_validation",
        "description": "chexpert_support_device.cross_validation",
        "peekOfCode": "\tall_config = list(itertools.compress(all_config, configs_available))\n\tfound_configs = len(all_config)\n\tprint(f'------ FOUND {found_configs} / {original_configs}---------')\n\tbest_model_results, best_model_configs = cv.get_optimal_model_results(\n\t\tmode=xv_method,\n\t\tconfigs=all_config,\n\t\tbase_dir=base_dir,\n\t\thparams=['alpha', 'sigma', 'l2_penalty', 'embedding_dim'],\n\t\tnum_workers=num_workers,\n\t\tpval=0.05)",
        "detail": "chexpert_support_device.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tfound_configs",
        "kind": 5,
        "importPath": "chexpert_support_device.cross_validation",
        "description": "chexpert_support_device.cross_validation",
        "peekOfCode": "\tfound_configs = len(all_config)\n\tprint(f'------ FOUND {found_configs} / {original_configs}---------')\n\tbest_model_results, best_model_configs = cv.get_optimal_model_results(\n\t\tmode=xv_method,\n\t\tconfigs=all_config,\n\t\tbase_dir=base_dir,\n\t\thparams=['alpha', 'sigma', 'l2_penalty', 'embedding_dim'],\n\t\tnum_workers=num_workers,\n\t\tpval=0.05)\n\tbest_model_results.to_csv(",
        "detail": "chexpert_support_device.cross_validation",
        "documentation": {}
    },
    {
        "label": "\timplemented_models",
        "kind": 5,
        "importPath": "chexpert_support_device.cross_validation",
        "description": "chexpert_support_device.cross_validation",
        "peekOfCode": "\timplemented_models = open(\n\t\tf'{Path(__file__).resolve().parent}/implemented_models.txt',\n\t\t\"r\").read().split(\"\\n\")\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--base_dir', '-base_dir',\n\t\thelp=\"Base directory\",\n\t\ttype=str)\n\tparser.add_argument('--experiment_name', '-experiment_name',\n\t\tdefault='skew_train',\n\t\tchoices=['unskew_train', 'skew_train'],",
        "detail": "chexpert_support_device.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tparser",
        "kind": 5,
        "importPath": "chexpert_support_device.cross_validation",
        "description": "chexpert_support_device.cross_validation",
        "peekOfCode": "\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--base_dir', '-base_dir',\n\t\thelp=\"Base directory\",\n\t\ttype=str)\n\tparser.add_argument('--experiment_name', '-experiment_name',\n\t\tdefault='skew_train',\n\t\tchoices=['unskew_train', 'skew_train'],\n\t\thelp=\"Which experiment to run\",\n\t\ttype=str)\n\tparser.add_argument('--model_to_tune', '-model_to_tune',",
        "detail": "chexpert_support_device.cross_validation",
        "documentation": {}
    },
    {
        "label": "\targs",
        "kind": 5,
        "importPath": "chexpert_support_device.cross_validation",
        "description": "chexpert_support_device.cross_validation",
        "peekOfCode": "\targs = vars(parser.parse_args())\n\tmain(**args)",
        "detail": "chexpert_support_device.cross_validation",
        "documentation": {}
    },
    {
        "label": "read_decode_jpg",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def read_decode_jpg(file_path):\n\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)\n\treturn img\ndef decode_number(label):\n\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(x, pixel, weighted):\n\tchest_image = x[0]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "decode_number",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def decode_number(label):\n\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(x, pixel, weighted):\n\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\tif weighted == 'True':",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "map_to_image_label",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def map_to_image_label(x, pixel, weighted):\n\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\tif weighted == 'True':\n\t\tsample_weights = x[4]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "map_to_image_label_test",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def map_to_image_label_test(x, pixel, weighted):\n\t\"\"\" same as normal function by makes sure to not use\n\tweighting. \"\"\"\n\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "sample_y2_on_y1",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def sample_y2_on_y1(df, y0_value, y1_value, dominant_probability, rng):\n\tdominant_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == y1_value))\n\t]\n\tsmall_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == (1 - y1_value)))\n\t]\n\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "sample_y1_on_main",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0 == y_value) & (df.y1 == y_value))]\n\tsmall_group = df.index[((df.y0 == y_value) & (df.y1 == (1 - y_value)))]\n\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability / small_probability) * len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group, size=int(\n\t\t\t\t(small_probability / dominant_probability) * len(dominant_group)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "fix_marginal",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability\n\tif len(y0_group) < (y0_probability / y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size=int((y1_probability / y0_probability) * len(y0_group)),\n\t\t\treplace=False).tolist()\n\telif len(y1_group) < (y1_probability / y0_probability) * len(y0_group):",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "get_skewed_data",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n\tif rng is None:\n\t\trng = np.random.RandomState(0)\n\t# --- Fix the conditional distributions of y2\n\tcand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n\tcand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n\tcand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n\tcand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)\n\tnew_cand_df = cand_df11.append(cand_df10).append(cand_df01).append(cand_df00)\n\tnew_cand_df.reset_index(inplace=True, drop=True)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "save_created_data",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def save_created_data(data_frame, experiment_directory, filename):\n\ttxt_df = f'{MAIN_DIR}/' + data_frame.Path + \\\n\t\t',' + data_frame.y0.astype(str) + \\\n\t\t',' + data_frame.y1.astype(str) + \\\n\t\t',' + data_frame.y2.astype(str)\n\ttxt_df.to_csv(f'{experiment_directory}/{filename}.txt',\n\t\tindex=False)\ndef load_created_data(chexpert_data_dir, random_seed, skew_train, weighted):\n\texperiment_directory = f'{chexpert_data_dir}/experiment_data/rs{random_seed}'\n\tskew_str = 'skew' if skew_train == 'True' else 'unskew'",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "load_created_data",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def load_created_data(chexpert_data_dir, random_seed, skew_train, weighted):\n\texperiment_directory = f'{chexpert_data_dir}/experiment_data/rs{random_seed}'\n\tskew_str = 'skew' if skew_train == 'True' else 'unskew'\n\ttrain_data = pd.read_csv(\n\t\tf'{experiment_directory}/{skew_str}_train.txt')\n\tif weighted == 'True':\n\t\ttrain_data = wt.get_simple_weights(train_data)\n\ttrain_data = train_data.values.tolist()\n\ttrain_data = [\n\t\ttuple(train_data[i][0].split(',')) for i in range(len(train_data))",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "create_save_chexpert_lists",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def create_save_chexpert_lists(chexpert_data_dir, p_tr=.7, p_val=0.25,\n\trandom_seed=None):\n\tif random_seed is None:\n\t\trng = np.random.RandomState(0)\n\telse:\n\t\trng = np.random.RandomState(random_seed)\n\texperiment_directory = f'{chexpert_data_dir}/experiment_data/rs{random_seed}'\n\tif not os.path.exists(experiment_directory):\n\t\tos.makedirs(experiment_directory)\n\t# --- read in the cleaned image filenames (see cohort_creation)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "build_input_fns",
        "kind": 2,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "def build_input_fns(chexpert_data_dir, skew_train='False',\n\tweighted='False', p_tr=.7, p_val=0.25, random_seed=None):\n\t# --- generate splits if they dont exist\n\tif not os.path.exists(\n\t\tf'{chexpert_data_dir}/experiment_data/rs{random_seed}/skew_train.txt'):\n\t\tcreate_save_chexpert_lists(\n\t\t\tchexpert_data_dir=chexpert_data_dir,\n\t\t\tp_tr=p_tr,\n\t\t\tp_val=p_val,\n\t\t\trandom_seed=random_seed)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "SOTO_MAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "SOTO_MAIN_DIR = '/nfs/turbo/coe-soto'\nRBG_MAIN_DIR = '/nfs/turbo/coe-rbg'\nMIT_MAIN_DIR = '/data/ddmg/slabs'\nif os.path.isdir(SOTO_MAIN_DIR):\n\tMAIN_DIR = SOTO_MAIN_DIR\nelif os.path.isdir(RBG_MAIN_DIR):\n\tMAIN_DIR = RBG_MAIN_DIR\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\ndef read_decode_jpg(file_path):",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "RBG_MAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "RBG_MAIN_DIR = '/nfs/turbo/coe-rbg'\nMIT_MAIN_DIR = '/data/ddmg/slabs'\nif os.path.isdir(SOTO_MAIN_DIR):\n\tMAIN_DIR = SOTO_MAIN_DIR\nelif os.path.isdir(RBG_MAIN_DIR):\n\tMAIN_DIR = RBG_MAIN_DIR\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\ndef read_decode_jpg(file_path):\n\timg = tf.io.read_file(file_path)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "MIT_MAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "MIT_MAIN_DIR = '/data/ddmg/slabs'\nif os.path.isdir(SOTO_MAIN_DIR):\n\tMAIN_DIR = SOTO_MAIN_DIR\nelif os.path.isdir(RBG_MAIN_DIR):\n\tMAIN_DIR = RBG_MAIN_DIR\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\ndef read_decode_jpg(file_path):\n\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tMAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tMAIN_DIR = SOTO_MAIN_DIR\nelif os.path.isdir(RBG_MAIN_DIR):\n\tMAIN_DIR = RBG_MAIN_DIR\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\ndef read_decode_jpg(file_path):\n\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)\n\treturn img\ndef decode_number(label):",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tMAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tMAIN_DIR = RBG_MAIN_DIR\nelif os.path.isdir(MIT_MAIN_DIR):\n\tMAIN_DIR = MIT_MAIN_DIR\ndef read_decode_jpg(file_path):\n\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)\n\treturn img\ndef decode_number(label):\n\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tMAIN_DIR",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tMAIN_DIR = MIT_MAIN_DIR\ndef read_decode_jpg(file_path):\n\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)\n\treturn img\ndef decode_number(label):\n\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(x, pixel, weighted):",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)\n\treturn img\ndef decode_number(label):\n\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(x, pixel, weighted):\n\tchest_image = x[0]\n\ty0 = x[1]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\timg = tf.image.decode_jpeg(img, channels=3)\n\treturn img\ndef decode_number(label):\n\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(x, pixel, weighted):\n\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tlabel",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(x, pixel, weighted):\n\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\tif weighted == 'True':\n\t\tsample_weights = x[4]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tlabel",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(x, pixel, weighted):\n\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\tif weighted == 'True':\n\t\tsample_weights = x[4]\n\t# decode images",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tchest_image",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\tif weighted == 'True':\n\t\tsample_weights = x[4]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty0",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\tif weighted == 'True':\n\t\tsample_weights = x[4]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty1",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty1 = x[2]\n\ty2 = x[3]\n\tif weighted == 'True':\n\t\tsample_weights = x[4]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty2",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty2 = x[3]\n\tif weighted == 'True':\n\t\tsample_weights = x[4]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tsample_weights",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tsample_weights = x[4]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = decode_number(sample_weights)\n\telse:",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = decode_number(sample_weights)\n\telse:\n\t\tsample_weights = None",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty0",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = decode_number(sample_weights)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty1",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = decode_number(sample_weights)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef map_to_image_label_test(x, pixel, weighted):",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty2",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = decode_number(sample_weights)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef map_to_image_label_test(x, pixel, weighted):\n\t\"\"\" same as normal function by makes sure to not use",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tlabels",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = decode_number(sample_weights)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef map_to_image_label_test(x, pixel, weighted):\n\t\"\"\" same as normal function by makes sure to not use\n\tweighting. \"\"\"",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tsample_weights",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tsample_weights = decode_number(sample_weights)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef map_to_image_label_test(x, pixel, weighted):\n\t\"\"\" same as normal function by makes sure to not use\n\tweighting. \"\"\"\n\tchest_image = x[0]\n\ty0 = x[1]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tsample_weights",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef map_to_image_label_test(x, pixel, weighted):\n\t\"\"\" same as normal function by makes sure to not use\n\tweighting. \"\"\"\n\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tlabels_and_weights",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef map_to_image_label_test(x, pixel, weighted):\n\t\"\"\" same as normal function by makes sure to not use\n\tweighting. \"\"\"\n\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\t# decode images",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tchest_image",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tchest_image = x[0]\n\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty0",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty0 = x[1]\n\ty1 = x[2]\n\ty2 = x[3]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty1",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty1 = x[2]\n\ty2 = x[3]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty2",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty2 = x[3]\n\t# decode images\n\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\timg = read_decode_jpg(chest_image)\n\t# resize, rescale  image\n\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\timg = tf.image.resize(img, (pixel, pixel))\n\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = tf.ones_like(y0)\n\telse:",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\timg = img / 255\n\t# get the label vector\n\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = tf.ones_like(y0)\n\telse:\n\t\tsample_weights = None",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty0",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty0 = decode_number(y0)\n\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = tf.ones_like(y0)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty1",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty1 = decode_number(y1)\n\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = tf.ones_like(y0)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef sample_y2_on_y1(df, y0_value, y1_value, dominant_probability, rng):",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty2",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty2 = decode_number(y2)\n\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = tf.ones_like(y0)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef sample_y2_on_y1(df, y0_value, y1_value, dominant_probability, rng):\n\tdominant_group = df.index[",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tlabels",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tlabels = tf.concat([y0, y1, y2], axis=0)\n\tif weighted == 'True':\n\t\tsample_weights = tf.ones_like(y0)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef sample_y2_on_y1(df, y0_value, y1_value, dominant_probability, rng):\n\tdominant_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == y1_value))",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tsample_weights",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tsample_weights = tf.ones_like(y0)\n\telse:\n\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef sample_y2_on_y1(df, y0_value, y1_value, dominant_probability, rng):\n\tdominant_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == y1_value))\n\t]\n\tsmall_group = df.index[",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tsample_weights",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tsample_weights = None\n\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef sample_y2_on_y1(df, y0_value, y1_value, dominant_probability, rng):\n\tdominant_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == y1_value))\n\t]\n\tsmall_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == (1 - y1_value)))\n\t]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tlabels_and_weights",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tlabels_and_weights = {'labels': labels, 'sample_weights': sample_weights}\n\treturn img, labels_and_weights\ndef sample_y2_on_y1(df, y0_value, y1_value, dominant_probability, rng):\n\tdominant_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == y1_value))\n\t]\n\tsmall_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == (1 - y1_value)))\n\t]\n\tsmall_probability = 1 - dominant_probability",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tdominant_group",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tdominant_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == y1_value))\n\t]\n\tsmall_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == (1 - y1_value)))\n\t]\n\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tsmall_group",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tsmall_group = df.index[\n\t\t((df.y0 == y0_value) & (df.y1 == y1_value) & (df.y2 == (1 - y1_value)))\n\t]\n\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability / small_probability) * len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tsmall_probability",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability / small_probability) * len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group, size=int(\n\t\t\t\t(small_probability / dominant_probability) * len(dominant_group)\n\t\t\t),",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group, size=int(\n\t\t\t\t(small_probability / dominant_probability) * len(dominant_group)\n\t\t\t),\n\t\t\treplace=False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability / dominant_probability) * len(dominant_group):",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tsmall_id = rng.choice(\n\t\t\tsmall_group, size=int(\n\t\t\t\t(small_probability / dominant_probability) * len(dominant_group)\n\t\t\t),\n\t\t\treplace=False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability / dominant_probability) * len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size=int(\n\t\t\t\t(dominant_probability / small_probability) * len(small_group)\n\t\t\t), replace=False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0 == y_value) & (df.y1 == y_value))]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size=int(\n\t\t\t\t(dominant_probability / small_probability) * len(small_group)\n\t\t\t), replace=False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0 == y_value) & (df.y1 == y_value))]\n\tsmall_group = df.index[((df.y0 == y_value) & (df.y1 == (1 - y_value)))]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tnew_ids",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0 == y_value) & (df.y1 == y_value))]\n\tsmall_group = df.index[((df.y0 == y_value) & (df.y1 == (1 - y_value)))]\n\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability / small_probability) * len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tdf_new",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0 == y_value) & (df.y1 == y_value))]\n\tsmall_group = df.index[((df.y0 == y_value) & (df.y1 == (1 - y_value)))]\n\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability / small_probability) * len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tdominant_group",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tdominant_group = df.index[((df.y0 == y_value) & (df.y1 == y_value))]\n\tsmall_group = df.index[((df.y0 == y_value) & (df.y1 == (1 - y_value)))]\n\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability / small_probability) * len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group, size=int(\n\t\t\t\t(small_probability / dominant_probability) * len(dominant_group)\n\t\t\t),",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tsmall_group",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tsmall_group = df.index[((df.y0 == y_value) & (df.y1 == (1 - y_value)))]\n\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability / small_probability) * len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group, size=int(\n\t\t\t\t(small_probability / dominant_probability) * len(dominant_group)\n\t\t\t),\n\t\t\treplace=False).tolist()",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tsmall_probability",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tsmall_probability = 1 - dominant_probability\n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability / small_probability) * len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group, size=int(\n\t\t\t\t(small_probability / dominant_probability) * len(dominant_group)\n\t\t\t),\n\t\t\treplace=False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group, size=int(\n\t\t\t\t(small_probability / dominant_probability) * len(dominant_group)\n\t\t\t),\n\t\t\treplace=False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability) * len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tsmall_id = rng.choice(\n\t\t\tsmall_group, size=int(\n\t\t\t\t(small_probability / dominant_probability) * len(dominant_group)\n\t\t\t),\n\t\t\treplace=False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability) * len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size=int(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size=int(\n\t\t\t\t(dominant_probability / small_probability) * len(small_group)\n\t\t\t), replace=False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size=int(\n\t\t\t\t(dominant_probability / small_probability) * len(small_group)\n\t\t\t), replace=False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tnew_ids",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability\n\tif len(y0_group) < (y0_probability / y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tdf_new",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability\n\tif len(y0_group) < (y0_probability / y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size=int((y1_probability / y0_probability) * len(y0_group)),",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty0_group",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability\n\tif len(y0_group) < (y0_probability / y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size=int((y1_probability / y0_probability) * len(y0_group)),\n\t\t\treplace=False).tolist()\n\telif len(y1_group) < (y1_probability / y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty1_group",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability\n\tif len(y0_group) < (y0_probability / y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size=int((y1_probability / y0_probability) * len(y0_group)),\n\t\t\treplace=False).tolist()\n\telif len(y1_group) < (y1_probability / y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ty1_probability",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ty1_probability = 1 - y0_probability\n\tif len(y0_group) < (y0_probability / y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size=int((y1_probability / y0_probability) * len(y0_group)),\n\t\t\treplace=False).tolist()\n\telif len(y1_group) < (y1_probability / y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size=int((y0_probability / y1_probability) * len(y1_group)),",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ty0_ids",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size=int((y1_probability / y0_probability) * len(y0_group)),\n\t\t\treplace=False).tolist()\n\telif len(y1_group) < (y1_probability / y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size=int((y0_probability / y1_probability) * len(y1_group)),\n\t\t\treplace=False\n\t\t).tolist()",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ty1_ids",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size=int((y1_probability / y0_probability) * len(y0_group)),\n\t\t\treplace=False).tolist()\n\telif len(y1_group) < (y1_probability / y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size=int((y0_probability / y1_probability) * len(y1_group)),\n\t\t\treplace=False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ty1_ids",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size=int((y0_probability / y1_probability) * len(y1_group)),\n\t\t\treplace=False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace=True, drop=True)\n\treshuffled_ids = rng.choice(dff.index,\n\t\tsize=len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop=True)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ty0_ids",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size=int((y0_probability / y1_probability) * len(y1_group)),\n\t\t\treplace=False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace=True, drop=True)\n\treshuffled_ids = rng.choice(dff.index,\n\t\tsize=len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop=True)\n\treturn dff",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tdff",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace=True, drop=True)\n\treshuffled_ids = rng.choice(dff.index,\n\t\tsize=len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop=True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n\tif rng is None:\n\t\trng = np.random.RandomState(0)\n\t# --- Fix the conditional distributions of y2",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\treshuffled_ids",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\treshuffled_ids = rng.choice(dff.index,\n\t\tsize=len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop=True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n\tif rng is None:\n\t\trng = np.random.RandomState(0)\n\t# --- Fix the conditional distributions of y2\n\tcand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n\tcand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tdff",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tdff = dff.iloc[reshuffled_ids].reset_index(drop=True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n\tif rng is None:\n\t\trng = np.random.RandomState(0)\n\t# --- Fix the conditional distributions of y2\n\tcand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n\tcand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n\tcand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n\tcand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\trng",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\trng = np.random.RandomState(0)\n\t# --- Fix the conditional distributions of y2\n\tcand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n\tcand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n\tcand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n\tcand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)\n\tnew_cand_df = cand_df11.append(cand_df10).append(cand_df01).append(cand_df00)\n\tnew_cand_df.reset_index(inplace=True, drop=True)\n\t# --- Fix the conditional distributions of y1\n\tcand_df1 = sample_y1_on_main(new_cand_df, 1, py1d, rng)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tcand_df11",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tcand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n\tcand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n\tcand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n\tcand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)\n\tnew_cand_df = cand_df11.append(cand_df10).append(cand_df01).append(cand_df00)\n\tnew_cand_df.reset_index(inplace=True, drop=True)\n\t# --- Fix the conditional distributions of y1\n\tcand_df1 = sample_y1_on_main(new_cand_df, 1, py1d, rng)\n\tcand_df0 = sample_y1_on_main(new_cand_df, 0, py1d, rng)\n\tcand_df10 = cand_df1.append(cand_df0)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tcand_df10",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tcand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n\tcand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n\tcand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)\n\tnew_cand_df = cand_df11.append(cand_df10).append(cand_df01).append(cand_df00)\n\tnew_cand_df.reset_index(inplace=True, drop=True)\n\t# --- Fix the conditional distributions of y1\n\tcand_df1 = sample_y1_on_main(new_cand_df, 1, py1d, rng)\n\tcand_df0 = sample_y1_on_main(new_cand_df, 0, py1d, rng)\n\tcand_df10 = cand_df1.append(cand_df0)\n\tcand_df10.reset_index(inplace=True, drop=True)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tcand_df01",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tcand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n\tcand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)\n\tnew_cand_df = cand_df11.append(cand_df10).append(cand_df01).append(cand_df00)\n\tnew_cand_df.reset_index(inplace=True, drop=True)\n\t# --- Fix the conditional distributions of y1\n\tcand_df1 = sample_y1_on_main(new_cand_df, 1, py1d, rng)\n\tcand_df0 = sample_y1_on_main(new_cand_df, 0, py1d, rng)\n\tcand_df10 = cand_df1.append(cand_df0)\n\tcand_df10.reset_index(inplace=True, drop=True)\n\t# --- Fix the marginal",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tcand_df00",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tcand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)\n\tnew_cand_df = cand_df11.append(cand_df10).append(cand_df01).append(cand_df00)\n\tnew_cand_df.reset_index(inplace=True, drop=True)\n\t# --- Fix the conditional distributions of y1\n\tcand_df1 = sample_y1_on_main(new_cand_df, 1, py1d, rng)\n\tcand_df0 = sample_y1_on_main(new_cand_df, 0, py1d, rng)\n\tcand_df10 = cand_df1.append(cand_df0)\n\tcand_df10.reset_index(inplace=True, drop=True)\n\t# --- Fix the marginal\n\tfinal_df = fix_marginal(cand_df10, py00, rng)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tnew_cand_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tnew_cand_df = cand_df11.append(cand_df10).append(cand_df01).append(cand_df00)\n\tnew_cand_df.reset_index(inplace=True, drop=True)\n\t# --- Fix the conditional distributions of y1\n\tcand_df1 = sample_y1_on_main(new_cand_df, 1, py1d, rng)\n\tcand_df0 = sample_y1_on_main(new_cand_df, 0, py1d, rng)\n\tcand_df10 = cand_df1.append(cand_df0)\n\tcand_df10.reset_index(inplace=True, drop=True)\n\t# --- Fix the marginal\n\tfinal_df = fix_marginal(cand_df10, py00, rng)\n\treturn final_df",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tcand_df1",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tcand_df1 = sample_y1_on_main(new_cand_df, 1, py1d, rng)\n\tcand_df0 = sample_y1_on_main(new_cand_df, 0, py1d, rng)\n\tcand_df10 = cand_df1.append(cand_df0)\n\tcand_df10.reset_index(inplace=True, drop=True)\n\t# --- Fix the marginal\n\tfinal_df = fix_marginal(cand_df10, py00, rng)\n\treturn final_df\ndef save_created_data(data_frame, experiment_directory, filename):\n\ttxt_df = f'{MAIN_DIR}/' + data_frame.Path + \\\n\t\t',' + data_frame.y0.astype(str) + \\",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tcand_df0",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tcand_df0 = sample_y1_on_main(new_cand_df, 0, py1d, rng)\n\tcand_df10 = cand_df1.append(cand_df0)\n\tcand_df10.reset_index(inplace=True, drop=True)\n\t# --- Fix the marginal\n\tfinal_df = fix_marginal(cand_df10, py00, rng)\n\treturn final_df\ndef save_created_data(data_frame, experiment_directory, filename):\n\ttxt_df = f'{MAIN_DIR}/' + data_frame.Path + \\\n\t\t',' + data_frame.y0.astype(str) + \\\n\t\t',' + data_frame.y1.astype(str) + \\",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tcand_df10",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tcand_df10 = cand_df1.append(cand_df0)\n\tcand_df10.reset_index(inplace=True, drop=True)\n\t# --- Fix the marginal\n\tfinal_df = fix_marginal(cand_df10, py00, rng)\n\treturn final_df\ndef save_created_data(data_frame, experiment_directory, filename):\n\ttxt_df = f'{MAIN_DIR}/' + data_frame.Path + \\\n\t\t',' + data_frame.y0.astype(str) + \\\n\t\t',' + data_frame.y1.astype(str) + \\\n\t\t',' + data_frame.y2.astype(str)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tfinal_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tfinal_df = fix_marginal(cand_df10, py00, rng)\n\treturn final_df\ndef save_created_data(data_frame, experiment_directory, filename):\n\ttxt_df = f'{MAIN_DIR}/' + data_frame.Path + \\\n\t\t',' + data_frame.y0.astype(str) + \\\n\t\t',' + data_frame.y1.astype(str) + \\\n\t\t',' + data_frame.y2.astype(str)\n\ttxt_df.to_csv(f'{experiment_directory}/{filename}.txt',\n\t\tindex=False)\ndef load_created_data(chexpert_data_dir, random_seed, skew_train, weighted):",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttxt_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttxt_df = f'{MAIN_DIR}/' + data_frame.Path + \\\n\t\t',' + data_frame.y0.astype(str) + \\\n\t\t',' + data_frame.y1.astype(str) + \\\n\t\t',' + data_frame.y2.astype(str)\n\ttxt_df.to_csv(f'{experiment_directory}/{filename}.txt',\n\t\tindex=False)\ndef load_created_data(chexpert_data_dir, random_seed, skew_train, weighted):\n\texperiment_directory = f'{chexpert_data_dir}/experiment_data/rs{random_seed}'\n\tskew_str = 'skew' if skew_train == 'True' else 'unskew'\n\ttrain_data = pd.read_csv(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\texperiment_directory",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\texperiment_directory = f'{chexpert_data_dir}/experiment_data/rs{random_seed}'\n\tskew_str = 'skew' if skew_train == 'True' else 'unskew'\n\ttrain_data = pd.read_csv(\n\t\tf'{experiment_directory}/{skew_str}_train.txt')\n\tif weighted == 'True':\n\t\ttrain_data = wt.get_simple_weights(train_data)\n\ttrain_data = train_data.values.tolist()\n\ttrain_data = [\n\t\ttuple(train_data[i][0].split(',')) for i in range(len(train_data))\n\t]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tskew_str",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tskew_str = 'skew' if skew_train == 'True' else 'unskew'\n\ttrain_data = pd.read_csv(\n\t\tf'{experiment_directory}/{skew_str}_train.txt')\n\tif weighted == 'True':\n\t\ttrain_data = wt.get_simple_weights(train_data)\n\ttrain_data = train_data.values.tolist()\n\ttrain_data = [\n\t\ttuple(train_data[i][0].split(',')) for i in range(len(train_data))\n\t]\n\tvalidation_data = pd.read_csv(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttrain_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttrain_data = pd.read_csv(\n\t\tf'{experiment_directory}/{skew_str}_train.txt')\n\tif weighted == 'True':\n\t\ttrain_data = wt.get_simple_weights(train_data)\n\ttrain_data = train_data.values.tolist()\n\ttrain_data = [\n\t\ttuple(train_data[i][0].split(',')) for i in range(len(train_data))\n\t]\n\tvalidation_data = pd.read_csv(\n\t\tf'{experiment_directory}/{skew_str}_valid.txt')",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ttrain_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ttrain_data = wt.get_simple_weights(train_data)\n\ttrain_data = train_data.values.tolist()\n\ttrain_data = [\n\t\ttuple(train_data[i][0].split(',')) for i in range(len(train_data))\n\t]\n\tvalidation_data = pd.read_csv(\n\t\tf'{experiment_directory}/{skew_str}_valid.txt')\n\tif weighted == 'True':\n\t\tvalidation_data = wt.get_simple_weights(validation_data)\n\tvalidation_data = validation_data.values.tolist()",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttrain_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttrain_data = train_data.values.tolist()\n\ttrain_data = [\n\t\ttuple(train_data[i][0].split(',')) for i in range(len(train_data))\n\t]\n\tvalidation_data = pd.read_csv(\n\t\tf'{experiment_directory}/{skew_str}_valid.txt')\n\tif weighted == 'True':\n\t\tvalidation_data = wt.get_simple_weights(validation_data)\n\tvalidation_data = validation_data.values.tolist()\n\tvalidation_data = [",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttrain_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttrain_data = [\n\t\ttuple(train_data[i][0].split(',')) for i in range(len(train_data))\n\t]\n\tvalidation_data = pd.read_csv(\n\t\tf'{experiment_directory}/{skew_str}_valid.txt')\n\tif weighted == 'True':\n\t\tvalidation_data = wt.get_simple_weights(validation_data)\n\tvalidation_data = validation_data.values.tolist()\n\tvalidation_data = [\n\t\ttuple(validation_data[i][0].split(',')) for i in range(len(validation_data))",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tvalidation_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tvalidation_data = pd.read_csv(\n\t\tf'{experiment_directory}/{skew_str}_valid.txt')\n\tif weighted == 'True':\n\t\tvalidation_data = wt.get_simple_weights(validation_data)\n\tvalidation_data = validation_data.values.tolist()\n\tvalidation_data = [\n\t\ttuple(validation_data[i][0].split(',')) for i in range(len(validation_data))\n\t]\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tvarying_joint_test_data_dict = {}",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tvalidation_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tvalidation_data = wt.get_simple_weights(validation_data)\n\tvalidation_data = validation_data.values.tolist()\n\tvalidation_data = [\n\t\ttuple(validation_data[i][0].split(',')) for i in range(len(validation_data))\n\t]\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tvarying_joint_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_test.txt'",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tvalidation_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tvalidation_data = validation_data.values.tolist()\n\tvalidation_data = [\n\t\ttuple(validation_data[i][0].split(',')) for i in range(len(validation_data))\n\t]\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tvarying_joint_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_test.txt'\n\t\t).values.tolist()",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tvalidation_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tvalidation_data = [\n\t\ttuple(validation_data[i][0].split(',')) for i in range(len(validation_data))\n\t]\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tvarying_joint_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tpskew_list",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tvarying_joint_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tvarying_joint_test_data_dict[pskew] = test_data",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tvarying_joint_test_data_dict",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tvarying_joint_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tvarying_joint_test_data_dict[pskew] = test_data\n\tfixed_joint_skew_test_data_dict = {}",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ttest_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tvarying_joint_test_data_dict[pskew] = test_data\n\tfixed_joint_skew_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ttest_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tvarying_joint_test_data_dict[pskew] = test_data\n\tfixed_joint_skew_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_fj09_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tvarying_joint_test_data_dict[pskew]",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tvarying_joint_test_data_dict[pskew] = test_data\n\tfixed_joint_skew_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_fj09_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tfixed_joint_skew_test_data_dict[pskew] = test_data",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tfixed_joint_skew_test_data_dict",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tfixed_joint_skew_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_fj09_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tfixed_joint_skew_test_data_dict[pskew] = test_data\n\tfixed_joint_unskew_test_data_dict = {}",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ttest_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_fj09_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tfixed_joint_skew_test_data_dict[pskew] = test_data\n\tfixed_joint_unskew_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ttest_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tfixed_joint_skew_test_data_dict[pskew] = test_data\n\tfixed_joint_unskew_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_fj05_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tfixed_joint_skew_test_data_dict[pskew]",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tfixed_joint_skew_test_data_dict[pskew] = test_data\n\tfixed_joint_unskew_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_fj05_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tfixed_joint_unskew_test_data_dict[pskew] = test_data",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tfixed_joint_unskew_test_data_dict",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tfixed_joint_unskew_test_data_dict = {}\n\tfor pskew in pskew_list:\n\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_fj05_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tfixed_joint_unskew_test_data_dict[pskew] = test_data\n\ttest_data_dict = {",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ttest_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ttest_data = pd.read_csv(\n\t\t\tf'{experiment_directory}/{pskew}_fj05_test.txt'\n\t\t).values.tolist()\n\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tfixed_joint_unskew_test_data_dict[pskew] = test_data\n\ttest_data_dict = {\n\t\t'varying_joint' : varying_joint_test_data_dict,\n\t\t'fixed_joint_0.9' : fixed_joint_skew_test_data_dict,",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\ttest_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\ttest_data = [\n\t\t\ttuple(test_data[i][0].split(',')) for i in range(len(test_data))\n\t\t]\n\t\tfixed_joint_unskew_test_data_dict[pskew] = test_data\n\ttest_data_dict = {\n\t\t'varying_joint' : varying_joint_test_data_dict,\n\t\t'fixed_joint_0.9' : fixed_joint_skew_test_data_dict,\n\t\t'fixed_joint_0.5': fixed_joint_unskew_test_data_dict\n\t}\n\treturn train_data, validation_data, test_data_dict",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tfixed_joint_unskew_test_data_dict[pskew]",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tfixed_joint_unskew_test_data_dict[pskew] = test_data\n\ttest_data_dict = {\n\t\t'varying_joint' : varying_joint_test_data_dict,\n\t\t'fixed_joint_0.9' : fixed_joint_skew_test_data_dict,\n\t\t'fixed_joint_0.5': fixed_joint_unskew_test_data_dict\n\t}\n\treturn train_data, validation_data, test_data_dict\ndef create_save_chexpert_lists(chexpert_data_dir, p_tr=.7, p_val=0.25,\n\trandom_seed=None):\n\tif random_seed is None:",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttest_data_dict",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttest_data_dict = {\n\t\t'varying_joint' : varying_joint_test_data_dict,\n\t\t'fixed_joint_0.9' : fixed_joint_skew_test_data_dict,\n\t\t'fixed_joint_0.5': fixed_joint_unskew_test_data_dict\n\t}\n\treturn train_data, validation_data, test_data_dict\ndef create_save_chexpert_lists(chexpert_data_dir, p_tr=.7, p_val=0.25,\n\trandom_seed=None):\n\tif random_seed is None:\n\t\trng = np.random.RandomState(0)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\trng",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\trng = np.random.RandomState(0)\n\telse:\n\t\trng = np.random.RandomState(random_seed)\n\texperiment_directory = f'{chexpert_data_dir}/experiment_data/rs{random_seed}'\n\tif not os.path.exists(experiment_directory):\n\t\tos.makedirs(experiment_directory)\n\t# --- read in the cleaned image filenames (see cohort_creation)\n\tdf = pd.read_csv(f'{chexpert_data_dir}/penumonia_nofinding_sd_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\trng",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\trng = np.random.RandomState(random_seed)\n\texperiment_directory = f'{chexpert_data_dir}/experiment_data/rs{random_seed}'\n\tif not os.path.exists(experiment_directory):\n\t\tos.makedirs(experiment_directory)\n\t# --- read in the cleaned image filenames (see cohort_creation)\n\tdf = pd.read_csv(f'{chexpert_data_dir}/penumonia_nofinding_sd_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize=int(len(df.patient.unique()) * p_tr), replace=False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\texperiment_directory",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\texperiment_directory = f'{chexpert_data_dir}/experiment_data/rs{random_seed}'\n\tif not os.path.exists(experiment_directory):\n\t\tos.makedirs(experiment_directory)\n\t# --- read in the cleaned image filenames (see cohort_creation)\n\tdf = pd.read_csv(f'{chexpert_data_dir}/penumonia_nofinding_sd_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize=int(len(df.patient.unique()) * p_tr), replace=False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tdf",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tdf = pd.read_csv(f'{chexpert_data_dir}/penumonia_nofinding_sd_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize=int(len(df.patient.unique()) * p_tr), replace=False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int((1 - p_val) * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttr_val_candidates",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize=int(len(df.patient.unique()) * p_tr), replace=False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int((1 - p_val) * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(\n\t\tdrop=True)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tts_candidates",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int((1 - p_val) * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(\n\t\tdrop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttr_candidates",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int((1 - p_val) * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(\n\t\tdrop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(\n\t\tdf.patient.unique())",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tval_candidates",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(\n\t\tdrop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(\n\t\tdf.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttr_candidates_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(\n\t\tdrop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(\n\t\tdf.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tval_candidates_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(\n\t\tdrop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(\n\t\tdf.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tts_candidates_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(\n\t\tdf.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets\n\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d=0.9, py2d=0.9, py00=0.7,\n\t\trng=rng)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttr_sk_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d=0.9, py2d=0.9, py00=0.7,\n\t\trng=rng)\n\tsave_created_data(tr_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_train')\n\ttr_usk_df = get_skewed_data(tr_candidates_df, py1d=0.5, py2d=0.5, py00=0.7,\n\t\trng=rng)\n\tsave_created_data(tr_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_train')\n\t# --- get validation datasets\n\tval_sk_df = get_skewed_data(val_candidates_df, py1d=0.9, py2d=0.9, py00=0.7,",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttr_usk_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttr_usk_df = get_skewed_data(tr_candidates_df, py1d=0.5, py2d=0.5, py00=0.7,\n\t\trng=rng)\n\tsave_created_data(tr_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_train')\n\t# --- get validation datasets\n\tval_sk_df = get_skewed_data(val_candidates_df, py1d=0.9, py2d=0.9, py00=0.7,\n\t\trng=rng)\n\tsave_created_data(val_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_valid')\n\tval_usk_df = get_skewed_data(val_candidates_df, py1d=0.5, py2d=0.5, py00=0.7,",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tval_sk_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tval_sk_df = get_skewed_data(val_candidates_df, py1d=0.9, py2d=0.9, py00=0.7,\n\t\trng=rng)\n\tsave_created_data(val_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_valid')\n\tval_usk_df = get_skewed_data(val_candidates_df, py1d=0.5, py2d=0.5, py00=0.7,\n\t\trng=rng)\n\tsave_created_data(val_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_valid')\n\t# --- get test\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tval_usk_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tval_usk_df = get_skewed_data(val_candidates_df, py1d=0.5, py2d=0.5, py00=0.7,\n\t\trng=rng)\n\tsave_created_data(val_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_valid')\n\t# --- get test\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00=0.7,\n\t\t\trng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tpskew_list",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00=0.7,\n\t\t\trng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_test')\n\t# get test fixed aux joint skewed\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=0.9, py00=0.7,\n\t\t\trng=rng)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tts_sk_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00=0.7,\n\t\t\trng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_test')\n\t# get test fixed aux joint skewed\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=0.9, py00=0.7,\n\t\t\trng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_fj09_test')",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tts_sk_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=0.9, py00=0.7,\n\t\t\trng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_fj09_test')\n\t# get test fixed aux joint\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=0.5, py00=0.7,\n\t\t\trng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_fj05_test')",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tts_sk_df",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=0.5, py00=0.7,\n\t\t\trng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_fj05_test')\ndef build_input_fns(chexpert_data_dir, skew_train='False',\n\tweighted='False', p_tr=.7, p_val=0.25, random_seed=None):\n\t# --- generate splits if they dont exist\n\tif not os.path.exists(\n\t\tf'{chexpert_data_dir}/experiment_data/rs{random_seed}/skew_train.txt'):\n\t\tcreate_save_chexpert_lists(",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\ttrain_data_size",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\ttrain_data_size = len(train_data)\n\t# Build an iterator over training batches.\n\tdef train_input_fn(params):\n\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])\n\t\tbatch_size = params['batch_size']\n\t\tnum_epochs = params['num_epochs']\n\t\tdataset = tf.data.Dataset.from_tensor_slices(train_data)\n\t\tdataset = dataset.map(map_to_image_label_wrapper, num_parallel_calls=1)\n\t\t# dataset = dataset.shuffle(int(train_data_size * 0.05)).batch(batch_size",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tmap_to_image_label_wrapper",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])\n\t\tbatch_size = params['batch_size']\n\t\tnum_epochs = params['num_epochs']\n\t\tdataset = tf.data.Dataset.from_tensor_slices(train_data)\n\t\tdataset = dataset.map(map_to_image_label_wrapper, num_parallel_calls=1)\n\t\t# dataset = dataset.shuffle(int(train_data_size * 0.05)).batch(batch_size\n\t\t# ).repeat(num_epochs)\n\t\tdataset = dataset.batch(batch_size).repeat(num_epochs)\n\t\treturn dataset",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tbatch_size",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tbatch_size = params['batch_size']\n\t\tnum_epochs = params['num_epochs']\n\t\tdataset = tf.data.Dataset.from_tensor_slices(train_data)\n\t\tdataset = dataset.map(map_to_image_label_wrapper, num_parallel_calls=1)\n\t\t# dataset = dataset.shuffle(int(train_data_size * 0.05)).batch(batch_size\n\t\t# ).repeat(num_epochs)\n\t\tdataset = dataset.batch(batch_size).repeat(num_epochs)\n\t\treturn dataset\n\t# Build an iterator over validation batches\n\tdef valid_input_fn(params):",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tnum_epochs",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tnum_epochs = params['num_epochs']\n\t\tdataset = tf.data.Dataset.from_tensor_slices(train_data)\n\t\tdataset = dataset.map(map_to_image_label_wrapper, num_parallel_calls=1)\n\t\t# dataset = dataset.shuffle(int(train_data_size * 0.05)).batch(batch_size\n\t\t# ).repeat(num_epochs)\n\t\tdataset = dataset.batch(batch_size).repeat(num_epochs)\n\t\treturn dataset\n\t# Build an iterator over validation batches\n\tdef valid_input_fn(params):\n\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label,",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tdataset",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tdataset = tf.data.Dataset.from_tensor_slices(train_data)\n\t\tdataset = dataset.map(map_to_image_label_wrapper, num_parallel_calls=1)\n\t\t# dataset = dataset.shuffle(int(train_data_size * 0.05)).batch(batch_size\n\t\t# ).repeat(num_epochs)\n\t\tdataset = dataset.batch(batch_size).repeat(num_epochs)\n\t\treturn dataset\n\t# Build an iterator over validation batches\n\tdef valid_input_fn(params):\n\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tdataset",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tdataset = dataset.map(map_to_image_label_wrapper, num_parallel_calls=1)\n\t\t# dataset = dataset.shuffle(int(train_data_size * 0.05)).batch(batch_size\n\t\t# ).repeat(num_epochs)\n\t\tdataset = dataset.batch(batch_size).repeat(num_epochs)\n\t\treturn dataset\n\t# Build an iterator over validation batches\n\tdef valid_input_fn(params):\n\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])\n\t\tbatch_size = params['batch_size']",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tdataset",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tdataset = dataset.batch(batch_size).repeat(num_epochs)\n\t\treturn dataset\n\t# Build an iterator over validation batches\n\tdef valid_input_fn(params):\n\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])\n\t\tbatch_size = params['batch_size']\n\t\tvalid_dataset = tf.data.Dataset.from_tensor_slices(valid_data)\n\t\tvalid_dataset = valid_dataset.map(map_to_image_label_wrapper,\n\t\t\tnum_parallel_calls=1)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tmap_to_image_label_wrapper",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])\n\t\tbatch_size = params['batch_size']\n\t\tvalid_dataset = tf.data.Dataset.from_tensor_slices(valid_data)\n\t\tvalid_dataset = valid_dataset.map(map_to_image_label_wrapper,\n\t\t\tnum_parallel_calls=1)\n\t\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(\n\t\t\t1)\n\t\treturn valid_dataset\n\t# Build an iterator over the heldout set (shifted distribution).",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tbatch_size",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tbatch_size = params['batch_size']\n\t\tvalid_dataset = tf.data.Dataset.from_tensor_slices(valid_data)\n\t\tvalid_dataset = valid_dataset.map(map_to_image_label_wrapper,\n\t\t\tnum_parallel_calls=1)\n\t\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(\n\t\t\t1)\n\t\treturn valid_dataset\n\t# Build an iterator over the heldout set (shifted distribution).\n\tdef eval_input_fn_creater(py, params, fixed_joint=False, aux_joint_skew=0.5):\n\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label_test,",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tvalid_dataset",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tvalid_dataset = tf.data.Dataset.from_tensor_slices(valid_data)\n\t\tvalid_dataset = valid_dataset.map(map_to_image_label_wrapper,\n\t\t\tnum_parallel_calls=1)\n\t\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(\n\t\t\t1)\n\t\treturn valid_dataset\n\t# Build an iterator over the heldout set (shifted distribution).\n\tdef eval_input_fn_creater(py, params, fixed_joint=False, aux_joint_skew=0.5):\n\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label_test,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tvalid_dataset",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tvalid_dataset = valid_dataset.map(map_to_image_label_wrapper,\n\t\t\tnum_parallel_calls=1)\n\t\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(\n\t\t\t1)\n\t\treturn valid_dataset\n\t# Build an iterator over the heldout set (shifted distribution).\n\tdef eval_input_fn_creater(py, params, fixed_joint=False, aux_joint_skew=0.5):\n\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label_test,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])\n\t\tif fixed_joint:",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tvalid_dataset",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(\n\t\t\t1)\n\t\treturn valid_dataset\n\t# Build an iterator over the heldout set (shifted distribution).\n\tdef eval_input_fn_creater(py, params, fixed_joint=False, aux_joint_skew=0.5):\n\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label_test,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])\n\t\tif fixed_joint:\n\t\t\tif aux_joint_skew == 0.9:\n\t\t\t\tshifted_test_data = shifted_data_dict['fixed_joint_0.9'][py]",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tmap_to_image_label_wrapper",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tmap_to_image_label_wrapper = functools.partial(map_to_image_label_test,\n\t\t\tpixel=params['pixel'], weighted=params['weighted'])\n\t\tif fixed_joint:\n\t\t\tif aux_joint_skew == 0.9:\n\t\t\t\tshifted_test_data = shifted_data_dict['fixed_joint_0.9'][py]\n\t\t\telif aux_joint_skew == 0.5:\n\t\t\t\tshifted_test_data = shifted_data_dict['fixed_joint_0.5'][py]\n\t\telse:\n\t\t\tshifted_test_data = shifted_data_dict['varying_joint'][py]\n\t\tbatch_size = params['batch_size']",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tshifted_test_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\t\t\tshifted_test_data = shifted_data_dict['fixed_joint_0.9'][py]\n\t\t\telif aux_joint_skew == 0.5:\n\t\t\t\tshifted_test_data = shifted_data_dict['fixed_joint_0.5'][py]\n\t\telse:\n\t\t\tshifted_test_data = shifted_data_dict['varying_joint'][py]\n\t\tbatch_size = params['batch_size']\n\t\tdef eval_input_fn():\n\t\t\teval_shift_dataset = tf.data.Dataset.from_tensor_slices(shifted_test_data)\n\t\t\teval_shift_dataset = eval_shift_dataset.map(map_to_image_label_wrapper)\n\t\t\teval_shift_dataset = eval_shift_dataset.batch(batch_size).repeat(1)",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tshifted_test_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\t\t\tshifted_test_data = shifted_data_dict['fixed_joint_0.5'][py]\n\t\telse:\n\t\t\tshifted_test_data = shifted_data_dict['varying_joint'][py]\n\t\tbatch_size = params['batch_size']\n\t\tdef eval_input_fn():\n\t\t\teval_shift_dataset = tf.data.Dataset.from_tensor_slices(shifted_test_data)\n\t\t\teval_shift_dataset = eval_shift_dataset.map(map_to_image_label_wrapper)\n\t\t\teval_shift_dataset = eval_shift_dataset.batch(batch_size).repeat(1)\n\t\t\treturn eval_shift_dataset\n\t\treturn eval_input_fn",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\t\tshifted_test_data",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\t\tshifted_test_data = shifted_data_dict['varying_joint'][py]\n\t\tbatch_size = params['batch_size']\n\t\tdef eval_input_fn():\n\t\t\teval_shift_dataset = tf.data.Dataset.from_tensor_slices(shifted_test_data)\n\t\t\teval_shift_dataset = eval_shift_dataset.map(map_to_image_label_wrapper)\n\t\t\teval_shift_dataset = eval_shift_dataset.batch(batch_size).repeat(1)\n\t\t\treturn eval_shift_dataset\n\t\treturn eval_input_fn\n\treturn train_data_size, train_input_fn, valid_input_fn, eval_input_fn_creater",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\tbatch_size",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\tbatch_size = params['batch_size']\n\t\tdef eval_input_fn():\n\t\t\teval_shift_dataset = tf.data.Dataset.from_tensor_slices(shifted_test_data)\n\t\t\teval_shift_dataset = eval_shift_dataset.map(map_to_image_label_wrapper)\n\t\t\teval_shift_dataset = eval_shift_dataset.batch(batch_size).repeat(1)\n\t\t\treturn eval_shift_dataset\n\t\treturn eval_input_fn\n\treturn train_data_size, train_input_fn, valid_input_fn, eval_input_fn_creater",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\t\teval_shift_dataset",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\t\teval_shift_dataset = tf.data.Dataset.from_tensor_slices(shifted_test_data)\n\t\t\teval_shift_dataset = eval_shift_dataset.map(map_to_image_label_wrapper)\n\t\t\teval_shift_dataset = eval_shift_dataset.batch(batch_size).repeat(1)\n\t\t\treturn eval_shift_dataset\n\t\treturn eval_input_fn\n\treturn train_data_size, train_input_fn, valid_input_fn, eval_input_fn_creater",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\t\teval_shift_dataset",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\t\teval_shift_dataset = eval_shift_dataset.map(map_to_image_label_wrapper)\n\t\t\teval_shift_dataset = eval_shift_dataset.batch(batch_size).repeat(1)\n\t\t\treturn eval_shift_dataset\n\t\treturn eval_input_fn\n\treturn train_data_size, train_input_fn, valid_input_fn, eval_input_fn_creater",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\t\t\teval_shift_dataset",
        "kind": 5,
        "importPath": "chexpert_support_device.data_builder",
        "description": "chexpert_support_device.data_builder",
        "peekOfCode": "\t\t\teval_shift_dataset = eval_shift_dataset.batch(batch_size).repeat(1)\n\t\t\treturn eval_shift_dataset\n\t\treturn eval_input_fn\n\treturn train_data_size, train_input_fn, valid_input_fn, eval_input_fn_creater",
        "detail": "chexpert_support_device.data_builder",
        "documentation": {}
    },
    {
        "label": "\tmove_old",
        "kind": 5,
        "importPath": "chexpert_support_device.launch_tensorboard",
        "description": "chexpert_support_device.launch_tensorboard",
        "peekOfCode": "\tmove_old = False\n\tscratch_dir = '/scratch/mmakar_root/mmakar0/mmakar/multiple_shortcut/chexpert/tuning'\n\tbashCommand = 'tensorboard --port 8088 --host localhost --logdir_spec '\n\tfor alpha in [0.0, 100.0, 1000.0, 10000.0]:\n\t\tfor sigma in [10.0, 100.0, 1000.0]:\n\t\t\tif (alpha == 0.0) and sigma !=10.0:\n\t\t\t\tcontinue\n\t\t\tparam_dict = {\n\t\t\t\t'random_seed': 0,\n\t\t\t\t'pixel': 128,",
        "detail": "chexpert_support_device.launch_tensorboard",
        "documentation": {}
    },
    {
        "label": "\tscratch_dir",
        "kind": 5,
        "importPath": "chexpert_support_device.launch_tensorboard",
        "description": "chexpert_support_device.launch_tensorboard",
        "peekOfCode": "\tscratch_dir = '/scratch/mmakar_root/mmakar0/mmakar/multiple_shortcut/chexpert/tuning'\n\tbashCommand = 'tensorboard --port 8088 --host localhost --logdir_spec '\n\tfor alpha in [0.0, 100.0, 1000.0, 10000.0]:\n\t\tfor sigma in [10.0, 100.0, 1000.0]:\n\t\t\tif (alpha == 0.0) and sigma !=10.0:\n\t\t\t\tcontinue\n\t\t\tparam_dict = {\n\t\t\t\t'random_seed': 0,\n\t\t\t\t'pixel': 128,\n\t\t\t\t'l2_penalty': 0.0,",
        "detail": "chexpert_support_device.launch_tensorboard",
        "documentation": {}
    },
    {
        "label": "\tbashCommand",
        "kind": 5,
        "importPath": "chexpert_support_device.launch_tensorboard",
        "description": "chexpert_support_device.launch_tensorboard",
        "peekOfCode": "\tbashCommand = 'tensorboard --port 8088 --host localhost --logdir_spec '\n\tfor alpha in [0.0, 100.0, 1000.0, 10000.0]:\n\t\tfor sigma in [10.0, 100.0, 1000.0]:\n\t\t\tif (alpha == 0.0) and sigma !=10.0:\n\t\t\t\tcontinue\n\t\t\tparam_dict = {\n\t\t\t\t'random_seed': 0,\n\t\t\t\t'pixel': 128,\n\t\t\t\t'l2_penalty': 0.0,\n\t\t\t\t'embedding_dim': -1,",
        "detail": "chexpert_support_device.launch_tensorboard",
        "documentation": {}
    },
    {
        "label": "\t\t\tparam_dict",
        "kind": 5,
        "importPath": "chexpert_support_device.launch_tensorboard",
        "description": "chexpert_support_device.launch_tensorboard",
        "peekOfCode": "\t\t\tparam_dict = {\n\t\t\t\t'random_seed': 0,\n\t\t\t\t'pixel': 128,\n\t\t\t\t'l2_penalty': 0.0,\n\t\t\t\t'embedding_dim': -1,\n\t\t\t\t'sigma': sigma,\n\t\t\t\t'alpha': alpha,\n\t\t\t\t\"architecture\": \"pretrained_densenet\",\n\t\t\t\t\"batch_size\": 64,\n\t\t\t\t'weighted': 'False',",
        "detail": "chexpert_support_device.launch_tensorboard",
        "documentation": {}
    },
    {
        "label": "\t\t\tconfig",
        "kind": 5,
        "importPath": "chexpert_support_device.launch_tensorboard",
        "description": "chexpert_support_device.launch_tensorboard",
        "peekOfCode": "\t\t\tconfig = collections.OrderedDict(sorted(param_dict.items()))\n\t\t\thash_string = utils.config_hasher(config)\n\t\t\tbashCommand = bashCommand + (\n\t\t\t\tf'a{alpha}_s{sigma}:{scratch_dir}/{hash_string},'\n\t\t\t)\n\tprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n\toutput, error = process.communicate()",
        "detail": "chexpert_support_device.launch_tensorboard",
        "documentation": {}
    },
    {
        "label": "\t\t\thash_string",
        "kind": 5,
        "importPath": "chexpert_support_device.launch_tensorboard",
        "description": "chexpert_support_device.launch_tensorboard",
        "peekOfCode": "\t\t\thash_string = utils.config_hasher(config)\n\t\t\tbashCommand = bashCommand + (\n\t\t\t\tf'a{alpha}_s{sigma}:{scratch_dir}/{hash_string},'\n\t\t\t)\n\tprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n\toutput, error = process.communicate()",
        "detail": "chexpert_support_device.launch_tensorboard",
        "documentation": {}
    },
    {
        "label": "\t\t\tbashCommand",
        "kind": 5,
        "importPath": "chexpert_support_device.launch_tensorboard",
        "description": "chexpert_support_device.launch_tensorboard",
        "peekOfCode": "\t\t\tbashCommand = bashCommand + (\n\t\t\t\tf'a{alpha}_s{sigma}:{scratch_dir}/{hash_string},'\n\t\t\t)\n\tprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n\toutput, error = process.communicate()",
        "detail": "chexpert_support_device.launch_tensorboard",
        "documentation": {}
    },
    {
        "label": "\tprocess",
        "kind": 5,
        "importPath": "chexpert_support_device.launch_tensorboard",
        "description": "chexpert_support_device.launch_tensorboard",
        "peekOfCode": "\tprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n\toutput, error = process.communicate()",
        "detail": "chexpert_support_device.launch_tensorboard",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "chexpert_support_device.main",
        "description": "chexpert_support_device.main",
        "peekOfCode": "def main(argv):\n\tdel argv\n\tdef dataset_builder():\n\t\treturn data_builder.build_input_fns(\n\t\t\tchexpert_data_dir=FLAGS.data_dir,\n\t\t\tskew_train=FLAGS.skew_train,\n\t\t\tweighted=FLAGS.weighted,\n\t\t\tp_tr=FLAGS.p_tr,\n\t\t\tp_val=FLAGS.p_val,\n\t\t\trandom_seed=FLAGS.random_seed)",
        "detail": "chexpert_support_device.main",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "chexpert_support_device.main",
        "description": "chexpert_support_device.main",
        "peekOfCode": "FLAGS = flags.FLAGS\nflags.DEFINE_string('skew_train', 'False', 'train on skewed data?')\nflags.DEFINE_float('p_tr', .7, 'proportion of data used for training.')\nflags.DEFINE_float('p_val', .25,\n\t'proportion of training data used for validation.')\nflags.DEFINE_integer('pixel', 128,\n\t'number of pixels in the image (i.e., resolution).')\nflags.DEFINE_string('data_dir', '/datadir/',\n\t\t\t\t\t\t\t\t\t\t'Directory of saved data.')\nflags.DEFINE_string('exp_dir', '/mydir/',",
        "detail": "chexpert_support_device.main",
        "documentation": {}
    },
    {
        "label": "\tpy1_y0_shift_list",
        "kind": 5,
        "importPath": "chexpert_support_device.main",
        "description": "chexpert_support_device.main",
        "peekOfCode": "\tpy1_y0_shift_list = [0.1, 0.5, 0.9]\n\ttrain.train(\n\t\texp_dir=FLAGS.exp_dir,\n\t\tcheckpoint_dir=FLAGS.checkpoint_dir,\n\t\tdataset_builder=dataset_builder,\n\t\tarchitecture=FLAGS.architecture,\n\t\ttraining_steps=FLAGS.training_steps,\n\t\tpixel=FLAGS.pixel,\n\t\tnum_epochs=FLAGS.num_epochs,\n\t\tbatch_size=FLAGS.batch_size,",
        "detail": "chexpert_support_device.main",
        "documentation": {}
    },
    {
        "label": "get_simple_weights",
        "kind": 2,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "def get_simple_weights(data):\n\t# --- load data\n\tdata = data['0'].str.split(\",\", expand=True)\n\tdata.columns = ['file_name', 'y0', 'y1', 'y2']\n\tdata['y0'] = data.y0.astype(np.float32)\n\tdata['y1'] = data.y1.astype(np.float32)\n\tdata['y2'] = data.y2.astype(np.float32)\n\tdata['weights'] = 0.0\n\t# --- compute weights\n\tfor y0_val in [0, 1]:",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\tdata",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\tdata = data['0'].str.split(\",\", expand=True)\n\tdata.columns = ['file_name', 'y0', 'y1', 'y2']\n\tdata['y0'] = data.y0.astype(np.float32)\n\tdata['y1'] = data.y1.astype(np.float32)\n\tdata['y2'] = data.y2.astype(np.float32)\n\tdata['weights'] = 0.0\n\t# --- compute weights\n\tfor y0_val in [0, 1]:\n\t\t\tfor y1_val in [0, 1]:\n\t\t\t\t\tfor y2_val in [0, 1]:",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\tdata.columns",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\tdata.columns = ['file_name', 'y0', 'y1', 'y2']\n\tdata['y0'] = data.y0.astype(np.float32)\n\tdata['y1'] = data.y1.astype(np.float32)\n\tdata['y2'] = data.y2.astype(np.float32)\n\tdata['weights'] = 0.0\n\t# --- compute weights\n\tfor y0_val in [0, 1]:\n\t\t\tfor y1_val in [0, 1]:\n\t\t\t\t\tfor y2_val in [0, 1]:\n\t\t\t\t\t\t\tmask = (data.y0 == y0_val) * (data.y1 == y1_val) * (data.y2 == y2_val) * 1.0",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\tdata['y0']",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\tdata['y0'] = data.y0.astype(np.float32)\n\tdata['y1'] = data.y1.astype(np.float32)\n\tdata['y2'] = data.y2.astype(np.float32)\n\tdata['weights'] = 0.0\n\t# --- compute weights\n\tfor y0_val in [0, 1]:\n\t\t\tfor y1_val in [0, 1]:\n\t\t\t\t\tfor y2_val in [0, 1]:\n\t\t\t\t\t\t\tmask = (data.y0 == y0_val) * (data.y1 == y1_val) * (data.y2 == y2_val) * 1.0\n\t\t\t\t\t\t\tdenom = np.mean(mask)",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\tdata['y1']",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\tdata['y1'] = data.y1.astype(np.float32)\n\tdata['y2'] = data.y2.astype(np.float32)\n\tdata['weights'] = 0.0\n\t# --- compute weights\n\tfor y0_val in [0, 1]:\n\t\t\tfor y1_val in [0, 1]:\n\t\t\t\t\tfor y2_val in [0, 1]:\n\t\t\t\t\t\t\tmask = (data.y0 == y0_val) * (data.y1 == y1_val) * (data.y2 == y2_val) * 1.0\n\t\t\t\t\t\t\tdenom = np.mean(mask)\n\t\t\t\t\t\t\tnum = np.mean((data.y0 == y0_val)) * np.mean((data.y1 == y1_val))  * np.mean((data.y2 == y2_val))",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\tdata['y2']",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\tdata['y2'] = data.y2.astype(np.float32)\n\tdata['weights'] = 0.0\n\t# --- compute weights\n\tfor y0_val in [0, 1]:\n\t\t\tfor y1_val in [0, 1]:\n\t\t\t\t\tfor y2_val in [0, 1]:\n\t\t\t\t\t\t\tmask = (data.y0 == y0_val) * (data.y1 == y1_val) * (data.y2 == y2_val) * 1.0\n\t\t\t\t\t\t\tdenom = np.mean(mask)\n\t\t\t\t\t\t\tnum = np.mean((data.y0 == y0_val)) * np.mean((data.y1 == y1_val))  * np.mean((data.y2 == y2_val))\n\t\t\t\t\t\t\tdata['weights'] = mask * (num/denom) + (1 - mask) * data['weights']",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\tdata['weights']",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\tdata['weights'] = 0.0\n\t# --- compute weights\n\tfor y0_val in [0, 1]:\n\t\t\tfor y1_val in [0, 1]:\n\t\t\t\t\tfor y2_val in [0, 1]:\n\t\t\t\t\t\t\tmask = (data.y0 == y0_val) * (data.y1 == y1_val) * (data.y2 == y2_val) * 1.0\n\t\t\t\t\t\t\tdenom = np.mean(mask)\n\t\t\t\t\t\t\tnum = np.mean((data.y0 == y0_val)) * np.mean((data.y1 == y1_val))  * np.mean((data.y2 == y2_val))\n\t\t\t\t\t\t\tdata['weights'] = mask * (num/denom) + (1 - mask) * data['weights']\n\tdata = data.file_name + \\",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\t\tmask",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\t\t\t\t\t\t\tmask = (data.y0 == y0_val) * (data.y1 == y1_val) * (data.y2 == y2_val) * 1.0\n\t\t\t\t\t\t\tdenom = np.mean(mask)\n\t\t\t\t\t\t\tnum = np.mean((data.y0 == y0_val)) * np.mean((data.y1 == y1_val))  * np.mean((data.y2 == y2_val))\n\t\t\t\t\t\t\tdata['weights'] = mask * (num/denom) + (1 - mask) * data['weights']\n\tdata = data.file_name + \\\n\t\t\t',' + data.y0.astype(str) + \\\n\t\t\t',' + data.y1.astype(str) + \\\n\t\t\t',' + data.y2.astype(str) + \\\n\t\t\t',' + data.weights.astype(str)\n\tdata = data.apply(lambda x: [x])",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\t\tdenom",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\t\t\t\t\t\t\tdenom = np.mean(mask)\n\t\t\t\t\t\t\tnum = np.mean((data.y0 == y0_val)) * np.mean((data.y1 == y1_val))  * np.mean((data.y2 == y2_val))\n\t\t\t\t\t\t\tdata['weights'] = mask * (num/denom) + (1 - mask) * data['weights']\n\tdata = data.file_name + \\\n\t\t\t',' + data.y0.astype(str) + \\\n\t\t\t',' + data.y1.astype(str) + \\\n\t\t\t',' + data.y2.astype(str) + \\\n\t\t\t',' + data.weights.astype(str)\n\tdata = data.apply(lambda x: [x])\n\treturn data",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\t\tnum",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\t\t\t\t\t\t\tnum = np.mean((data.y0 == y0_val)) * np.mean((data.y1 == y1_val))  * np.mean((data.y2 == y2_val))\n\t\t\t\t\t\t\tdata['weights'] = mask * (num/denom) + (1 - mask) * data['weights']\n\tdata = data.file_name + \\\n\t\t\t',' + data.y0.astype(str) + \\\n\t\t\t',' + data.y1.astype(str) + \\\n\t\t\t',' + data.y2.astype(str) + \\\n\t\t\t',' + data.weights.astype(str)\n\tdata = data.apply(lambda x: [x])\n\treturn data",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\t\t\tdata['weights']",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\t\t\t\t\t\t\tdata['weights'] = mask * (num/denom) + (1 - mask) * data['weights']\n\tdata = data.file_name + \\\n\t\t\t',' + data.y0.astype(str) + \\\n\t\t\t',' + data.y1.astype(str) + \\\n\t\t\t',' + data.y2.astype(str) + \\\n\t\t\t',' + data.weights.astype(str)\n\tdata = data.apply(lambda x: [x])\n\treturn data",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\tdata",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\tdata = data.file_name + \\\n\t\t\t',' + data.y0.astype(str) + \\\n\t\t\t',' + data.y1.astype(str) + \\\n\t\t\t',' + data.y2.astype(str) + \\\n\t\t\t',' + data.weights.astype(str)\n\tdata = data.apply(lambda x: [x])\n\treturn data",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "\tdata",
        "kind": 5,
        "importPath": "chexpert_support_device.weighting",
        "description": "chexpert_support_device.weighting",
        "peekOfCode": "\tdata = data.apply(lambda x: [x])\n\treturn data",
        "detail": "chexpert_support_device.weighting",
        "documentation": {}
    },
    {
        "label": "PretrainedDenseNet121",
        "kind": 6,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "class PretrainedDenseNet121(tf.keras.Model):\n\t\"\"\"pretrained Densenet architecture.\"\"\"\n\tdef __init__(self, embedding_dim=-1, l2_penalty=0.0,\n\t\tl2_penalty_last_only=False):\n\t\tsuper(PretrainedDenseNet121, self).__init__()\n\t\tself.embedding_dim = embedding_dim\n\t\tself.densenet = DenseNet121(include_top=False,\n\t\t\tweights='imagenet')\n\t\tself.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n\t\tif not l2_penalty_last_only:",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "create_architecture",
        "kind": 2,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "def create_architecture(params):\n\tif (params['architecture'] == 'pretrained_densenet'):\n\t\tnet = PretrainedDenseNet121(\n\t\t\tembedding_dim=params[\"embedding_dim\"],\n\t\t\tl2_penalty=params[\"l2_penalty\"])\n\telse:\n\t\traise NotImplementedError(\n\t\t\t\"need to implement other architectures\")\n\treturn net\nclass PretrainedDenseNet121(tf.keras.Model):",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\tnet",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\tnet = PretrainedDenseNet121(\n\t\t\tembedding_dim=params[\"embedding_dim\"],\n\t\t\tl2_penalty=params[\"l2_penalty\"])\n\telse:\n\t\traise NotImplementedError(\n\t\t\t\"need to implement other architectures\")\n\treturn net\nclass PretrainedDenseNet121(tf.keras.Model):\n\t\"\"\"pretrained Densenet architecture.\"\"\"\n\tdef __init__(self, embedding_dim=-1, l2_penalty=0.0,",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\tself.embedding_dim",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\tself.embedding_dim = embedding_dim\n\t\tself.densenet = DenseNet121(include_top=False,\n\t\t\tweights='imagenet')\n\t\tself.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n\t\tif not l2_penalty_last_only:\n\t\t\tregularizer = tf.keras.regularizers.l2(l2_penalty)\n\t\t\tfor layer in self.densenet.layers:\n\t\t\t\tif hasattr(layer, 'kernel'):\n\t\t\t\t\tself.add_loss(lambda layer=layer: regularizer(layer.kernel))\n\t\tif self.embedding_dim != -1:",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\tself.densenet",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\tself.densenet = DenseNet121(include_top=False,\n\t\t\tweights='imagenet')\n\t\tself.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n\t\tif not l2_penalty_last_only:\n\t\t\tregularizer = tf.keras.regularizers.l2(l2_penalty)\n\t\t\tfor layer in self.densenet.layers:\n\t\t\t\tif hasattr(layer, 'kernel'):\n\t\t\t\t\tself.add_loss(lambda layer=layer: regularizer(layer.kernel))\n\t\tif self.embedding_dim != -1:\n\t\t\tself.embedding = tf.keras.layers.Dense(self.embedding_dim,",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\tself.avg_pool",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\tself.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n\t\tif not l2_penalty_last_only:\n\t\t\tregularizer = tf.keras.regularizers.l2(l2_penalty)\n\t\t\tfor layer in self.densenet.layers:\n\t\t\t\tif hasattr(layer, 'kernel'):\n\t\t\t\t\tself.add_loss(lambda layer=layer: regularizer(layer.kernel))\n\t\tif self.embedding_dim != -1:\n\t\t\tself.embedding = tf.keras.layers.Dense(self.embedding_dim,\n\t\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t\tself.dense = tf.keras.layers.Dense(1,",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\t\tregularizer",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\t\tregularizer = tf.keras.regularizers.l2(l2_penalty)\n\t\t\tfor layer in self.densenet.layers:\n\t\t\t\tif hasattr(layer, 'kernel'):\n\t\t\t\t\tself.add_loss(lambda layer=layer: regularizer(layer.kernel))\n\t\tif self.embedding_dim != -1:\n\t\t\tself.embedding = tf.keras.layers.Dense(self.embedding_dim,\n\t\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t\tself.dense = tf.keras.layers.Dense(1,\n\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t@tf.function",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\t\tself.embedding",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\t\tself.embedding = tf.keras.layers.Dense(self.embedding_dim,\n\t\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t\tself.dense = tf.keras.layers.Dense(1,\n\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t@tf.function\n\tdef call(self, inputs, training=False):\n\t\tx = self.densenet(inputs, training)\n\t\tx = self.avg_pool(x)\n\t\tif self.embedding_dim != -1:\n\t\t\tx = self.embedding(x)",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\tself.dense",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\tself.dense = tf.keras.layers.Dense(1,\n\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t@tf.function\n\tdef call(self, inputs, training=False):\n\t\tx = self.densenet(inputs, training)\n\t\tx = self.avg_pool(x)\n\t\tif self.embedding_dim != -1:\n\t\t\tx = self.embedding(x)\n\t\treturn self.dense(x), x",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\tx",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\tx = self.densenet(inputs, training)\n\t\tx = self.avg_pool(x)\n\t\tif self.embedding_dim != -1:\n\t\t\tx = self.embedding(x)\n\t\treturn self.dense(x), x",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\tx",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\tx = self.avg_pool(x)\n\t\tif self.embedding_dim != -1:\n\t\t\tx = self.embedding(x)\n\t\treturn self.dense(x), x",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "\t\t\tx",
        "kind": 5,
        "importPath": "shared.architectures",
        "description": "shared.architectures",
        "peekOfCode": "\t\t\tx = self.embedding(x)\n\t\treturn self.dense(x), x",
        "detail": "shared.architectures",
        "documentation": {}
    },
    {
        "label": "import_helper",
        "kind": 2,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "def import_helper(config, base_dir):\n\t\"\"\"Imports the dictionary with the results of an experiment.\n\tArgs:\n\t\targs: tuple with model, config where\n\t\t\tmodel: str, name of the model we're importing the performance of\n\t\t\tconfig: dictionary, expected to have the following: exp_dir, the experiment\n\t\t\t\tdirectory random_seed,  random seed for the experiment py1_y0_s,\n\t\t\t\tprobability of y1=1| y0=1 in the shifted test distribution alpha,\n\t\t\t\thsic/cross prediction penalty sigma,  kernel bandwidth for the hsic penalty\n\t\t\t\tl2_penalty,  regularization parameter dropout_rate,  drop out rate",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "import_results",
        "kind": 2,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "def import_results(configs, num_workers, base_dir):\n\timport_helper_wrapper = functools.partial(import_helper, base_dir=base_dir)\n\tpool = multiprocessing.Pool(num_workers)\n\tres = []\n\tfor config_res in tqdm.tqdm(pool.imap_unordered(import_helper_wrapper,\n\t\tconfigs), total=len(configs)):\n\t\tres.append(config_res)\n\tres = pd.concat(res, axis=0, ignore_index=True, sort=False)\n\treturn res, configs\ndef reshape_results(results):",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "reshape_results",
        "kind": 2,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "def reshape_results(results):\n\tshift_columns = [col for col in results.columns if 'shift' in col]\n\tshift_metrics_columns = [\n\t\tcol for col in shift_columns if ('pred_loss' in col) or ('accuracy' in col) or ('auc' in col)\n\t]\n\t# shift_metrics_columns = shift_metrics_columns + [\n\t# \tf'shift_{py}_hsic' for py in [0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9]]\n\tresults = results[shift_metrics_columns]\n\tresults = results.transpose()\n\tresults['py1_y0_s'] = results.index.str[6:10]",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "get_optimal_model_results",
        "kind": 2,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "def get_optimal_model_results(mode, configs, base_dir, hparams,\n\t num_workers, pval):\n\tif mode not in ['classic', 'two_step']:\n\t\traise NotImplementedError('Can only run classic or two_step modes')\n\tif mode == 'classic':\n\t\treturn get_optimal_model_classic(configs, None, base_dir, hparams, num_workers)\n\telif mode =='two_step':\n\t\treturn get_optimal_model_two_step(configs, base_dir, hparams, pval, num_workers)\ndef get_optimal_model_two_step(configs, base_dir, hparams, pval, num_workers):\n\tall_results, available_configs = import_results(configs, num_workers, base_dir)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "get_optimal_model_two_step",
        "kind": 2,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "def get_optimal_model_two_step(configs, base_dir, hparams, pval, num_workers):\n\tall_results, available_configs = import_results(configs, num_workers, base_dir)\n\tsigma_results = get_sigma.get_optimal_sigma(available_configs, kfolds=3,\n\t\tnum_workers=0, base_dir=base_dir)\n\tprint(\"this is sig res\")\n\tprint(sigma_results.sort_values(['random_seed', 'sigma', 'alpha']))\n\tbest_pval = sigma_results.groupby('random_seed').pval.max()\n\tbest_pval = best_pval.to_frame()\n\tbest_pval.reset_index(inplace=True, drop=False)\n\tbest_pval.rename(columns={'pval': 'best_pval'}, inplace=True)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "get_optimal_model_classic",
        "kind": 2,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "def get_optimal_model_classic(configs, filtered_results, base_dir, hparams, num_workers):\n\tif ((configs is None) and (filtered_results is None)):\n\t\traise ValueError(\"Need either configs or table of results_dict\")\n\tif configs is not None:\n\t\tprint(\"getting results\")\n\t\tall_results, _ = import_results(configs, num_workers, base_dir)\n\telse:\n\t\tall_results = filtered_results.copy()\n\tcolumns_to_keep = hparams + ['random_seed', 'validation_pred_loss']\n\tbest_loss = all_results[columns_to_keep]",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\thash_string",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\thash_string = config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\tif not os.path.exists(performance_file):\n\t\tlogging.error('Couldnt find %s', performance_file)\n\t\treturn None\n\tresults_dict = pickle.load(open(performance_file, 'rb'))\n\tresults_dict.update(config)\n\tresults_dict['hash'] = hash_string\n\treturn pd.DataFrame(results_dict, index=[0])",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\thash_dir",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\tif not os.path.exists(performance_file):\n\t\tlogging.error('Couldnt find %s', performance_file)\n\t\treturn None\n\tresults_dict = pickle.load(open(performance_file, 'rb'))\n\tresults_dict.update(config)\n\tresults_dict['hash'] = hash_string\n\treturn pd.DataFrame(results_dict, index=[0])\ndef import_results(configs, num_workers, base_dir):",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tperformance_file",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\tif not os.path.exists(performance_file):\n\t\tlogging.error('Couldnt find %s', performance_file)\n\t\treturn None\n\tresults_dict = pickle.load(open(performance_file, 'rb'))\n\tresults_dict.update(config)\n\tresults_dict['hash'] = hash_string\n\treturn pd.DataFrame(results_dict, index=[0])\ndef import_results(configs, num_workers, base_dir):\n\timport_helper_wrapper = functools.partial(import_helper, base_dir=base_dir)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults_dict",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults_dict = pickle.load(open(performance_file, 'rb'))\n\tresults_dict.update(config)\n\tresults_dict['hash'] = hash_string\n\treturn pd.DataFrame(results_dict, index=[0])\ndef import_results(configs, num_workers, base_dir):\n\timport_helper_wrapper = functools.partial(import_helper, base_dir=base_dir)\n\tpool = multiprocessing.Pool(num_workers)\n\tres = []\n\tfor config_res in tqdm.tqdm(pool.imap_unordered(import_helper_wrapper,\n\t\tconfigs), total=len(configs)):",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults_dict['hash']",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults_dict['hash'] = hash_string\n\treturn pd.DataFrame(results_dict, index=[0])\ndef import_results(configs, num_workers, base_dir):\n\timport_helper_wrapper = functools.partial(import_helper, base_dir=base_dir)\n\tpool = multiprocessing.Pool(num_workers)\n\tres = []\n\tfor config_res in tqdm.tqdm(pool.imap_unordered(import_helper_wrapper,\n\t\tconfigs), total=len(configs)):\n\t\tres.append(config_res)\n\tres = pd.concat(res, axis=0, ignore_index=True, sort=False)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\timport_helper_wrapper",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\timport_helper_wrapper = functools.partial(import_helper, base_dir=base_dir)\n\tpool = multiprocessing.Pool(num_workers)\n\tres = []\n\tfor config_res in tqdm.tqdm(pool.imap_unordered(import_helper_wrapper,\n\t\tconfigs), total=len(configs)):\n\t\tres.append(config_res)\n\tres = pd.concat(res, axis=0, ignore_index=True, sort=False)\n\treturn res, configs\ndef reshape_results(results):\n\tshift_columns = [col for col in results.columns if 'shift' in col]",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tpool",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tpool = multiprocessing.Pool(num_workers)\n\tres = []\n\tfor config_res in tqdm.tqdm(pool.imap_unordered(import_helper_wrapper,\n\t\tconfigs), total=len(configs)):\n\t\tres.append(config_res)\n\tres = pd.concat(res, axis=0, ignore_index=True, sort=False)\n\treturn res, configs\ndef reshape_results(results):\n\tshift_columns = [col for col in results.columns if 'shift' in col]\n\tshift_metrics_columns = [",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tres",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tres = []\n\tfor config_res in tqdm.tqdm(pool.imap_unordered(import_helper_wrapper,\n\t\tconfigs), total=len(configs)):\n\t\tres.append(config_res)\n\tres = pd.concat(res, axis=0, ignore_index=True, sort=False)\n\treturn res, configs\ndef reshape_results(results):\n\tshift_columns = [col for col in results.columns if 'shift' in col]\n\tshift_metrics_columns = [\n\t\tcol for col in shift_columns if ('pred_loss' in col) or ('accuracy' in col) or ('auc' in col)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tres",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tres = pd.concat(res, axis=0, ignore_index=True, sort=False)\n\treturn res, configs\ndef reshape_results(results):\n\tshift_columns = [col for col in results.columns if 'shift' in col]\n\tshift_metrics_columns = [\n\t\tcol for col in shift_columns if ('pred_loss' in col) or ('accuracy' in col) or ('auc' in col)\n\t]\n\t# shift_metrics_columns = shift_metrics_columns + [\n\t# \tf'shift_{py}_hsic' for py in [0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9]]\n\tresults = results[shift_metrics_columns]",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tshift_columns",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tshift_columns = [col for col in results.columns if 'shift' in col]\n\tshift_metrics_columns = [\n\t\tcol for col in shift_columns if ('pred_loss' in col) or ('accuracy' in col) or ('auc' in col)\n\t]\n\t# shift_metrics_columns = shift_metrics_columns + [\n\t# \tf'shift_{py}_hsic' for py in [0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9]]\n\tresults = results[shift_metrics_columns]\n\tresults = results.transpose()\n\tresults['py1_y0_s'] = results.index.str[6:10]\n\tresults['py1_y0_s'] = results.py1_y0_s.str.replace('_', '')",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tshift_metrics_columns",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tshift_metrics_columns = [\n\t\tcol for col in shift_columns if ('pred_loss' in col) or ('accuracy' in col) or ('auc' in col)\n\t]\n\t# shift_metrics_columns = shift_metrics_columns + [\n\t# \tf'shift_{py}_hsic' for py in [0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9]]\n\tresults = results[shift_metrics_columns]\n\tresults = results.transpose()\n\tresults['py1_y0_s'] = results.index.str[6:10]\n\tresults['py1_y0_s'] = results.py1_y0_s.str.replace('_', '')\n\tresults['py1_y0_s'] = results.py1_y0_s.astype(float)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults = results[shift_metrics_columns]\n\tresults = results.transpose()\n\tresults['py1_y0_s'] = results.index.str[6:10]\n\tresults['py1_y0_s'] = results.py1_y0_s.str.replace('_', '')\n\tresults['py1_y0_s'] = results.py1_y0_s.astype(float)\n\tresults_auc = results[(results.index.str.contains('auc'))]\n\tresults_auc = results_auc.rename(columns={\n\t\tcol: f'auc_{col}' for col in results_auc.columns if col != 'py1_y0_s'\n\t})\n\tresults_loss = results[(results.index.str.contains('pred_loss'))]",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults = results.transpose()\n\tresults['py1_y0_s'] = results.index.str[6:10]\n\tresults['py1_y0_s'] = results.py1_y0_s.str.replace('_', '')\n\tresults['py1_y0_s'] = results.py1_y0_s.astype(float)\n\tresults_auc = results[(results.index.str.contains('auc'))]\n\tresults_auc = results_auc.rename(columns={\n\t\tcol: f'auc_{col}' for col in results_auc.columns if col != 'py1_y0_s'\n\t})\n\tresults_loss = results[(results.index.str.contains('pred_loss'))]\n\tresults_loss = results_loss.rename(columns={",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults['py1_y0_s']",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults['py1_y0_s'] = results.index.str[6:10]\n\tresults['py1_y0_s'] = results.py1_y0_s.str.replace('_', '')\n\tresults['py1_y0_s'] = results.py1_y0_s.astype(float)\n\tresults_auc = results[(results.index.str.contains('auc'))]\n\tresults_auc = results_auc.rename(columns={\n\t\tcol: f'auc_{col}' for col in results_auc.columns if col != 'py1_y0_s'\n\t})\n\tresults_loss = results[(results.index.str.contains('pred_loss'))]\n\tresults_loss = results_loss.rename(columns={\n\t\tcol: f'loss_{col}' for col in results_loss.columns if col != 'py1_y0_s'",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults['py1_y0_s']",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults['py1_y0_s'] = results.py1_y0_s.str.replace('_', '')\n\tresults['py1_y0_s'] = results.py1_y0_s.astype(float)\n\tresults_auc = results[(results.index.str.contains('auc'))]\n\tresults_auc = results_auc.rename(columns={\n\t\tcol: f'auc_{col}' for col in results_auc.columns if col != 'py1_y0_s'\n\t})\n\tresults_loss = results[(results.index.str.contains('pred_loss'))]\n\tresults_loss = results_loss.rename(columns={\n\t\tcol: f'loss_{col}' for col in results_loss.columns if col != 'py1_y0_s'\n\t})",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults['py1_y0_s']",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults['py1_y0_s'] = results.py1_y0_s.astype(float)\n\tresults_auc = results[(results.index.str.contains('auc'))]\n\tresults_auc = results_auc.rename(columns={\n\t\tcol: f'auc_{col}' for col in results_auc.columns if col != 'py1_y0_s'\n\t})\n\tresults_loss = results[(results.index.str.contains('pred_loss'))]\n\tresults_loss = results_loss.rename(columns={\n\t\tcol: f'loss_{col}' for col in results_loss.columns if col != 'py1_y0_s'\n\t})\n\tresults_final = results_auc.merge(results_loss, on=['py1_y0_s'])",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults_auc",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults_auc = results[(results.index.str.contains('auc'))]\n\tresults_auc = results_auc.rename(columns={\n\t\tcol: f'auc_{col}' for col in results_auc.columns if col != 'py1_y0_s'\n\t})\n\tresults_loss = results[(results.index.str.contains('pred_loss'))]\n\tresults_loss = results_loss.rename(columns={\n\t\tcol: f'loss_{col}' for col in results_loss.columns if col != 'py1_y0_s'\n\t})\n\tresults_final = results_auc.merge(results_loss, on=['py1_y0_s'])\n\tprint(results_final)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults_auc",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults_auc = results_auc.rename(columns={\n\t\tcol: f'auc_{col}' for col in results_auc.columns if col != 'py1_y0_s'\n\t})\n\tresults_loss = results[(results.index.str.contains('pred_loss'))]\n\tresults_loss = results_loss.rename(columns={\n\t\tcol: f'loss_{col}' for col in results_loss.columns if col != 'py1_y0_s'\n\t})\n\tresults_final = results_auc.merge(results_loss, on=['py1_y0_s'])\n\tprint(results_final)\n\treturn results_final",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults_loss",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults_loss = results[(results.index.str.contains('pred_loss'))]\n\tresults_loss = results_loss.rename(columns={\n\t\tcol: f'loss_{col}' for col in results_loss.columns if col != 'py1_y0_s'\n\t})\n\tresults_final = results_auc.merge(results_loss, on=['py1_y0_s'])\n\tprint(results_final)\n\treturn results_final\ndef get_optimal_model_results(mode, configs, base_dir, hparams,\n\t num_workers, pval):\n\tif mode not in ['classic', 'two_step']:",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults_loss",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults_loss = results_loss.rename(columns={\n\t\tcol: f'loss_{col}' for col in results_loss.columns if col != 'py1_y0_s'\n\t})\n\tresults_final = results_auc.merge(results_loss, on=['py1_y0_s'])\n\tprint(results_final)\n\treturn results_final\ndef get_optimal_model_results(mode, configs, base_dir, hparams,\n\t num_workers, pval):\n\tif mode not in ['classic', 'two_step']:\n\t\traise NotImplementedError('Can only run classic or two_step modes')",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tresults_final",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tresults_final = results_auc.merge(results_loss, on=['py1_y0_s'])\n\tprint(results_final)\n\treturn results_final\ndef get_optimal_model_results(mode, configs, base_dir, hparams,\n\t num_workers, pval):\n\tif mode not in ['classic', 'two_step']:\n\t\traise NotImplementedError('Can only run classic or two_step modes')\n\tif mode == 'classic':\n\t\treturn get_optimal_model_classic(configs, None, base_dir, hparams, num_workers)\n\telif mode =='two_step':",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tsigma_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tsigma_results = get_sigma.get_optimal_sigma(available_configs, kfolds=3,\n\t\tnum_workers=0, base_dir=base_dir)\n\tprint(\"this is sig res\")\n\tprint(sigma_results.sort_values(['random_seed', 'sigma', 'alpha']))\n\tbest_pval = sigma_results.groupby('random_seed').pval.max()\n\tbest_pval = best_pval.to_frame()\n\tbest_pval.reset_index(inplace=True, drop=False)\n\tbest_pval.rename(columns={'pval': 'best_pval'}, inplace=True)\n\tsmallest_hsic = sigma_results.groupby('random_seed').hsic.min()\n\tsmallest_hsic = smallest_hsic.to_frame()",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tbest_pval",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tbest_pval = sigma_results.groupby('random_seed').pval.max()\n\tbest_pval = best_pval.to_frame()\n\tbest_pval.reset_index(inplace=True, drop=False)\n\tbest_pval.rename(columns={'pval': 'best_pval'}, inplace=True)\n\tsmallest_hsic = sigma_results.groupby('random_seed').hsic.min()\n\tsmallest_hsic = smallest_hsic.to_frame()\n\tsmallest_hsic.reset_index(inplace=True, drop=False)\n\tsmallest_hsic.rename(columns={'hsic': 'smallest_hsic'}, inplace=True)\n\tsigma_results = sigma_results.merge(best_pval, on ='random_seed')\n\tsigma_results = sigma_results.merge(smallest_hsic, on ='random_seed')",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tbest_pval",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tbest_pval = best_pval.to_frame()\n\tbest_pval.reset_index(inplace=True, drop=False)\n\tbest_pval.rename(columns={'pval': 'best_pval'}, inplace=True)\n\tsmallest_hsic = sigma_results.groupby('random_seed').hsic.min()\n\tsmallest_hsic = smallest_hsic.to_frame()\n\tsmallest_hsic.reset_index(inplace=True, drop=False)\n\tsmallest_hsic.rename(columns={'hsic': 'smallest_hsic'}, inplace=True)\n\tsigma_results = sigma_results.merge(best_pval, on ='random_seed')\n\tsigma_results = sigma_results.merge(smallest_hsic, on ='random_seed')\n\tfiltered_results = all_results.merge(sigma_results, on=['random_seed', 'sigma', 'alpha'])",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tsmallest_hsic",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tsmallest_hsic = sigma_results.groupby('random_seed').hsic.min()\n\tsmallest_hsic = smallest_hsic.to_frame()\n\tsmallest_hsic.reset_index(inplace=True, drop=False)\n\tsmallest_hsic.rename(columns={'hsic': 'smallest_hsic'}, inplace=True)\n\tsigma_results = sigma_results.merge(best_pval, on ='random_seed')\n\tsigma_results = sigma_results.merge(smallest_hsic, on ='random_seed')\n\tfiltered_results = all_results.merge(sigma_results, on=['random_seed', 'sigma', 'alpha'])\n\tfiltered_results = filtered_results[\n\t\t(((filtered_results.pval >= pval) &  (filtered_results.best_pval >= pval)) | \\\n\t\t((filtered_results.best_pval < pval) &  (filtered_results.hsic == filtered_results.smallest_hsic)))",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tsmallest_hsic",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tsmallest_hsic = smallest_hsic.to_frame()\n\tsmallest_hsic.reset_index(inplace=True, drop=False)\n\tsmallest_hsic.rename(columns={'hsic': 'smallest_hsic'}, inplace=True)\n\tsigma_results = sigma_results.merge(best_pval, on ='random_seed')\n\tsigma_results = sigma_results.merge(smallest_hsic, on ='random_seed')\n\tfiltered_results = all_results.merge(sigma_results, on=['random_seed', 'sigma', 'alpha'])\n\tfiltered_results = filtered_results[\n\t\t(((filtered_results.pval >= pval) &  (filtered_results.best_pval >= pval)) | \\\n\t\t((filtered_results.best_pval < pval) &  (filtered_results.hsic == filtered_results.smallest_hsic)))\n\t\t]",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tsigma_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tsigma_results = sigma_results.merge(best_pval, on ='random_seed')\n\tsigma_results = sigma_results.merge(smallest_hsic, on ='random_seed')\n\tfiltered_results = all_results.merge(sigma_results, on=['random_seed', 'sigma', 'alpha'])\n\tfiltered_results = filtered_results[\n\t\t(((filtered_results.pval >= pval) &  (filtered_results.best_pval >= pval)) | \\\n\t\t((filtered_results.best_pval < pval) &  (filtered_results.hsic == filtered_results.smallest_hsic)))\n\t\t]\n\tfiltered_results.drop(['pval', 'best_pval'], inplace=True, axis=1)\n\tfiltered_results.reset_index(drop=True, inplace=True)\n\tunique_filtered_results = filtered_results[['random_seed', 'sigma', 'alpha']].copy()",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tsigma_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tsigma_results = sigma_results.merge(smallest_hsic, on ='random_seed')\n\tfiltered_results = all_results.merge(sigma_results, on=['random_seed', 'sigma', 'alpha'])\n\tfiltered_results = filtered_results[\n\t\t(((filtered_results.pval >= pval) &  (filtered_results.best_pval >= pval)) | \\\n\t\t((filtered_results.best_pval < pval) &  (filtered_results.hsic == filtered_results.smallest_hsic)))\n\t\t]\n\tfiltered_results.drop(['pval', 'best_pval'], inplace=True, axis=1)\n\tfiltered_results.reset_index(drop=True, inplace=True)\n\tunique_filtered_results = filtered_results[['random_seed', 'sigma', 'alpha']].copy()\n\tunique_filtered_results.drop_duplicates(inplace=True)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tfiltered_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tfiltered_results = all_results.merge(sigma_results, on=['random_seed', 'sigma', 'alpha'])\n\tfiltered_results = filtered_results[\n\t\t(((filtered_results.pval >= pval) &  (filtered_results.best_pval >= pval)) | \\\n\t\t((filtered_results.best_pval < pval) &  (filtered_results.hsic == filtered_results.smallest_hsic)))\n\t\t]\n\tfiltered_results.drop(['pval', 'best_pval'], inplace=True, axis=1)\n\tfiltered_results.reset_index(drop=True, inplace=True)\n\tunique_filtered_results = filtered_results[['random_seed', 'sigma', 'alpha']].copy()\n\tunique_filtered_results.drop_duplicates(inplace=True)\n\treturn get_optimal_model_classic(None, filtered_results, base_dir, hparams, num_workers)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tfiltered_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tfiltered_results = filtered_results[\n\t\t(((filtered_results.pval >= pval) &  (filtered_results.best_pval >= pval)) | \\\n\t\t((filtered_results.best_pval < pval) &  (filtered_results.hsic == filtered_results.smallest_hsic)))\n\t\t]\n\tfiltered_results.drop(['pval', 'best_pval'], inplace=True, axis=1)\n\tfiltered_results.reset_index(drop=True, inplace=True)\n\tunique_filtered_results = filtered_results[['random_seed', 'sigma', 'alpha']].copy()\n\tunique_filtered_results.drop_duplicates(inplace=True)\n\treturn get_optimal_model_classic(None, filtered_results, base_dir, hparams, num_workers)\ndef get_optimal_model_classic(configs, filtered_results, base_dir, hparams, num_workers):",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tunique_filtered_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tunique_filtered_results = filtered_results[['random_seed', 'sigma', 'alpha']].copy()\n\tunique_filtered_results.drop_duplicates(inplace=True)\n\treturn get_optimal_model_classic(None, filtered_results, base_dir, hparams, num_workers)\ndef get_optimal_model_classic(configs, filtered_results, base_dir, hparams, num_workers):\n\tif ((configs is None) and (filtered_results is None)):\n\t\traise ValueError(\"Need either configs or table of results_dict\")\n\tif configs is not None:\n\t\tprint(\"getting results\")\n\t\tall_results, _ = import_results(configs, num_workers, base_dir)\n\telse:",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\t\tall_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\t\tall_results = filtered_results.copy()\n\tcolumns_to_keep = hparams + ['random_seed', 'validation_pred_loss']\n\tbest_loss = all_results[columns_to_keep]\n\tbest_loss = best_loss.groupby('random_seed').validation_pred_loss.min()\n\tbest_loss = best_loss.to_frame()\n\tbest_loss.reset_index(drop=False, inplace=True)\n\tbest_loss.rename(columns={'validation_pred_loss': 'min_validation_pred_loss'},\n\t\tinplace=True)\n\tall_results = all_results.merge(best_loss, on='random_seed')\n\tall_results = all_results[",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tcolumns_to_keep",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tcolumns_to_keep = hparams + ['random_seed', 'validation_pred_loss']\n\tbest_loss = all_results[columns_to_keep]\n\tbest_loss = best_loss.groupby('random_seed').validation_pred_loss.min()\n\tbest_loss = best_loss.to_frame()\n\tbest_loss.reset_index(drop=False, inplace=True)\n\tbest_loss.rename(columns={'validation_pred_loss': 'min_validation_pred_loss'},\n\t\tinplace=True)\n\tall_results = all_results.merge(best_loss, on='random_seed')\n\tall_results = all_results[\n\t\t(all_results.validation_pred_loss == all_results.min_validation_pred_loss)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tbest_loss",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tbest_loss = all_results[columns_to_keep]\n\tbest_loss = best_loss.groupby('random_seed').validation_pred_loss.min()\n\tbest_loss = best_loss.to_frame()\n\tbest_loss.reset_index(drop=False, inplace=True)\n\tbest_loss.rename(columns={'validation_pred_loss': 'min_validation_pred_loss'},\n\t\tinplace=True)\n\tall_results = all_results.merge(best_loss, on='random_seed')\n\tall_results = all_results[\n\t\t(all_results.validation_pred_loss == all_results.min_validation_pred_loss)\n\t]",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tbest_loss",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tbest_loss = best_loss.groupby('random_seed').validation_pred_loss.min()\n\tbest_loss = best_loss.to_frame()\n\tbest_loss.reset_index(drop=False, inplace=True)\n\tbest_loss.rename(columns={'validation_pred_loss': 'min_validation_pred_loss'},\n\t\tinplace=True)\n\tall_results = all_results.merge(best_loss, on='random_seed')\n\tall_results = all_results[\n\t\t(all_results.validation_pred_loss == all_results.min_validation_pred_loss)\n\t]\n\t# print(all_results[['random_seed', 'sigma', 'alpha', 'l2_penalty']])",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tbest_loss",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tbest_loss = best_loss.to_frame()\n\tbest_loss.reset_index(drop=False, inplace=True)\n\tbest_loss.rename(columns={'validation_pred_loss': 'min_validation_pred_loss'},\n\t\tinplace=True)\n\tall_results = all_results.merge(best_loss, on='random_seed')\n\tall_results = all_results[\n\t\t(all_results.validation_pred_loss == all_results.min_validation_pred_loss)\n\t]\n\t# print(all_results[['random_seed', 'sigma', 'alpha', 'l2_penalty']])\n\toptimal_configs = all_results[['random_seed', 'hash']]",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tall_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tall_results = all_results.merge(best_loss, on='random_seed')\n\tall_results = all_results[\n\t\t(all_results.validation_pred_loss == all_results.min_validation_pred_loss)\n\t]\n\t# print(all_results[['random_seed', 'sigma', 'alpha', 'l2_penalty']])\n\toptimal_configs = all_results[['random_seed', 'hash']]\n\t# --- get the final results over all runs\n\tmean_results = all_results.mean(axis=0).to_frame()\n\tmean_results.rename(columns={0: 'mean'}, inplace=True)\n\tstd_results = all_results.std(axis=0).to_frame()",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tall_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tall_results = all_results[\n\t\t(all_results.validation_pred_loss == all_results.min_validation_pred_loss)\n\t]\n\t# print(all_results[['random_seed', 'sigma', 'alpha', 'l2_penalty']])\n\toptimal_configs = all_results[['random_seed', 'hash']]\n\t# --- get the final results over all runs\n\tmean_results = all_results.mean(axis=0).to_frame()\n\tmean_results.rename(columns={0: 'mean'}, inplace=True)\n\tstd_results = all_results.std(axis=0).to_frame()\n\tstd_results.rename(columns={0: 'std'}, inplace=True)",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\toptimal_configs",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\toptimal_configs = all_results[['random_seed', 'hash']]\n\t# --- get the final results over all runs\n\tmean_results = all_results.mean(axis=0).to_frame()\n\tmean_results.rename(columns={0: 'mean'}, inplace=True)\n\tstd_results = all_results.std(axis=0).to_frame()\n\tstd_results.rename(columns={0: 'std'}, inplace=True)\n\tfinal_results = mean_results.merge(\n\t\tstd_results, left_index=True, right_index=True\n\t)\n\tfinal_results = final_results.transpose()",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tmean_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tmean_results = all_results.mean(axis=0).to_frame()\n\tmean_results.rename(columns={0: 'mean'}, inplace=True)\n\tstd_results = all_results.std(axis=0).to_frame()\n\tstd_results.rename(columns={0: 'std'}, inplace=True)\n\tfinal_results = mean_results.merge(\n\t\tstd_results, left_index=True, right_index=True\n\t)\n\tfinal_results = final_results.transpose()\n\tfinal_results_clean = reshape_results(final_results)\n\treturn final_results_clean, optimal_configs",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tstd_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tstd_results = all_results.std(axis=0).to_frame()\n\tstd_results.rename(columns={0: 'std'}, inplace=True)\n\tfinal_results = mean_results.merge(\n\t\tstd_results, left_index=True, right_index=True\n\t)\n\tfinal_results = final_results.transpose()\n\tfinal_results_clean = reshape_results(final_results)\n\treturn final_results_clean, optimal_configs",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tfinal_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tfinal_results = mean_results.merge(\n\t\tstd_results, left_index=True, right_index=True\n\t)\n\tfinal_results = final_results.transpose()\n\tfinal_results_clean = reshape_results(final_results)\n\treturn final_results_clean, optimal_configs",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tfinal_results",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tfinal_results = final_results.transpose()\n\tfinal_results_clean = reshape_results(final_results)\n\treturn final_results_clean, optimal_configs",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "\tfinal_results_clean",
        "kind": 5,
        "importPath": "shared.cross_validation",
        "description": "shared.cross_validation",
        "peekOfCode": "\tfinal_results_clean = reshape_results(final_results)\n\treturn final_results_clean, optimal_configs",
        "detail": "shared.cross_validation",
        "documentation": {}
    },
    {
        "label": "compute_loss",
        "kind": 2,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "def compute_loss(labels, logits, embedding, sample_weights, params):\n\tif params['weighted'] == 'False':\n\t\tprediction_loss, hsic_loss = compute_loss_unweighted(labels, logits,\n\t\t\tembedding, params)\n\telse:\n\t\tprediction_loss, hsic_loss = compute_loss_weighted(labels, logits,\n\t\t\tembedding, sample_weights, params)\n\treturn prediction_loss, hsic_loss\ndef compute_loss_unweighted(labels, logits, embedding, params):\n\t# labels: ground truth labels([y0(pnemounia), y1(sex), y2(support device)])",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "compute_loss_unweighted",
        "kind": 2,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "def compute_loss_unweighted(labels, logits, embedding, params):\n\t# labels: ground truth labels([y0(pnemounia), y1(sex), y2(support device)])\n\t# logits: predicted label(pnemounia)\n\t# embedding: a learned representation vector\n\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\tindividual_losses = tf.keras.losses.binary_crossentropy(y_main, logits,\n\t\tfrom_logits=True)\n\tunweighted_loss = tf.reduce_mean(individual_losses)\n\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "compute_loss_weighted",
        "kind": 2,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "def compute_loss_weighted(labels, logits, embedding, sample_weights, params):\n\t# labels: ground truth labels([y0(pnemounia), y1(sex), y2(support device)])\n\t# logits: predicted label(pnemounia)\n\t# embedding: a learned representation vector\n\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\tindividual_losses = tf.keras.losses.binary_crossentropy(y_main, logits,\n\t\tfrom_logits=True)\n\tweighted_loss = sample_weights * individual_losses\n\tweighted_loss = tf.math.divide_no_nan(\n\t\ttf.reduce_sum(weighted_loss),",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "hsic",
        "kind": 2,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "def hsic(x, y, sample_weights, sigma=1.0):\n\t\"\"\" Computes the weighted HSIC between two arbitrary variables x, y\"\"\"\n\tif len(x.shape) == 1:\n\t\t\tx = tf.expand_dims(x, axis=-1)\n\tif len(y.shape) == 1:\n\t\t\ty = tf.expand_dims(y, axis=-1)\n\tif sample_weights == None:\n\t\t\tsample_weights = tf.ones((tf.shape(y)[0], 1))\n\tif len(sample_weights.shape) == 1:\n\t\t\tsample_weights = tf.expand_dims(sample_weights, axis=-1)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "auroc",
        "kind": 2,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "def auroc(labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\tauc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\tauc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric\ndef get_prediction_by_group(labels, predictions):\n\tmean_pred_dict = {}\n\tfor y0_val in [0, 1]:\n\t\tfor y1_val in [0, 1]:",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "get_prediction_by_group",
        "kind": 2,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "def get_prediction_by_group(labels, predictions):\n\tmean_pred_dict = {}\n\tfor y0_val in [0, 1]:\n\t\tfor y1_val in [0, 1]:\n\t\t\tfor y2_val in [0, 1]:\n\t\t\t\ty0_mask = y0_val * labels[:, 0] + (1.0 - y0_val) * (1.0 - labels[:, 0])\n\t\t\t\ty1_mask = y1_val * labels[:, 1] + (1.0 - y1_val) * (1.0 - labels[:, 1])\n\t\t\t\ty2_mask = y2_val * labels[:, 2] + (1.0 - y2_val) * (1.0 - labels[:, 2])\n\t\t\t\tlabels_mask = tf.where(y0_mask * y1_mask * y2_mask)\n\t\t\t\tmean_pred_dict[f'mean_pred_{y0_val}{y1_val}{y2_val}'] = tf.compat.v1.metrics.mean(",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "get_hsic_at_sigmas",
        "kind": 2,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "def get_hsic_at_sigmas(sigma_list, labels, embedding, sample_weights,\n\teager):\n\tresult_dict = {}\n\tfor sigma_val in sigma_list:\n\t\thsic_at_sigma = hsic(embedding, labels[:, 1:], sample_weights=sample_weights,\n\t\t sigma=sigma_val)\n\t\tif eager:\n\t\t\tresult_dict[f'hsic{sigma_val}'] = hsic_at_sigma.numpy()\n\t\telse:\n\t\t\tresult_dict[f'hsic{sigma_val}'] = tf.compat.v1.metrics.mean(",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "get_eval_metrics_dict",
        "kind": 2,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "def get_eval_metrics_dict(labels, predictions, sample_weights, params, eager=False,\n\tsigma_list=[0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]):\n\tdel params\n\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\teval_metrics_dict = {}\n\teval_metrics_dict[\"auc\"] = auroc(\n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])\n\tmean_pred_dict = get_prediction_by_group(labels, predictions[\"probabilities\"])\n\thsic_val_dict = get_hsic_at_sigmas(sigma_list, labels,\n\t\tpredictions['embedding'], sample_weights, eager=eager)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\ty_main",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\tindividual_losses = tf.keras.losses.binary_crossentropy(y_main, logits,\n\t\tfrom_logits=True)\n\tunweighted_loss = tf.reduce_mean(individual_losses)\n\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:\n\t\thsic_loss = hsic(embedding, aux_y, sample_weights=None,\n\t\t\tsigma=params['sigma'])\n\telse:\n\t\thsic_loss = 0.0",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tindividual_losses",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tindividual_losses = tf.keras.losses.binary_crossentropy(y_main, logits,\n\t\tfrom_logits=True)\n\tunweighted_loss = tf.reduce_mean(individual_losses)\n\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:\n\t\thsic_loss = hsic(embedding, aux_y, sample_weights=None,\n\t\t\tsigma=params['sigma'])\n\telse:\n\t\thsic_loss = 0.0\n\treturn unweighted_loss, hsic_loss",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tunweighted_loss",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tunweighted_loss = tf.reduce_mean(individual_losses)\n\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:\n\t\thsic_loss = hsic(embedding, aux_y, sample_weights=None,\n\t\t\tsigma=params['sigma'])\n\telse:\n\t\thsic_loss = 0.0\n\treturn unweighted_loss, hsic_loss\ndef compute_loss_weighted(labels, logits, embedding, sample_weights, params):\n\t# labels: ground truth labels([y0(pnemounia), y1(sex), y2(support device)])",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\taux_y",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:\n\t\thsic_loss = hsic(embedding, aux_y, sample_weights=None,\n\t\t\tsigma=params['sigma'])\n\telse:\n\t\thsic_loss = 0.0\n\treturn unweighted_loss, hsic_loss\ndef compute_loss_weighted(labels, logits, embedding, sample_weights, params):\n\t# labels: ground truth labels([y0(pnemounia), y1(sex), y2(support device)])\n\t# logits: predicted label(pnemounia)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\thsic_loss",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\thsic_loss = hsic(embedding, aux_y, sample_weights=None,\n\t\t\tsigma=params['sigma'])\n\telse:\n\t\thsic_loss = 0.0\n\treturn unweighted_loss, hsic_loss\ndef compute_loss_weighted(labels, logits, embedding, sample_weights, params):\n\t# labels: ground truth labels([y0(pnemounia), y1(sex), y2(support device)])\n\t# logits: predicted label(pnemounia)\n\t# embedding: a learned representation vector\n\ty_main = tf.expand_dims(labels[:, 0], axis=-1)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\thsic_loss",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\thsic_loss = 0.0\n\treturn unweighted_loss, hsic_loss\ndef compute_loss_weighted(labels, logits, embedding, sample_weights, params):\n\t# labels: ground truth labels([y0(pnemounia), y1(sex), y2(support device)])\n\t# logits: predicted label(pnemounia)\n\t# embedding: a learned representation vector\n\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\tindividual_losses = tf.keras.losses.binary_crossentropy(y_main, logits,\n\t\tfrom_logits=True)\n\tweighted_loss = sample_weights * individual_losses",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\ty_main",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\tindividual_losses = tf.keras.losses.binary_crossentropy(y_main, logits,\n\t\tfrom_logits=True)\n\tweighted_loss = sample_weights * individual_losses\n\tweighted_loss = tf.math.divide_no_nan(\n\t\ttf.reduce_sum(weighted_loss),\n\t\ttf.reduce_sum(sample_weights)\n\t)\n\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tindividual_losses",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tindividual_losses = tf.keras.losses.binary_crossentropy(y_main, logits,\n\t\tfrom_logits=True)\n\tweighted_loss = sample_weights * individual_losses\n\tweighted_loss = tf.math.divide_no_nan(\n\t\ttf.reduce_sum(weighted_loss),\n\t\ttf.reduce_sum(sample_weights)\n\t)\n\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:\n\t\thsic_loss = hsic(embedding, aux_y, sample_weights=sample_weights,",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tweighted_loss",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tweighted_loss = sample_weights * individual_losses\n\tweighted_loss = tf.math.divide_no_nan(\n\t\ttf.reduce_sum(weighted_loss),\n\t\ttf.reduce_sum(sample_weights)\n\t)\n\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:\n\t\thsic_loss = hsic(embedding, aux_y, sample_weights=sample_weights,\n\t\t\tsigma=params['sigma'])\n\telse:",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tweighted_loss",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tweighted_loss = tf.math.divide_no_nan(\n\t\ttf.reduce_sum(weighted_loss),\n\t\ttf.reduce_sum(sample_weights)\n\t)\n\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:\n\t\thsic_loss = hsic(embedding, aux_y, sample_weights=sample_weights,\n\t\t\tsigma=params['sigma'])\n\telse:\n\t\thsic_loss = 0.0",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\taux_y",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\taux_y = labels[:, 1:]\n\tif params['alpha'] > 0:\n\t\thsic_loss = hsic(embedding, aux_y, sample_weights=sample_weights,\n\t\t\tsigma=params['sigma'])\n\telse:\n\t\thsic_loss = 0.0\n\treturn weighted_loss, hsic_loss\ndef hsic(x, y, sample_weights, sigma=1.0):\n\t\"\"\" Computes the weighted HSIC between two arbitrary variables x, y\"\"\"\n\tif len(x.shape) == 1:",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\thsic_loss",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\thsic_loss = hsic(embedding, aux_y, sample_weights=sample_weights,\n\t\t\tsigma=params['sigma'])\n\telse:\n\t\thsic_loss = 0.0\n\treturn weighted_loss, hsic_loss\ndef hsic(x, y, sample_weights, sigma=1.0):\n\t\"\"\" Computes the weighted HSIC between two arbitrary variables x, y\"\"\"\n\tif len(x.shape) == 1:\n\t\t\tx = tf.expand_dims(x, axis=-1)\n\tif len(y.shape) == 1:",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\thsic_loss",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\thsic_loss = 0.0\n\treturn weighted_loss, hsic_loss\ndef hsic(x, y, sample_weights, sigma=1.0):\n\t\"\"\" Computes the weighted HSIC between two arbitrary variables x, y\"\"\"\n\tif len(x.shape) == 1:\n\t\t\tx = tf.expand_dims(x, axis=-1)\n\tif len(y.shape) == 1:\n\t\t\ty = tf.expand_dims(y, axis=-1)\n\tif sample_weights == None:\n\t\t\tsample_weights = tf.ones((tf.shape(y)[0], 1))",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\tx",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\tx = tf.expand_dims(x, axis=-1)\n\tif len(y.shape) == 1:\n\t\t\ty = tf.expand_dims(y, axis=-1)\n\tif sample_weights == None:\n\t\t\tsample_weights = tf.ones((tf.shape(y)[0], 1))\n\tif len(sample_weights.shape) == 1:\n\t\t\tsample_weights = tf.expand_dims(sample_weights, axis=-1)\n\tsample_weights_T = tf.transpose(sample_weights)\n\tkernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\ty",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\ty = tf.expand_dims(y, axis=-1)\n\tif sample_weights == None:\n\t\t\tsample_weights = tf.ones((tf.shape(y)[0], 1))\n\tif len(sample_weights.shape) == 1:\n\t\t\tsample_weights = tf.expand_dims(sample_weights, axis=-1)\n\tsample_weights_T = tf.transpose(sample_weights)\n\tkernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_xx = kernel_fxx.matrix(x, x)\n\tkernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\tsample_weights",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\tsample_weights = tf.ones((tf.shape(y)[0], 1))\n\tif len(sample_weights.shape) == 1:\n\t\t\tsample_weights = tf.expand_dims(sample_weights, axis=-1)\n\tsample_weights_T = tf.transpose(sample_weights)\n\tkernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_xx = kernel_fxx.matrix(x, x)\n\tkernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_yy = kernel_fyy.matrix(y, y)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\tsample_weights",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\tsample_weights = tf.expand_dims(sample_weights, axis=-1)\n\tsample_weights_T = tf.transpose(sample_weights)\n\tkernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_xx = kernel_fxx.matrix(x, x)\n\tkernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_yy = kernel_fyy.matrix(y, y)\n\tN = tf.cast(tf.shape(y)[0], tf.float32)\n\t# First term",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tsample_weights_T",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tsample_weights_T = tf.transpose(sample_weights)\n\tkernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_xx = kernel_fxx.matrix(x, x)\n\tkernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_yy = kernel_fyy.matrix(y, y)\n\tN = tf.cast(tf.shape(y)[0], tf.float32)\n\t# First term\n\thsic_1 = tf.math.multiply(kernel_xx, kernel_yy)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tkernel_fxx",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tkernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_xx = kernel_fxx.matrix(x, x)\n\tkernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_yy = kernel_fyy.matrix(y, y)\n\tN = tf.cast(tf.shape(y)[0], tf.float32)\n\t# First term\n\thsic_1 = tf.math.multiply(kernel_xx, kernel_yy)\n\thsic_1 = tf.linalg.matmul(sample_weights_T, hsic_1)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tkernel_xx",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tkernel_xx = kernel_fxx.matrix(x, x)\n\tkernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_yy = kernel_fyy.matrix(y, y)\n\tN = tf.cast(tf.shape(y)[0], tf.float32)\n\t# First term\n\thsic_1 = tf.math.multiply(kernel_xx, kernel_yy)\n\thsic_1 = tf.linalg.matmul(sample_weights_T, hsic_1)\n\thsic_1 = tf.linalg.matmul(hsic_1, sample_weights)\n\thsic_1 = hsic_1 / (N **2)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tkernel_fyy",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tkernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(\n\t\t\tamplitude=1.0, length_scale=sigma)\n\tkernel_yy = kernel_fyy.matrix(y, y)\n\tN = tf.cast(tf.shape(y)[0], tf.float32)\n\t# First term\n\thsic_1 = tf.math.multiply(kernel_xx, kernel_yy)\n\thsic_1 = tf.linalg.matmul(sample_weights_T, hsic_1)\n\thsic_1 = tf.linalg.matmul(hsic_1, sample_weights)\n\thsic_1 = hsic_1 / (N **2)\n\t# Second term",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tkernel_yy",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tkernel_yy = kernel_fyy.matrix(y, y)\n\tN = tf.cast(tf.shape(y)[0], tf.float32)\n\t# First term\n\thsic_1 = tf.math.multiply(kernel_xx, kernel_yy)\n\thsic_1 = tf.linalg.matmul(sample_weights_T, hsic_1)\n\thsic_1 = tf.linalg.matmul(hsic_1, sample_weights)\n\thsic_1 = hsic_1 / (N **2)\n\t# Second term\n\t# Note there is a typo in the paper. Authors will update\n\tW_matrix = tf.linalg.matmul(sample_weights, sample_weights_T)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tN",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tN = tf.cast(tf.shape(y)[0], tf.float32)\n\t# First term\n\thsic_1 = tf.math.multiply(kernel_xx, kernel_yy)\n\thsic_1 = tf.linalg.matmul(sample_weights_T, hsic_1)\n\thsic_1 = tf.linalg.matmul(hsic_1, sample_weights)\n\thsic_1 = hsic_1 / (N **2)\n\t# Second term\n\t# Note there is a typo in the paper. Authors will update\n\tW_matrix = tf.linalg.matmul(sample_weights, sample_weights_T)\n\thsic_2 = tf.math.multiply(kernel_yy, W_matrix)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_1",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_1 = tf.math.multiply(kernel_xx, kernel_yy)\n\thsic_1 = tf.linalg.matmul(sample_weights_T, hsic_1)\n\thsic_1 = tf.linalg.matmul(hsic_1, sample_weights)\n\thsic_1 = hsic_1 / (N **2)\n\t# Second term\n\t# Note there is a typo in the paper. Authors will update\n\tW_matrix = tf.linalg.matmul(sample_weights, sample_weights_T)\n\thsic_2 = tf.math.multiply(kernel_yy, W_matrix)\n\thsic_2 = tf.reduce_sum(hsic_2, keepdims=True) * tf.reduce_sum(kernel_xx, keepdims=True)\n\thsic_2 = hsic_2 / (N ** 4)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_1",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_1 = tf.linalg.matmul(sample_weights_T, hsic_1)\n\thsic_1 = tf.linalg.matmul(hsic_1, sample_weights)\n\thsic_1 = hsic_1 / (N **2)\n\t# Second term\n\t# Note there is a typo in the paper. Authors will update\n\tW_matrix = tf.linalg.matmul(sample_weights, sample_weights_T)\n\thsic_2 = tf.math.multiply(kernel_yy, W_matrix)\n\thsic_2 = tf.reduce_sum(hsic_2, keepdims=True) * tf.reduce_sum(kernel_xx, keepdims=True)\n\thsic_2 = hsic_2 / (N ** 4)\n\t# third term",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_1",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_1 = tf.linalg.matmul(hsic_1, sample_weights)\n\thsic_1 = hsic_1 / (N **2)\n\t# Second term\n\t# Note there is a typo in the paper. Authors will update\n\tW_matrix = tf.linalg.matmul(sample_weights, sample_weights_T)\n\thsic_2 = tf.math.multiply(kernel_yy, W_matrix)\n\thsic_2 = tf.reduce_sum(hsic_2, keepdims=True) * tf.reduce_sum(kernel_xx, keepdims=True)\n\thsic_2 = hsic_2 / (N ** 4)\n\t# third term\n\thsic_3 = tf.linalg.matmul(kernel_yy, sample_weights)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_1",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_1 = hsic_1 / (N **2)\n\t# Second term\n\t# Note there is a typo in the paper. Authors will update\n\tW_matrix = tf.linalg.matmul(sample_weights, sample_weights_T)\n\thsic_2 = tf.math.multiply(kernel_yy, W_matrix)\n\thsic_2 = tf.reduce_sum(hsic_2, keepdims=True) * tf.reduce_sum(kernel_xx, keepdims=True)\n\thsic_2 = hsic_2 / (N ** 4)\n\t# third term\n\thsic_3 = tf.linalg.matmul(kernel_yy, sample_weights)\n\thsic_3 = tf.math.multiply(",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tW_matrix",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tW_matrix = tf.linalg.matmul(sample_weights, sample_weights_T)\n\thsic_2 = tf.math.multiply(kernel_yy, W_matrix)\n\thsic_2 = tf.reduce_sum(hsic_2, keepdims=True) * tf.reduce_sum(kernel_xx, keepdims=True)\n\thsic_2 = hsic_2 / (N ** 4)\n\t# third term\n\thsic_3 = tf.linalg.matmul(kernel_yy, sample_weights)\n\thsic_3 = tf.math.multiply(\n\t\t\ttf.reduce_sum(kernel_xx, axis =1, keepdims=True), hsic_3)\n\thsic_3 = tf.linalg.matmul(sample_weights_T, hsic_3)\n\thsic_3 = 2 * hsic_3 / (N ** 3)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_2",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_2 = tf.math.multiply(kernel_yy, W_matrix)\n\thsic_2 = tf.reduce_sum(hsic_2, keepdims=True) * tf.reduce_sum(kernel_xx, keepdims=True)\n\thsic_2 = hsic_2 / (N ** 4)\n\t# third term\n\thsic_3 = tf.linalg.matmul(kernel_yy, sample_weights)\n\thsic_3 = tf.math.multiply(\n\t\t\ttf.reduce_sum(kernel_xx, axis =1, keepdims=True), hsic_3)\n\thsic_3 = tf.linalg.matmul(sample_weights_T, hsic_3)\n\thsic_3 = 2 * hsic_3 / (N ** 3)\n\thsic_val = hsic_1 + hsic_2 - hsic_3",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_2",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_2 = tf.reduce_sum(hsic_2, keepdims=True) * tf.reduce_sum(kernel_xx, keepdims=True)\n\thsic_2 = hsic_2 / (N ** 4)\n\t# third term\n\thsic_3 = tf.linalg.matmul(kernel_yy, sample_weights)\n\thsic_3 = tf.math.multiply(\n\t\t\ttf.reduce_sum(kernel_xx, axis =1, keepdims=True), hsic_3)\n\thsic_3 = tf.linalg.matmul(sample_weights_T, hsic_3)\n\thsic_3 = 2 * hsic_3 / (N ** 3)\n\thsic_val = hsic_1 + hsic_2 - hsic_3\n\thsic_val = tf.maximum(0.0, hsic_val)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_2",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_2 = hsic_2 / (N ** 4)\n\t# third term\n\thsic_3 = tf.linalg.matmul(kernel_yy, sample_weights)\n\thsic_3 = tf.math.multiply(\n\t\t\ttf.reduce_sum(kernel_xx, axis =1, keepdims=True), hsic_3)\n\thsic_3 = tf.linalg.matmul(sample_weights_T, hsic_3)\n\thsic_3 = 2 * hsic_3 / (N ** 3)\n\thsic_val = hsic_1 + hsic_2 - hsic_3\n\thsic_val = tf.maximum(0.0, hsic_val)\n\treturn hsic_val",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_3",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_3 = tf.linalg.matmul(kernel_yy, sample_weights)\n\thsic_3 = tf.math.multiply(\n\t\t\ttf.reduce_sum(kernel_xx, axis =1, keepdims=True), hsic_3)\n\thsic_3 = tf.linalg.matmul(sample_weights_T, hsic_3)\n\thsic_3 = 2 * hsic_3 / (N ** 3)\n\thsic_val = hsic_1 + hsic_2 - hsic_3\n\thsic_val = tf.maximum(0.0, hsic_val)\n\treturn hsic_val\ndef auroc(labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_3",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_3 = tf.math.multiply(\n\t\t\ttf.reduce_sum(kernel_xx, axis =1, keepdims=True), hsic_3)\n\thsic_3 = tf.linalg.matmul(sample_weights_T, hsic_3)\n\thsic_3 = 2 * hsic_3 / (N ** 3)\n\thsic_val = hsic_1 + hsic_2 - hsic_3\n\thsic_val = tf.maximum(0.0, hsic_val)\n\treturn hsic_val\ndef auroc(labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\tauc_metric = tf.keras.metrics.AUC(name=\"auroc\")",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_3",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_3 = tf.linalg.matmul(sample_weights_T, hsic_3)\n\thsic_3 = 2 * hsic_3 / (N ** 3)\n\thsic_val = hsic_1 + hsic_2 - hsic_3\n\thsic_val = tf.maximum(0.0, hsic_val)\n\treturn hsic_val\ndef auroc(labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\tauc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\tauc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_3",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_3 = 2 * hsic_3 / (N ** 3)\n\thsic_val = hsic_1 + hsic_2 - hsic_3\n\thsic_val = tf.maximum(0.0, hsic_val)\n\treturn hsic_val\ndef auroc(labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\tauc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\tauc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_val",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_val = hsic_1 + hsic_2 - hsic_3\n\thsic_val = tf.maximum(0.0, hsic_val)\n\treturn hsic_val\ndef auroc(labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\tauc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\tauc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric\ndef get_prediction_by_group(labels, predictions):",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_val",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_val = tf.maximum(0.0, hsic_val)\n\treturn hsic_val\ndef auroc(labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\tauc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\tauc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric\ndef get_prediction_by_group(labels, predictions):\n\tmean_pred_dict = {}",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tauc_metric",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tauc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\tauc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric\ndef get_prediction_by_group(labels, predictions):\n\tmean_pred_dict = {}\n\tfor y0_val in [0, 1]:\n\t\tfor y1_val in [0, 1]:\n\t\t\tfor y2_val in [0, 1]:\n\t\t\t\ty0_mask = y0_val * labels[:, 0] + (1.0 - y0_val) * (1.0 - labels[:, 0])",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tmean_pred_dict",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tmean_pred_dict = {}\n\tfor y0_val in [0, 1]:\n\t\tfor y1_val in [0, 1]:\n\t\t\tfor y2_val in [0, 1]:\n\t\t\t\ty0_mask = y0_val * labels[:, 0] + (1.0 - y0_val) * (1.0 - labels[:, 0])\n\t\t\t\ty1_mask = y1_val * labels[:, 1] + (1.0 - y1_val) * (1.0 - labels[:, 1])\n\t\t\t\ty2_mask = y2_val * labels[:, 2] + (1.0 - y2_val) * (1.0 - labels[:, 2])\n\t\t\t\tlabels_mask = tf.where(y0_mask * y1_mask * y2_mask)\n\t\t\t\tmean_pred_dict[f'mean_pred_{y0_val}{y1_val}{y2_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\t\ttf.gather(predictions, labels_mask)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ty0_mask",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\t\ty0_mask = y0_val * labels[:, 0] + (1.0 - y0_val) * (1.0 - labels[:, 0])\n\t\t\t\ty1_mask = y1_val * labels[:, 1] + (1.0 - y1_val) * (1.0 - labels[:, 1])\n\t\t\t\ty2_mask = y2_val * labels[:, 2] + (1.0 - y2_val) * (1.0 - labels[:, 2])\n\t\t\t\tlabels_mask = tf.where(y0_mask * y1_mask * y2_mask)\n\t\t\t\tmean_pred_dict[f'mean_pred_{y0_val}{y1_val}{y2_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\t\ttf.gather(predictions, labels_mask)\n\t\t\t\t)\n\treturn mean_pred_dict\ndef get_hsic_at_sigmas(sigma_list, labels, embedding, sample_weights,\n\teager):",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ty1_mask",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\t\ty1_mask = y1_val * labels[:, 1] + (1.0 - y1_val) * (1.0 - labels[:, 1])\n\t\t\t\ty2_mask = y2_val * labels[:, 2] + (1.0 - y2_val) * (1.0 - labels[:, 2])\n\t\t\t\tlabels_mask = tf.where(y0_mask * y1_mask * y2_mask)\n\t\t\t\tmean_pred_dict[f'mean_pred_{y0_val}{y1_val}{y2_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\t\ttf.gather(predictions, labels_mask)\n\t\t\t\t)\n\treturn mean_pred_dict\ndef get_hsic_at_sigmas(sigma_list, labels, embedding, sample_weights,\n\teager):\n\tresult_dict = {}",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ty2_mask",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\t\ty2_mask = y2_val * labels[:, 2] + (1.0 - y2_val) * (1.0 - labels[:, 2])\n\t\t\t\tlabels_mask = tf.where(y0_mask * y1_mask * y2_mask)\n\t\t\t\tmean_pred_dict[f'mean_pred_{y0_val}{y1_val}{y2_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\t\ttf.gather(predictions, labels_mask)\n\t\t\t\t)\n\treturn mean_pred_dict\ndef get_hsic_at_sigmas(sigma_list, labels, embedding, sample_weights,\n\teager):\n\tresult_dict = {}\n\tfor sigma_val in sigma_list:",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tlabels_mask",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\t\tlabels_mask = tf.where(y0_mask * y1_mask * y2_mask)\n\t\t\t\tmean_pred_dict[f'mean_pred_{y0_val}{y1_val}{y2_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\t\ttf.gather(predictions, labels_mask)\n\t\t\t\t)\n\treturn mean_pred_dict\ndef get_hsic_at_sigmas(sigma_list, labels, embedding, sample_weights,\n\teager):\n\tresult_dict = {}\n\tfor sigma_val in sigma_list:\n\t\thsic_at_sigma = hsic(embedding, labels[:, 1:], sample_weights=sample_weights,",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\t\tmean_pred_dict[f'mean_pred_{y0_val}{y1_val}{y2_val}']",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\t\tmean_pred_dict[f'mean_pred_{y0_val}{y1_val}{y2_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\t\ttf.gather(predictions, labels_mask)\n\t\t\t\t)\n\treturn mean_pred_dict\ndef get_hsic_at_sigmas(sigma_list, labels, embedding, sample_weights,\n\teager):\n\tresult_dict = {}\n\tfor sigma_val in sigma_list:\n\t\thsic_at_sigma = hsic(embedding, labels[:, 1:], sample_weights=sample_weights,\n\t\t sigma=sigma_val)",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tresult_dict",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tresult_dict = {}\n\tfor sigma_val in sigma_list:\n\t\thsic_at_sigma = hsic(embedding, labels[:, 1:], sample_weights=sample_weights,\n\t\t sigma=sigma_val)\n\t\tif eager:\n\t\t\tresult_dict[f'hsic{sigma_val}'] = hsic_at_sigma.numpy()\n\t\telse:\n\t\t\tresult_dict[f'hsic{sigma_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\thsic_at_sigma)\n\treturn result_dict",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\thsic_at_sigma",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\thsic_at_sigma = hsic(embedding, labels[:, 1:], sample_weights=sample_weights,\n\t\t sigma=sigma_val)\n\t\tif eager:\n\t\t\tresult_dict[f'hsic{sigma_val}'] = hsic_at_sigma.numpy()\n\t\telse:\n\t\t\tresult_dict[f'hsic{sigma_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\thsic_at_sigma)\n\treturn result_dict\ndef get_eval_metrics_dict(labels, predictions, sample_weights, params, eager=False,\n\tsigma_list=[0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]):",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\tresult_dict[f'hsic{sigma_val}']",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\tresult_dict[f'hsic{sigma_val}'] = hsic_at_sigma.numpy()\n\t\telse:\n\t\t\tresult_dict[f'hsic{sigma_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\thsic_at_sigma)\n\treturn result_dict\ndef get_eval_metrics_dict(labels, predictions, sample_weights, params, eager=False,\n\tsigma_list=[0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]):\n\tdel params\n\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\teval_metrics_dict = {}",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\t\t\tresult_dict[f'hsic{sigma_val}']",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\t\t\tresult_dict[f'hsic{sigma_val}'] = tf.compat.v1.metrics.mean(\n\t\t\t\thsic_at_sigma)\n\treturn result_dict\ndef get_eval_metrics_dict(labels, predictions, sample_weights, params, eager=False,\n\tsigma_list=[0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]):\n\tdel params\n\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\teval_metrics_dict = {}\n\teval_metrics_dict[\"auc\"] = auroc(\n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\ty_main",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\teval_metrics_dict = {}\n\teval_metrics_dict[\"auc\"] = auroc(\n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])\n\tmean_pred_dict = get_prediction_by_group(labels, predictions[\"probabilities\"])\n\thsic_val_dict = get_hsic_at_sigmas(sigma_list, labels,\n\t\tpredictions['embedding'], sample_weights, eager=eager)\n\treturn {**eval_metrics_dict, **mean_pred_dict, **hsic_val_dict}",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\teval_metrics_dict",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\teval_metrics_dict = {}\n\teval_metrics_dict[\"auc\"] = auroc(\n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])\n\tmean_pred_dict = get_prediction_by_group(labels, predictions[\"probabilities\"])\n\thsic_val_dict = get_hsic_at_sigmas(sigma_list, labels,\n\t\tpredictions['embedding'], sample_weights, eager=eager)\n\treturn {**eval_metrics_dict, **mean_pred_dict, **hsic_val_dict}",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\teval_metrics_dict[\"auc\"]",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\teval_metrics_dict[\"auc\"] = auroc(\n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])\n\tmean_pred_dict = get_prediction_by_group(labels, predictions[\"probabilities\"])\n\thsic_val_dict = get_hsic_at_sigmas(sigma_list, labels,\n\t\tpredictions['embedding'], sample_weights, eager=eager)\n\treturn {**eval_metrics_dict, **mean_pred_dict, **hsic_val_dict}",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\tmean_pred_dict",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\tmean_pred_dict = get_prediction_by_group(labels, predictions[\"probabilities\"])\n\thsic_val_dict = get_hsic_at_sigmas(sigma_list, labels,\n\t\tpredictions['embedding'], sample_weights, eager=eager)\n\treturn {**eval_metrics_dict, **mean_pred_dict, **hsic_val_dict}",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "\thsic_val_dict",
        "kind": 5,
        "importPath": "shared.evaluation",
        "description": "shared.evaluation",
        "peekOfCode": "\thsic_val_dict = get_hsic_at_sigmas(sigma_list, labels,\n\t\tpredictions['embedding'], sample_weights, eager=eager)\n\treturn {**eval_metrics_dict, **mean_pred_dict, **hsic_val_dict}",
        "detail": "shared.evaluation",
        "documentation": {}
    },
    {
        "label": "get_last_saved_model",
        "kind": 2,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "def get_last_saved_model(estimator_dir):\n\tsubdirs = [x for x in Path(estimator_dir).iterdir()\n\t\tif x.is_dir() and 'temp' not in str(x)]\n\ttry:\n\t\tlatest_model_dir = str(sorted(subdirs)[-1])\n\t\tloaded = tf.saved_model.load(latest_model_dir)\n\t\tmodel = loaded.signatures[\"serving_default\"]\n\texcept:\n\t\tprint(estimator_dir)\n\treturn model",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "get_data_chexpert",
        "kind": 2,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "def get_data_chexpert(kfolds, config, base_dir):\n\texperiment_directory = f\"{base_dir}/experiment_data/rs{config['random_seed']}\"\n\t_, valid_data, _ = chx.load_created_data(\n\t\tchexpert_data_dir=base_dir, random_seed=config['random_seed'],\n\t\tskew_train=config['skew_train'], weighted=config['weighted'])\n\tmap_to_image_label_given_pixel = functools.partial(chx.map_to_image_label,\n\t\tpixel=config['pixel'], weighted=config['weighted'])\n\tvalid_dataset = tf.data.Dataset.from_tensor_slices(valid_data)\n\tvalid_dataset = valid_dataset.map(map_to_image_label_given_pixel, num_parallel_calls=1)\n\tbatch_size = int(len(valid_data) / kfolds)",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "get_optimal_sigma_for_run",
        "kind": 2,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "def get_optimal_sigma_for_run(config, kfolds, base_dir):\n\t# -- get the dataset\n\tvalid_dataset = get_data_chexpert(kfolds, config, base_dir)\n\t# -- model\n\thash_string = utils.config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string, 'saved_model')\n\tmodel = get_last_saved_model(hash_dir)\n\t# ---compute hsic over folds\n\tmetric_values = []\n\tfor batch_id, examples in enumerate(valid_dataset):",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "get_optimal_sigma",
        "kind": 2,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "def get_optimal_sigma(all_config, kfolds, num_workers, base_dir):\n\tall_results = []\n\trunner_wrapper = functools.partial(get_optimal_sigma_for_run,\n\t\tkfolds=kfolds, base_dir=base_dir)\n\tif num_workers <=0:\n\t\tfor cid, config in enumerate(all_config):\n\t\t\tprint(cid)\n\t\t\tresults = runner_wrapper(config)\n\t\t\tall_results.append(results)\n\telse:",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tsubdirs",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tsubdirs = [x for x in Path(estimator_dir).iterdir()\n\t\tif x.is_dir() and 'temp' not in str(x)]\n\ttry:\n\t\tlatest_model_dir = str(sorted(subdirs)[-1])\n\t\tloaded = tf.saved_model.load(latest_model_dir)\n\t\tmodel = loaded.signatures[\"serving_default\"]\n\texcept:\n\t\tprint(estimator_dir)\n\treturn model\ndef get_data_chexpert(kfolds, config, base_dir):",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tlatest_model_dir",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tlatest_model_dir = str(sorted(subdirs)[-1])\n\t\tloaded = tf.saved_model.load(latest_model_dir)\n\t\tmodel = loaded.signatures[\"serving_default\"]\n\texcept:\n\t\tprint(estimator_dir)\n\treturn model\ndef get_data_chexpert(kfolds, config, base_dir):\n\texperiment_directory = f\"{base_dir}/experiment_data/rs{config['random_seed']}\"\n\t_, valid_data, _ = chx.load_created_data(\n\t\tchexpert_data_dir=base_dir, random_seed=config['random_seed'],",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tloaded",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tloaded = tf.saved_model.load(latest_model_dir)\n\t\tmodel = loaded.signatures[\"serving_default\"]\n\texcept:\n\t\tprint(estimator_dir)\n\treturn model\ndef get_data_chexpert(kfolds, config, base_dir):\n\texperiment_directory = f\"{base_dir}/experiment_data/rs{config['random_seed']}\"\n\t_, valid_data, _ = chx.load_created_data(\n\t\tchexpert_data_dir=base_dir, random_seed=config['random_seed'],\n\t\tskew_train=config['skew_train'], weighted=config['weighted'])",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tmodel",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tmodel = loaded.signatures[\"serving_default\"]\n\texcept:\n\t\tprint(estimator_dir)\n\treturn model\ndef get_data_chexpert(kfolds, config, base_dir):\n\texperiment_directory = f\"{base_dir}/experiment_data/rs{config['random_seed']}\"\n\t_, valid_data, _ = chx.load_created_data(\n\t\tchexpert_data_dir=base_dir, random_seed=config['random_seed'],\n\t\tskew_train=config['skew_train'], weighted=config['weighted'])\n\tmap_to_image_label_given_pixel = functools.partial(chx.map_to_image_label,",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\texperiment_directory",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\texperiment_directory = f\"{base_dir}/experiment_data/rs{config['random_seed']}\"\n\t_, valid_data, _ = chx.load_created_data(\n\t\tchexpert_data_dir=base_dir, random_seed=config['random_seed'],\n\t\tskew_train=config['skew_train'], weighted=config['weighted'])\n\tmap_to_image_label_given_pixel = functools.partial(chx.map_to_image_label,\n\t\tpixel=config['pixel'], weighted=config['weighted'])\n\tvalid_dataset = tf.data.Dataset.from_tensor_slices(valid_data)\n\tvalid_dataset = valid_dataset.map(map_to_image_label_given_pixel, num_parallel_calls=1)\n\tbatch_size = int(len(valid_data) / kfolds)\n\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(1)",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tmap_to_image_label_given_pixel",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tmap_to_image_label_given_pixel = functools.partial(chx.map_to_image_label,\n\t\tpixel=config['pixel'], weighted=config['weighted'])\n\tvalid_dataset = tf.data.Dataset.from_tensor_slices(valid_data)\n\tvalid_dataset = valid_dataset.map(map_to_image_label_given_pixel, num_parallel_calls=1)\n\tbatch_size = int(len(valid_data) / kfolds)\n\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(1)\n\treturn valid_dataset\ndef get_optimal_sigma_for_run(config, kfolds, base_dir):\n\t# -- get the dataset\n\tvalid_dataset = get_data_chexpert(kfolds, config, base_dir)",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tvalid_dataset",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tvalid_dataset = tf.data.Dataset.from_tensor_slices(valid_data)\n\tvalid_dataset = valid_dataset.map(map_to_image_label_given_pixel, num_parallel_calls=1)\n\tbatch_size = int(len(valid_data) / kfolds)\n\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(1)\n\treturn valid_dataset\ndef get_optimal_sigma_for_run(config, kfolds, base_dir):\n\t# -- get the dataset\n\tvalid_dataset = get_data_chexpert(kfolds, config, base_dir)\n\t# -- model\n\thash_string = utils.config_hasher(config)",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tvalid_dataset",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tvalid_dataset = valid_dataset.map(map_to_image_label_given_pixel, num_parallel_calls=1)\n\tbatch_size = int(len(valid_data) / kfolds)\n\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(1)\n\treturn valid_dataset\ndef get_optimal_sigma_for_run(config, kfolds, base_dir):\n\t# -- get the dataset\n\tvalid_dataset = get_data_chexpert(kfolds, config, base_dir)\n\t# -- model\n\thash_string = utils.config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string, 'saved_model')",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tbatch_size",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tbatch_size = int(len(valid_data) / kfolds)\n\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(1)\n\treturn valid_dataset\ndef get_optimal_sigma_for_run(config, kfolds, base_dir):\n\t# -- get the dataset\n\tvalid_dataset = get_data_chexpert(kfolds, config, base_dir)\n\t# -- model\n\thash_string = utils.config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string, 'saved_model')\n\tmodel = get_last_saved_model(hash_dir)",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tvalid_dataset",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tvalid_dataset = valid_dataset.batch(batch_size, drop_remainder=True).repeat(1)\n\treturn valid_dataset\ndef get_optimal_sigma_for_run(config, kfolds, base_dir):\n\t# -- get the dataset\n\tvalid_dataset = get_data_chexpert(kfolds, config, base_dir)\n\t# -- model\n\thash_string = utils.config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string, 'saved_model')\n\tmodel = get_last_saved_model(hash_dir)\n\t# ---compute hsic over folds",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tvalid_dataset",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tvalid_dataset = get_data_chexpert(kfolds, config, base_dir)\n\t# -- model\n\thash_string = utils.config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string, 'saved_model')\n\tmodel = get_last_saved_model(hash_dir)\n\t# ---compute hsic over folds\n\tmetric_values = []\n\tfor batch_id, examples in enumerate(valid_dataset):\n\t\t# print(f'{batch_id} / {kfolds}')\n\t\tx, labels_weights = examples",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\thash_string",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\thash_string = utils.config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string, 'saved_model')\n\tmodel = get_last_saved_model(hash_dir)\n\t# ---compute hsic over folds\n\tmetric_values = []\n\tfor batch_id, examples in enumerate(valid_dataset):\n\t\t# print(f'{batch_id} / {kfolds}')\n\t\tx, labels_weights = examples\n\t\tsample_weights = labels_weights['sample_weights']\n\t\tlabels = labels_weights['labels']",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\thash_dir",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\thash_dir = os.path.join(base_dir, 'tuning', hash_string, 'saved_model')\n\tmodel = get_last_saved_model(hash_dir)\n\t# ---compute hsic over folds\n\tmetric_values = []\n\tfor batch_id, examples in enumerate(valid_dataset):\n\t\t# print(f'{batch_id} / {kfolds}')\n\t\tx, labels_weights = examples\n\t\tsample_weights = labels_weights['sample_weights']\n\t\tlabels = labels_weights['labels']\n\t\tlogits = model(tf.convert_to_tensor(x))['logits']",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tmodel",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tmodel = get_last_saved_model(hash_dir)\n\t# ---compute hsic over folds\n\tmetric_values = []\n\tfor batch_id, examples in enumerate(valid_dataset):\n\t\t# print(f'{batch_id} / {kfolds}')\n\t\tx, labels_weights = examples\n\t\tsample_weights = labels_weights['sample_weights']\n\t\tlabels = labels_weights['labels']\n\t\tlogits = model(tf.convert_to_tensor(x))['logits']\n\t\tzpred = model(tf.convert_to_tensor(x))['embedding']",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tmetric_values",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tmetric_values = []\n\tfor batch_id, examples in enumerate(valid_dataset):\n\t\t# print(f'{batch_id} / {kfolds}')\n\t\tx, labels_weights = examples\n\t\tsample_weights = labels_weights['sample_weights']\n\t\tlabels = labels_weights['labels']\n\t\tlogits = model(tf.convert_to_tensor(x))['logits']\n\t\tzpred = model(tf.convert_to_tensor(x))['embedding']\n\t\tmetric_value = evaluation.hsic(\n\t\t\tx=zpred, y=labels[:, 1:],",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tsample_weights",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tsample_weights = labels_weights['sample_weights']\n\t\tlabels = labels_weights['labels']\n\t\tlogits = model(tf.convert_to_tensor(x))['logits']\n\t\tzpred = model(tf.convert_to_tensor(x))['embedding']\n\t\tmetric_value = evaluation.hsic(\n\t\t\tx=zpred, y=labels[:, 1:],\n\t\t\tsample_weights=sample_weights,\n\t\t\tsigma=config['sigma'])[[0]].numpy()\n\t\tmetric_values.append(metric_value)\n\tcurr_results = pd.DataFrame({",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tlabels",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tlabels = labels_weights['labels']\n\t\tlogits = model(tf.convert_to_tensor(x))['logits']\n\t\tzpred = model(tf.convert_to_tensor(x))['embedding']\n\t\tmetric_value = evaluation.hsic(\n\t\t\tx=zpred, y=labels[:, 1:],\n\t\t\tsample_weights=sample_weights,\n\t\t\tsigma=config['sigma'])[[0]].numpy()\n\t\tmetric_values.append(metric_value)\n\tcurr_results = pd.DataFrame({\n\t\t'random_seed': config['random_seed'],",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tlogits",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tlogits = model(tf.convert_to_tensor(x))['logits']\n\t\tzpred = model(tf.convert_to_tensor(x))['embedding']\n\t\tmetric_value = evaluation.hsic(\n\t\t\tx=zpred, y=labels[:, 1:],\n\t\t\tsample_weights=sample_weights,\n\t\t\tsigma=config['sigma'])[[0]].numpy()\n\t\tmetric_values.append(metric_value)\n\tcurr_results = pd.DataFrame({\n\t\t'random_seed': config['random_seed'],\n\t\t'alpha': config['alpha'],",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tzpred",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tzpred = model(tf.convert_to_tensor(x))['embedding']\n\t\tmetric_value = evaluation.hsic(\n\t\t\tx=zpred, y=labels[:, 1:],\n\t\t\tsample_weights=sample_weights,\n\t\t\tsigma=config['sigma'])[[0]].numpy()\n\t\tmetric_values.append(metric_value)\n\tcurr_results = pd.DataFrame({\n\t\t'random_seed': config['random_seed'],\n\t\t'alpha': config['alpha'],\n\t\t'sigma': config['sigma'],",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tmetric_value",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tmetric_value = evaluation.hsic(\n\t\t\tx=zpred, y=labels[:, 1:],\n\t\t\tsample_weights=sample_weights,\n\t\t\tsigma=config['sigma'])[[0]].numpy()\n\t\tmetric_values.append(metric_value)\n\tcurr_results = pd.DataFrame({\n\t\t'random_seed': config['random_seed'],\n\t\t'alpha': config['alpha'],\n\t\t'sigma': config['sigma'],\n\t\t'hsic': np.mean(metric_values),",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tcurr_results",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tcurr_results = pd.DataFrame({\n\t\t'random_seed': config['random_seed'],\n\t\t'alpha': config['alpha'],\n\t\t'sigma': config['sigma'],\n\t\t'hsic': np.mean(metric_values),\n\t\t'pval': stats.ttest_1samp(metric_values, 0.0)[1]\n\t}, index=[0])\n\tif (np.mean(metric_values) == 0.0 and np.var(metric_values) == 0.0):\n\t\tcurr_results['pval'] = 1\n\treturn curr_results",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tcurr_results['pval']",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tcurr_results['pval'] = 1\n\treturn curr_results\ndef get_optimal_sigma(all_config, kfolds, num_workers, base_dir):\n\tall_results = []\n\trunner_wrapper = functools.partial(get_optimal_sigma_for_run,\n\t\tkfolds=kfolds, base_dir=base_dir)\n\tif num_workers <=0:\n\t\tfor cid, config in enumerate(all_config):\n\t\t\tprint(cid)\n\t\t\tresults = runner_wrapper(config)",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tall_results",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tall_results = []\n\trunner_wrapper = functools.partial(get_optimal_sigma_for_run,\n\t\tkfolds=kfolds, base_dir=base_dir)\n\tif num_workers <=0:\n\t\tfor cid, config in enumerate(all_config):\n\t\t\tprint(cid)\n\t\t\tresults = runner_wrapper(config)\n\t\t\tall_results.append(results)\n\telse:\n\t\tpool = multiprocessing.Pool(num_workers)",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\trunner_wrapper",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\trunner_wrapper = functools.partial(get_optimal_sigma_for_run,\n\t\tkfolds=kfolds, base_dir=base_dir)\n\tif num_workers <=0:\n\t\tfor cid, config in enumerate(all_config):\n\t\t\tprint(cid)\n\t\t\tresults = runner_wrapper(config)\n\t\t\tall_results.append(results)\n\telse:\n\t\tpool = multiprocessing.Pool(num_workers)\n\t\tfor results in tqdm.tqdm(pool.imap_unordered(runner_wrapper, all_config), total=len(all_config)):",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\t\tresults",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\t\tresults = runner_wrapper(config)\n\t\t\tall_results.append(results)\n\telse:\n\t\tpool = multiprocessing.Pool(num_workers)\n\t\tfor results in tqdm.tqdm(pool.imap_unordered(runner_wrapper, all_config), total=len(all_config)):\n\t\t\tall_results.append(results)\n\tall_results = pd.concat(all_results, axis=0, ignore_index=True)\n\treturn all_results",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\t\tpool",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\t\tpool = multiprocessing.Pool(num_workers)\n\t\tfor results in tqdm.tqdm(pool.imap_unordered(runner_wrapper, all_config), total=len(all_config)):\n\t\t\tall_results.append(results)\n\tall_results = pd.concat(all_results, axis=0, ignore_index=True)\n\treturn all_results",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "\tall_results",
        "kind": 5,
        "importPath": "shared.get_sigma",
        "description": "shared.get_sigma",
        "peekOfCode": "\tall_results = pd.concat(all_results, axis=0, ignore_index=True)\n\treturn all_results",
        "detail": "shared.get_sigma",
        "documentation": {}
    },
    {
        "label": "EvalCheckpointSaverListener",
        "kind": 6,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "class EvalCheckpointSaverListener(tf.estimator.CheckpointSaverListener):\n\t\"\"\" Allows evaluation on multiple datasets \"\"\"\n\tdef __init__(self, estimator, input_fn, name):\n\t\tself.estimator = estimator\n\t\tself.input_fn = input_fn\n\t\tself.name = name\n\tdef after_save(self, session, global_step):\n\t\tdel session, global_step\n\t\tif self.name == \"train\":\n\t\t\tself.estimator.evaluate(self.input_fn, name=self.name, steps=1)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "serving_input_fn",
        "kind": 2,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "def serving_input_fn():\n\t\"\"\"Serving function to facilitate model saving.\"\"\"\n\t# feat = array_ops.placeholder(dtype=dtypes.float32, shape=[None, 28, 28, 3])\n\tfeat = tf.python.ops.array_ops.placeholder(\n\t\tdtype=tf.python.framework.dtypes.float32)\n\treturn tf.estimator.export.TensorServingInputReceiver(features=feat,\n\t\treceiver_tensors=feat)\ndef serving_input_fn_simple_arch():\n\t\"\"\"Serving function to facilitate model saving.\"\"\"\n\tfeat = tf.python.ops.array_ops.placeholder(",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "serving_input_fn_simple_arch",
        "kind": 2,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "def serving_input_fn_simple_arch():\n\t\"\"\"Serving function to facilitate model saving.\"\"\"\n\tfeat = tf.python.ops.array_ops.placeholder(\n\t\tdtype=tf.python.framework.dtypes.float32, shape=[None, 28, 28, 3])\n\treturn tf.estimator.export.TensorServingInputReceiver(features=feat,\n\t\treceiver_tensors=feat)\ndef model_fn(features, labels, mode, params):\n\t\"\"\" Main training function .\"\"\"\n\tnet = architectures.create_architecture(params)\n\ttraining_state = mode == tf.estimator.ModeKeys.TRAIN",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "model_fn",
        "kind": 2,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "def model_fn(features, labels, mode, params):\n\t\"\"\" Main training function .\"\"\"\n\tnet = architectures.create_architecture(params)\n\ttraining_state = mode == tf.estimator.ModeKeys.TRAIN\n\tlogits, zpred = net(features, training=training_state)\n\typred = tf.nn.sigmoid(logits)\n\tpredictions = {\n\t\t\"classes\": tf.cast(tf.math.greater_equal(ypred, .5), dtype=tf.float32),\n\t\t\"logits\": logits,\n\t\t\"probabilities\": ypred,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "def train(exp_dir,\n\t\t\t\t\tcheckpoint_dir,\n\t\t\t\t\tdataset_builder,\n\t\t\t\t\tarchitecture,\n\t\t\t\t\ttraining_steps,\n\t\t\t\t\tpixel,\n\t\t\t\t\tnum_epochs,\n\t\t\t\t\tbatch_size,\n\t\t\t\t\talpha,\n\t\t\t\t\tsigma,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tself.estimator",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tself.estimator = estimator\n\t\tself.input_fn = input_fn\n\t\tself.name = name\n\tdef after_save(self, session, global_step):\n\t\tdel session, global_step\n\t\tif self.name == \"train\":\n\t\t\tself.estimator.evaluate(self.input_fn, name=self.name, steps=1)\n\t\telse:\n\t\t\tself.estimator.evaluate(self.input_fn, name=self.name)\ndef serving_input_fn():",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tself.input_fn",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tself.input_fn = input_fn\n\t\tself.name = name\n\tdef after_save(self, session, global_step):\n\t\tdel session, global_step\n\t\tif self.name == \"train\":\n\t\t\tself.estimator.evaluate(self.input_fn, name=self.name, steps=1)\n\t\telse:\n\t\t\tself.estimator.evaluate(self.input_fn, name=self.name)\ndef serving_input_fn():\n\t\"\"\"Serving function to facilitate model saving.\"\"\"",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tself.name",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tself.name = name\n\tdef after_save(self, session, global_step):\n\t\tdel session, global_step\n\t\tif self.name == \"train\":\n\t\t\tself.estimator.evaluate(self.input_fn, name=self.name, steps=1)\n\t\telse:\n\t\t\tself.estimator.evaluate(self.input_fn, name=self.name)\ndef serving_input_fn():\n\t\"\"\"Serving function to facilitate model saving.\"\"\"\n\t# feat = array_ops.placeholder(dtype=dtypes.float32, shape=[None, 28, 28, 3])",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tfeat",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tfeat = tf.python.ops.array_ops.placeholder(\n\t\tdtype=tf.python.framework.dtypes.float32)\n\treturn tf.estimator.export.TensorServingInputReceiver(features=feat,\n\t\treceiver_tensors=feat)\ndef serving_input_fn_simple_arch():\n\t\"\"\"Serving function to facilitate model saving.\"\"\"\n\tfeat = tf.python.ops.array_ops.placeholder(\n\t\tdtype=tf.python.framework.dtypes.float32, shape=[None, 28, 28, 3])\n\treturn tf.estimator.export.TensorServingInputReceiver(features=feat,\n\t\treceiver_tensors=feat)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tfeat",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tfeat = tf.python.ops.array_ops.placeholder(\n\t\tdtype=tf.python.framework.dtypes.float32, shape=[None, 28, 28, 3])\n\treturn tf.estimator.export.TensorServingInputReceiver(features=feat,\n\t\treceiver_tensors=feat)\ndef model_fn(features, labels, mode, params):\n\t\"\"\" Main training function .\"\"\"\n\tnet = architectures.create_architecture(params)\n\ttraining_state = mode == tf.estimator.ModeKeys.TRAIN\n\tlogits, zpred = net(features, training=training_state)\n\typred = tf.nn.sigmoid(logits)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tnet",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tnet = architectures.create_architecture(params)\n\ttraining_state = mode == tf.estimator.ModeKeys.TRAIN\n\tlogits, zpred = net(features, training=training_state)\n\typred = tf.nn.sigmoid(logits)\n\tpredictions = {\n\t\t\"classes\": tf.cast(tf.math.greater_equal(ypred, .5), dtype=tf.float32),\n\t\t\"logits\": logits,\n\t\t\"probabilities\": ypred,\n\t\t\"embedding\": zpred\n\t}",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\ttraining_state",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\ttraining_state = mode == tf.estimator.ModeKeys.TRAIN\n\tlogits, zpred = net(features, training=training_state)\n\typred = tf.nn.sigmoid(logits)\n\tpredictions = {\n\t\t\"classes\": tf.cast(tf.math.greater_equal(ypred, .5), dtype=tf.float32),\n\t\t\"logits\": logits,\n\t\t\"probabilities\": ypred,\n\t\t\"embedding\": zpred\n\t}\n\tif mode == tf.estimator.ModeKeys.PREDICT:",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\typred",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\typred = tf.nn.sigmoid(logits)\n\tpredictions = {\n\t\t\"classes\": tf.cast(tf.math.greater_equal(ypred, .5), dtype=tf.float32),\n\t\t\"logits\": logits,\n\t\t\"probabilities\": ypred,\n\t\t\"embedding\": zpred\n\t}\n\tif mode == tf.estimator.ModeKeys.PREDICT:\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode=mode,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tpredictions",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tpredictions = {\n\t\t\"classes\": tf.cast(tf.math.greater_equal(ypred, .5), dtype=tf.float32),\n\t\t\"logits\": logits,\n\t\t\"probabilities\": ypred,\n\t\t\"embedding\": zpred\n\t}\n\tif mode == tf.estimator.ModeKeys.PREDICT:\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode=mode,\n\t\t\tpredictions=predictions,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tsample_weights",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tsample_weights = labels['sample_weights']\n\tlabels = labels['labels']\n\tif mode == tf.estimator.ModeKeys.EVAL:\n\t\tmain_eval_metrics = {}\n\t\t# -- main loss components\n\t\teval_pred_loss, eval_hsic = evaluation.compute_loss(labels, logits, zpred,\n\t\t\tsample_weights, params)\n\t\tmain_eval_metrics['pred_loss'] = tf.compat.v1.metrics.mean(eval_pred_loss)\n\t\tmain_eval_metrics['hsic'] = tf.compat.v1.metrics.mean(eval_hsic)\n\t\tloss = eval_pred_loss + params[\"alpha\"] * eval_hsic",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tlabels",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tlabels = labels['labels']\n\tif mode == tf.estimator.ModeKeys.EVAL:\n\t\tmain_eval_metrics = {}\n\t\t# -- main loss components\n\t\teval_pred_loss, eval_hsic = evaluation.compute_loss(labels, logits, zpred,\n\t\t\tsample_weights, params)\n\t\tmain_eval_metrics['pred_loss'] = tf.compat.v1.metrics.mean(eval_pred_loss)\n\t\tmain_eval_metrics['hsic'] = tf.compat.v1.metrics.mean(eval_hsic)\n\t\tloss = eval_pred_loss + params[\"alpha\"] * eval_hsic\n\t\t# -- additional eval metrics",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tmain_eval_metrics",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tmain_eval_metrics = {}\n\t\t# -- main loss components\n\t\teval_pred_loss, eval_hsic = evaluation.compute_loss(labels, logits, zpred,\n\t\t\tsample_weights, params)\n\t\tmain_eval_metrics['pred_loss'] = tf.compat.v1.metrics.mean(eval_pred_loss)\n\t\tmain_eval_metrics['hsic'] = tf.compat.v1.metrics.mean(eval_hsic)\n\t\tloss = eval_pred_loss + params[\"alpha\"] * eval_hsic\n\t\t# -- additional eval metrics\n\t\t# TODO add evaluation metrics\n\t\tadditional_eval_metrics = evaluation.get_eval_metrics_dict(labels,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tmain_eval_metrics['pred_loss']",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tmain_eval_metrics['pred_loss'] = tf.compat.v1.metrics.mean(eval_pred_loss)\n\t\tmain_eval_metrics['hsic'] = tf.compat.v1.metrics.mean(eval_hsic)\n\t\tloss = eval_pred_loss + params[\"alpha\"] * eval_hsic\n\t\t# -- additional eval metrics\n\t\t# TODO add evaluation metrics\n\t\tadditional_eval_metrics = evaluation.get_eval_metrics_dict(labels,\n\t\t\tpredictions, sample_weights, params)\n\t\teval_metrics = {**main_eval_metrics, **additional_eval_metrics}\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode=mode, loss=loss, train_op=None, eval_metric_ops=eval_metrics)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tmain_eval_metrics['hsic']",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tmain_eval_metrics['hsic'] = tf.compat.v1.metrics.mean(eval_hsic)\n\t\tloss = eval_pred_loss + params[\"alpha\"] * eval_hsic\n\t\t# -- additional eval metrics\n\t\t# TODO add evaluation metrics\n\t\tadditional_eval_metrics = evaluation.get_eval_metrics_dict(labels,\n\t\t\tpredictions, sample_weights, params)\n\t\teval_metrics = {**main_eval_metrics, **additional_eval_metrics}\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode=mode, loss=loss, train_op=None, eval_metric_ops=eval_metrics)\n\tif mode == tf.estimator.ModeKeys.TRAIN:",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tloss",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tloss = eval_pred_loss + params[\"alpha\"] * eval_hsic\n\t\t# -- additional eval metrics\n\t\t# TODO add evaluation metrics\n\t\tadditional_eval_metrics = evaluation.get_eval_metrics_dict(labels,\n\t\t\tpredictions, sample_weights, params)\n\t\teval_metrics = {**main_eval_metrics, **additional_eval_metrics}\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode=mode, loss=loss, train_op=None, eval_metric_ops=eval_metrics)\n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t\topt = tf.keras.optimizers.Adam()",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tadditional_eval_metrics",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tadditional_eval_metrics = evaluation.get_eval_metrics_dict(labels,\n\t\t\tpredictions, sample_weights, params)\n\t\teval_metrics = {**main_eval_metrics, **additional_eval_metrics}\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode=mode, loss=loss, train_op=None, eval_metric_ops=eval_metrics)\n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t\topt = tf.keras.optimizers.Adam()\n\t\tglobal_step = tf.compat.v1.train.get_global_step()\n\t\tckpt = tf.train.Checkpoint(\n\t\t\tstep=global_step, optimizer=opt, net=net)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\teval_metrics",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\teval_metrics = {**main_eval_metrics, **additional_eval_metrics}\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode=mode, loss=loss, train_op=None, eval_metric_ops=eval_metrics)\n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t\topt = tf.keras.optimizers.Adam()\n\t\tglobal_step = tf.compat.v1.train.get_global_step()\n\t\tckpt = tf.train.Checkpoint(\n\t\t\tstep=global_step, optimizer=opt, net=net)\n\t\twith tf.GradientTape() as tape:\n\t\t\tlogits, zpred = net(features, training=training_state)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\topt",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\topt = tf.keras.optimizers.Adam()\n\t\tglobal_step = tf.compat.v1.train.get_global_step()\n\t\tckpt = tf.train.Checkpoint(\n\t\t\tstep=global_step, optimizer=opt, net=net)\n\t\twith tf.GradientTape() as tape:\n\t\t\tlogits, zpred = net(features, training=training_state)\n\t\t\typred = tf.nn.sigmoid(logits)\n\t\t\tprediction_loss, hsic_loss = evaluation.compute_loss(labels, logits, zpred,\n\t\t\t\tsample_weights, params)\n\t\t\tregularization_loss = tf.reduce_sum(net.losses)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tglobal_step",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tglobal_step = tf.compat.v1.train.get_global_step()\n\t\tckpt = tf.train.Checkpoint(\n\t\t\tstep=global_step, optimizer=opt, net=net)\n\t\twith tf.GradientTape() as tape:\n\t\t\tlogits, zpred = net(features, training=training_state)\n\t\t\typred = tf.nn.sigmoid(logits)\n\t\t\tprediction_loss, hsic_loss = evaluation.compute_loss(labels, logits, zpred,\n\t\t\t\tsample_weights, params)\n\t\t\tregularization_loss = tf.reduce_sum(net.losses)\n\t\t\tloss = regularization_loss + prediction_loss + params[\"alpha\"] * hsic_loss",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tckpt",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tckpt = tf.train.Checkpoint(\n\t\t\tstep=global_step, optimizer=opt, net=net)\n\t\twith tf.GradientTape() as tape:\n\t\t\tlogits, zpred = net(features, training=training_state)\n\t\t\typred = tf.nn.sigmoid(logits)\n\t\t\tprediction_loss, hsic_loss = evaluation.compute_loss(labels, logits, zpred,\n\t\t\t\tsample_weights, params)\n\t\t\tregularization_loss = tf.reduce_sum(net.losses)\n\t\t\tloss = regularization_loss + prediction_loss + params[\"alpha\"] * hsic_loss\n\t\tvariables = net.trainable_variables",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\t\typred",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\t\typred = tf.nn.sigmoid(logits)\n\t\t\tprediction_loss, hsic_loss = evaluation.compute_loss(labels, logits, zpred,\n\t\t\t\tsample_weights, params)\n\t\t\tregularization_loss = tf.reduce_sum(net.losses)\n\t\t\tloss = regularization_loss + prediction_loss + params[\"alpha\"] * hsic_loss\n\t\tvariables = net.trainable_variables\n\t\tgradients = tape.gradient(loss, variables)\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode,\n\t\t\tloss=loss,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\t\tregularization_loss",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\t\tregularization_loss = tf.reduce_sum(net.losses)\n\t\t\tloss = regularization_loss + prediction_loss + params[\"alpha\"] * hsic_loss\n\t\tvariables = net.trainable_variables\n\t\tgradients = tape.gradient(loss, variables)\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode,\n\t\t\tloss=loss,\n\t\t\ttrain_op=tf.group(\n\t\t\t\topt.apply_gradients(zip(gradients, variables)),\n\t\t\t\tckpt.step.assign_add(1)))",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\t\tloss",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\t\tloss = regularization_loss + prediction_loss + params[\"alpha\"] * hsic_loss\n\t\tvariables = net.trainable_variables\n\t\tgradients = tape.gradient(loss, variables)\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode,\n\t\t\tloss=loss,\n\t\t\ttrain_op=tf.group(\n\t\t\t\topt.apply_gradients(zip(gradients, variables)),\n\t\t\t\tckpt.step.assign_add(1)))\ndef train(exp_dir,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tvariables",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tvariables = net.trainable_variables\n\t\tgradients = tape.gradient(loss, variables)\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode,\n\t\t\tloss=loss,\n\t\t\ttrain_op=tf.group(\n\t\t\t\topt.apply_gradients(zip(gradients, variables)),\n\t\t\t\tckpt.step.assign_add(1)))\ndef train(exp_dir,\n\t\t\t\t\tcheckpoint_dir,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tgradients",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tgradients = tape.gradient(loss, variables)\n\t\treturn tf.estimator.EstimatorSpec(\n\t\t\tmode,\n\t\t\tloss=loss,\n\t\t\ttrain_op=tf.group(\n\t\t\t\topt.apply_gradients(zip(gradients, variables)),\n\t\t\t\tckpt.step.assign_add(1)))\ndef train(exp_dir,\n\t\t\t\t\tcheckpoint_dir,\n\t\t\t\t\tdataset_builder,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tinput_fns",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tinput_fns = dataset_builder()\n\ttrain_data_size, train_input_fn, valid_input_fn, eval_input_fn_creater = input_fns\n\tsteps_per_epoch = int(train_data_size / batch_size)\n\tparams = {\n\t\t\"pixel\": pixel,\n\t\t\"architecture\": architecture,\n\t\t\"num_epochs\": num_epochs,\n\t\t\"batch_size\": batch_size,\n\t\t\"steps_per_epoch\": steps_per_epoch,\n\t\t\"alpha\": alpha,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tsteps_per_epoch",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tsteps_per_epoch = int(train_data_size / batch_size)\n\tparams = {\n\t\t\"pixel\": pixel,\n\t\t\"architecture\": architecture,\n\t\t\"num_epochs\": num_epochs,\n\t\t\"batch_size\": batch_size,\n\t\t\"steps_per_epoch\": steps_per_epoch,\n\t\t\"alpha\": alpha,\n\t\t\"sigma\": sigma,\n\t\t\"weighted\": weighted,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tparams",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tparams = {\n\t\t\"pixel\": pixel,\n\t\t\"architecture\": architecture,\n\t\t\"num_epochs\": num_epochs,\n\t\t\"batch_size\": batch_size,\n\t\t\"steps_per_epoch\": steps_per_epoch,\n\t\t\"alpha\": alpha,\n\t\t\"sigma\": sigma,\n\t\t\"weighted\": weighted,\n\t\t\"conditional_hsic\": conditional_hsic,",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tsave_checkpoints_steps",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tsave_checkpoints_steps = 50\n\telse:\n\t\tsave_checkpoints_steps = 100000\n\trun_config = tf.estimator.RunConfig(\n\t\ttf_random_seed=random_seed,\n\t\tsave_checkpoints_steps=save_checkpoints_steps,\n\t\t# keep_checkpoint_max=2\n\t\t)\n\test = tf.estimator.Estimator(\n\t\tmodel_fn, model_dir=checkpoint_dir, params=params, config=run_config)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\tsave_checkpoints_steps",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\tsave_checkpoints_steps = 100000\n\trun_config = tf.estimator.RunConfig(\n\t\ttf_random_seed=random_seed,\n\t\tsave_checkpoints_steps=save_checkpoints_steps,\n\t\t# keep_checkpoint_max=2\n\t\t)\n\test = tf.estimator.Estimator(\n\t\tmodel_fn, model_dir=checkpoint_dir, params=params, config=run_config)\n\tprint(f\"=====steps_per_epoch {steps_per_epoch}======\")\n\tif training_steps == 0:",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\trun_config",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\trun_config = tf.estimator.RunConfig(\n\t\ttf_random_seed=random_seed,\n\t\tsave_checkpoints_steps=save_checkpoints_steps,\n\t\t# keep_checkpoint_max=2\n\t\t)\n\test = tf.estimator.Estimator(\n\t\tmodel_fn, model_dir=checkpoint_dir, params=params, config=run_config)\n\tprint(f\"=====steps_per_epoch {steps_per_epoch}======\")\n\tif training_steps == 0:\n\t\ttraining_steps = int(params['num_epochs'] * steps_per_epoch)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\test",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\test = tf.estimator.Estimator(\n\t\tmodel_fn, model_dir=checkpoint_dir, params=params, config=run_config)\n\tprint(f\"=====steps_per_epoch {steps_per_epoch}======\")\n\tif training_steps == 0:\n\t\ttraining_steps = int(params['num_epochs'] * steps_per_epoch)\n\tprint(f'=======TRAINING STEPS {training_steps}=============')\n\t# saving_listeners = [\n\t# \tEvalCheckpointSaverListener(est, train_input_fn, \"train\"),\n\t# \tEvalCheckpointSaverListener(est, eval_input_fn_creater(0.1, params), \"0.1\"),\n\t# \tEvalCheckpointSaverListener(est, eval_input_fn_creater(0.5, params), \"0.5\"),",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\ttraining_steps",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\ttraining_steps = int(params['num_epochs'] * steps_per_epoch)\n\tprint(f'=======TRAINING STEPS {training_steps}=============')\n\t# saving_listeners = [\n\t# \tEvalCheckpointSaverListener(est, train_input_fn, \"train\"),\n\t# \tEvalCheckpointSaverListener(est, eval_input_fn_creater(0.1, params), \"0.1\"),\n\t# \tEvalCheckpointSaverListener(est, eval_input_fn_creater(0.5, params), \"0.5\"),\n\t# \tEvalCheckpointSaverListener(est, eval_input_fn_creater(0.9, params), \"0.9\"),\n\t# ]\n\test.train(train_input_fn, steps=training_steps)\n\tvalidation_results = est.evaluate(valid_input_fn)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tvalidation_results",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tvalidation_results = est.evaluate(valid_input_fn)\n\tresults = {\"validation\": validation_results}\n\t# ---- non-asymmetric analysis\n\tif py1_y0_shift_list is not None:\n\t\t# -- during testing, we dont have access to labels/weights\n\t\tfor py in py1_y0_shift_list:\n\t\t\teval_input_fn = eval_input_fn_creater(py, params,\n\t\t\t\tfixed_joint=True, aux_joint_skew=0.9)\n\t\t\tdistribution_results = est.evaluate(eval_input_fn, steps=1e5)\n\t\t\tresults[f'shift_{py}'] = distribution_results",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tresults",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tresults = {\"validation\": validation_results}\n\t# ---- non-asymmetric analysis\n\tif py1_y0_shift_list is not None:\n\t\t# -- during testing, we dont have access to labels/weights\n\t\tfor py in py1_y0_shift_list:\n\t\t\teval_input_fn = eval_input_fn_creater(py, params,\n\t\t\t\tfixed_joint=True, aux_joint_skew=0.9)\n\t\t\tdistribution_results = est.evaluate(eval_input_fn, steps=1e5)\n\t\t\tresults[f'shift_{py}'] = distribution_results\n\t# save results",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\t\teval_input_fn",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\t\teval_input_fn = eval_input_fn_creater(py, params,\n\t\t\t\tfixed_joint=True, aux_joint_skew=0.9)\n\t\t\tdistribution_results = est.evaluate(eval_input_fn, steps=1e5)\n\t\t\tresults[f'shift_{py}'] = distribution_results\n\t# save results\n\tsavefile = f\"{exp_dir}/performance.pkl\"\n\tresults = train_utils.flatten_dict(results)\n\tpickle.dump(results, open(savefile, \"wb\"))\n\t# save model\n\test.export_saved_model(f'{exp_dir}/saved_model', serving_input_fn)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\t\tdistribution_results",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\t\tdistribution_results = est.evaluate(eval_input_fn, steps=1e5)\n\t\t\tresults[f'shift_{py}'] = distribution_results\n\t# save results\n\tsavefile = f\"{exp_dir}/performance.pkl\"\n\tresults = train_utils.flatten_dict(results)\n\tpickle.dump(results, open(savefile, \"wb\"))\n\t# save model\n\test.export_saved_model(f'{exp_dir}/saved_model', serving_input_fn)\n\tif ((cleanup == 'True') & (debugger == 'False')):\n\t\ttrain_utils.cleanup_directory(checkpoint_dir)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\t\t\tresults[f'shift_{py}']",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\t\t\tresults[f'shift_{py}'] = distribution_results\n\t# save results\n\tsavefile = f\"{exp_dir}/performance.pkl\"\n\tresults = train_utils.flatten_dict(results)\n\tpickle.dump(results, open(savefile, \"wb\"))\n\t# save model\n\test.export_saved_model(f'{exp_dir}/saved_model', serving_input_fn)\n\tif ((cleanup == 'True') & (debugger == 'False')):\n\t\ttrain_utils.cleanup_directory(checkpoint_dir)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tsavefile",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tsavefile = f\"{exp_dir}/performance.pkl\"\n\tresults = train_utils.flatten_dict(results)\n\tpickle.dump(results, open(savefile, \"wb\"))\n\t# save model\n\test.export_saved_model(f'{exp_dir}/saved_model', serving_input_fn)\n\tif ((cleanup == 'True') & (debugger == 'False')):\n\t\ttrain_utils.cleanup_directory(checkpoint_dir)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "\tresults",
        "kind": 5,
        "importPath": "shared.train",
        "description": "shared.train",
        "peekOfCode": "\tresults = train_utils.flatten_dict(results)\n\tpickle.dump(results, open(savefile, \"wb\"))\n\t# save model\n\test.export_saved_model(f'{exp_dir}/saved_model', serving_input_fn)\n\tif ((cleanup == 'True') & (debugger == 'False')):\n\t\ttrain_utils.cleanup_directory(checkpoint_dir)",
        "detail": "shared.train",
        "documentation": {}
    },
    {
        "label": "restrict_GPU_tf",
        "kind": 2,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "def restrict_GPU_tf(gpuid, memfrac=0, use_cpu=False):\n\t\"\"\" Function to pick the gpu to run on\n\t\tArgs:\n\t\t\tgpuid: str, comma separated list \"0\" or \"0,1\" or even \"0,1,3\"\n\t\t\tmemfrac: float, fraction of memory. By default grows dynamically\n\t\"\"\"\n\tif not use_cpu:\n\t\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid\n\t\tconfig = tf.compat.v1.ConfigProto()\n\t\tif memfrac == 0:",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "cleanup_directory",
        "kind": 2,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "def cleanup_directory(directory):\n\t\"\"\"Deletes all files within directory and its subdirectories.\n\tArgs:\n\t\tdirectory: string, the directory to clean up\n\tReturns:\n\t\tNone\n\t\"\"\"\n\tif os.path.exists(directory):\n\t\tfiles = glob.glob(f\"{directory}/*\", recursive=True)\n\t\tfor f in files:",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "flatten_dict",
        "kind": 2,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "def flatten_dict(dd, separator='_', prefix=''):\n\t\"\"\" Flattens the dictionary with eval metrics \"\"\"\n\treturn {\n\t\tprefix + separator + k if prefix else k: v\n\t\tfor kk, vv in dd.items()\n\t\tfor k, v in flatten_dict(vv, separator, kk).items()\n\t} if isinstance(dd,\n\t\tdict) else {prefix: dd}\ndef config_hasher(config):\n\t\"\"\"Generates hash string for a given config.",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "config_hasher",
        "kind": 2,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "def config_hasher(config):\n\t\"\"\"Generates hash string for a given config.\n\tArgs:\n\t\tconfig: dict with hyperparams ordered by key\n\tReturns:\n\t\thash of config\n\t\"\"\"\n\tconfig_string = ' '.join('--%s %s' % (k, str(v)) for k, v in config.items())\n\thash_string = hashlib.sha256(config_string.encode()).hexdigest()\n\treturn hash_string",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "tried_config",
        "kind": 2,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "def tried_config(config, base_dir):\n\t\"\"\"Tests if config has been tried before.\n\tArgs:\n\t\tconfig: hyperparam config\n\t\tbase_dir: directory where the tuning folder lives\n\t\"\"\"\n\thash_string = config_hasher(config)\n\t# print(hash_string)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tprint(hash_dir)",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "tried_config_file",
        "kind": 2,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "def tried_config_file(config, base_dir):\n\t\"\"\"Tests if config has been tried before.\n\tArgs:\n\t\tconfig: hyperparam config\n\t\tbase_dir: directory where the tuning folder lives\n\t\"\"\"\n\thash_string = config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\t# performance_file = os.path.join(hash_dir, 'asym_performance.pkl')",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "delete_config_file",
        "kind": 2,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "def delete_config_file(config, base_dir):\n\t\"\"\" deletes results for the specified config.\n\t\tArgs:\n\t\tconfig: hyperparam config\n\t\tbase_dir: directory where the tuning folder lives\n\t\"\"\"\n\thash_string = config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tif os.path.exists(hash_dir):\n\t\tcall(f'rm -rf {hash_dir}', shell=True)",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\t\tos.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\t\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid\n\t\tconfig = tf.compat.v1.ConfigProto()\n\t\tif memfrac == 0:\n\t\t\tconfig.gpu_options.allow_growth = True\n\t\telse:\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = memfrac\n\t\ttf.compat.v1.Session(config=config)\n\t\tprint(\"Using GPU:{} with {:.0f}% of the memory\".format(gpuid, memfrac * 100))\n\telse:\n\t\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\t\tconfig",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\t\tconfig = tf.compat.v1.ConfigProto()\n\t\tif memfrac == 0:\n\t\t\tconfig.gpu_options.allow_growth = True\n\t\telse:\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = memfrac\n\t\ttf.compat.v1.Session(config=config)\n\t\tprint(\"Using GPU:{} with {:.0f}% of the memory\".format(gpuid, memfrac * 100))\n\telse:\n\t\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\t\tos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\t\t\tconfig.gpu_options.allow_growth",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\t\t\tconfig.gpu_options.allow_growth = True\n\t\telse:\n\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = memfrac\n\t\ttf.compat.v1.Session(config=config)\n\t\tprint(\"Using GPU:{} with {:.0f}% of the memory\".format(gpuid, memfrac * 100))\n\telse:\n\t\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\t\tos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\t\tprint(\"Using CPU\")\ndef cleanup_directory(directory):",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\t\t\tconfig.gpu_options.per_process_gpu_memory_fraction = memfrac\n\t\ttf.compat.v1.Session(config=config)\n\t\tprint(\"Using GPU:{} with {:.0f}% of the memory\".format(gpuid, memfrac * 100))\n\telse:\n\t\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\t\tos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\t\tprint(\"Using CPU\")\ndef cleanup_directory(directory):\n\t\"\"\"Deletes all files within directory and its subdirectories.\n\tArgs:",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\t\tos.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\t\tos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\t\tos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\t\tprint(\"Using CPU\")\ndef cleanup_directory(directory):\n\t\"\"\"Deletes all files within directory and its subdirectories.\n\tArgs:\n\t\tdirectory: string, the directory to clean up\n\tReturns:\n\t\tNone\n\t\"\"\"",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\t\tos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\t\tos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\t\tprint(\"Using CPU\")\ndef cleanup_directory(directory):\n\t\"\"\"Deletes all files within directory and its subdirectories.\n\tArgs:\n\t\tdirectory: string, the directory to clean up\n\tReturns:\n\t\tNone\n\t\"\"\"\n\tif os.path.exists(directory):",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\t\tfiles",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\t\tfiles = glob.glob(f\"{directory}/*\", recursive=True)\n\t\tfor f in files:\n\t\t\tif os.path.isdir(f):\n\t\t\t\tos.system(f\"rm {f}/* \")\n\t\t\telse:\n\t\t\t\tos.system(f\"rm {f}\")\ndef flatten_dict(dd, separator='_', prefix=''):\n\t\"\"\" Flattens the dictionary with eval metrics \"\"\"\n\treturn {\n\t\tprefix + separator + k if prefix else k: v",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\tconfig_string",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\tconfig_string = ' '.join('--%s %s' % (k, str(v)) for k, v in config.items())\n\thash_string = hashlib.sha256(config_string.encode()).hexdigest()\n\treturn hash_string\ndef tried_config(config, base_dir):\n\t\"\"\"Tests if config has been tried before.\n\tArgs:\n\t\tconfig: hyperparam config\n\t\tbase_dir: directory where the tuning folder lives\n\t\"\"\"\n\thash_string = config_hasher(config)",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\thash_string",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\thash_string = hashlib.sha256(config_string.encode()).hexdigest()\n\treturn hash_string\ndef tried_config(config, base_dir):\n\t\"\"\"Tests if config has been tried before.\n\tArgs:\n\t\tconfig: hyperparam config\n\t\tbase_dir: directory where the tuning folder lives\n\t\"\"\"\n\thash_string = config_hasher(config)\n\t# print(hash_string)",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\thash_string",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\thash_string = config_hasher(config)\n\t# print(hash_string)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tprint(hash_dir)\n\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\t# performance_file = os.path.join(hash_dir, 'asym_performance.pkl')\n\treturn os.path.isfile(performance_file)\ndef tried_config_file(config, base_dir):\n\t\"\"\"Tests if config has been tried before.\n\tArgs:",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\thash_dir",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tprint(hash_dir)\n\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\t# performance_file = os.path.join(hash_dir, 'asym_performance.pkl')\n\treturn os.path.isfile(performance_file)\ndef tried_config_file(config, base_dir):\n\t\"\"\"Tests if config has been tried before.\n\tArgs:\n\t\tconfig: hyperparam config\n\t\tbase_dir: directory where the tuning folder lives",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\tperformance_file",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\t# performance_file = os.path.join(hash_dir, 'asym_performance.pkl')\n\treturn os.path.isfile(performance_file)\ndef tried_config_file(config, base_dir):\n\t\"\"\"Tests if config has been tried before.\n\tArgs:\n\t\tconfig: hyperparam config\n\t\tbase_dir: directory where the tuning folder lives\n\t\"\"\"\n\thash_string = config_hasher(config)",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\thash_string",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\thash_string = config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\t# performance_file = os.path.join(hash_dir, 'asym_performance.pkl')\n\tif os.path.isfile(performance_file):\n\t\treturn config\ndef delete_config_file(config, base_dir):\n\t\"\"\" deletes results for the specified config.\n\t\tArgs:\n\t\tconfig: hyperparam config",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\thash_dir",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\t# performance_file = os.path.join(hash_dir, 'asym_performance.pkl')\n\tif os.path.isfile(performance_file):\n\t\treturn config\ndef delete_config_file(config, base_dir):\n\t\"\"\" deletes results for the specified config.\n\t\tArgs:\n\t\tconfig: hyperparam config\n\t\tbase_dir: directory where the tuning folder lives",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\tperformance_file",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\tperformance_file = os.path.join(hash_dir, 'performance.pkl')\n\t# performance_file = os.path.join(hash_dir, 'asym_performance.pkl')\n\tif os.path.isfile(performance_file):\n\t\treturn config\ndef delete_config_file(config, base_dir):\n\t\"\"\" deletes results for the specified config.\n\t\tArgs:\n\t\tconfig: hyperparam config\n\t\tbase_dir: directory where the tuning folder lives\n\t\"\"\"",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\thash_string",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\thash_string = config_hasher(config)\n\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tif os.path.exists(hash_dir):\n\t\tcall(f'rm -rf {hash_dir}', shell=True)",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "\thash_dir",
        "kind": 5,
        "importPath": "shared.train_utils",
        "description": "shared.train_utils",
        "peekOfCode": "\thash_dir = os.path.join(base_dir, 'tuning', hash_string)\n\tif os.path.exists(hash_dir):\n\t\tcall(f'rm -rf {hash_dir}', shell=True)",
        "detail": "shared.train_utils",
        "documentation": {}
    },
    {
        "label": "sample_y2_on_y1",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def sample_y2_on_y1(df, y0_value, y1_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==y1_value))]\n\tsmall_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==(1-y1_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "sample_y1_on_main",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "fix_marginal",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "get_skewed_data",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n    cand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n    cand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n    cand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)\n    new_cand_df = cand_df11.append(cand_df10).append(cand_df01).append(cand_df00)\n    new_cand_df.reset_index(inplace=True, drop=True)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "save_created_data",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def save_created_data(df, experiment_directory, filename):\n    IMG_MAIN_DIR = \"/nfs/turbo/coe-rbg/\"\n    df['Path'] = IMG_MAIN_DIR + df['Path']\n    df.drop(['uid', 'patient', 'study'], axis = 1, inplace = True)\n    df.to_csv(f'{experiment_directory}/{filename}.csv', index=False)\ndef create_save_chexpert_lists(experiment_directory, p_tr=.7, random_seed=None):\n\tif random_seed is None:\n\t\trng = np.random.RandomState(0)\n\telse:\n\t\trng = np.random.RandomState(random_seed)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "create_save_chexpert_lists",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def create_save_chexpert_lists(experiment_directory, p_tr=.7, random_seed=None):\n\tif random_seed is None:\n\t\trng = np.random.RandomState(0)\n\telse:\n\t\trng = np.random.RandomState(random_seed)\n\t# --- read in the cleaned image filenames (see chexpert_creation)\n\tdf = pd.read_csv('./penumonia_nofinding_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "read_decode_jpg",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def read_decode_jpg(file_path):\n\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)  # Decode a JPEG-encoded image to a uint8 tensor.\n\treturn img\ndef map_to_image_label(img_dir, label):\n    # print(\"@@@@@@@@@@@@@@@@@@@@@image dir: \"+img_dir)\n    img = tf.io.read_file(img_dir)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # Resize and rescale the image\n    img_height = 128",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "map_to_image_label",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def map_to_image_label(img_dir, label):\n    # print(\"@@@@@@@@@@@@@@@@@@@@@image dir: \"+img_dir)\n    img = tf.io.read_file(img_dir)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # Resize and rescale the image\n    img_height = 128\n    img_width = 128\n    img = tf.image.resize(img, (img_height, img_width))\n    img = img / 255\n    # decode number?",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "create_dataset",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def create_dataset(csv_dir, params):\n    df = pd.read_csv(csv_dir)\n    file_paths = df['Path'].values\n    # print(file_paths)\n    label = df['Age'].values   # penumonia or not\n    label = tf.cast(label, tf.float32)\n    batch_size = params['batch_size']\n    ds = tf.data.Dataset.from_tensor_slices((file_paths, label))\n    # print(\"!!!!!!!!!!!!!!!!\")\n    ds = ds.map(map_to_image_label).batch(batch_size)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "load_created_data",
        "kind": 2,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "def load_created_data(data_dir, skew_train, params):\n    skew_str = 'skew' if skew_train == 'True' else 'unskew'\n    train_data = create_dataset(f'{data_dir}/{skew_str}_train.csv', params)\n    valid_data = create_dataset(f'{data_dir}/{skew_str}_valid.csv', params)\n    test_data_dict = {}\n    pskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n    for pskew in pskew_list:\n        test_data = create_dataset(f'{data_dir}/{pskew}_test.csv', params)\n        test_data_dict[pskew] = test_data\n    return train_data, valid_data, test_data_dict",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "MAIN_DIR",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "MAIN_DIR = \"/nfs/turbo/coe-rbg/zhengji/single_shortcut/chexpert/\"\n\"\"\"\nData preparation\n\"\"\"\npath = MAIN_DIR + 'age_data'\nif not os.path.exists(path):\n    os.mkdir(path)\n\"\"\"\nAssume y_value = 0\nDominant probability: P(y0=y_value, y1=y_value) = len(dominant group) / len(group)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "path = MAIN_DIR + 'age_data'\nif not os.path.exists(path):\n    os.mkdir(path)\n\"\"\"\nAssume y_value = 0\nDominant probability: P(y0=y_value, y1=y_value) = len(dominant group) / len(group)\nSmall probability: P(y0=0, y1=1-y_value) = len(small group) / len(group)\nlen(dominant group) \n= len(dominant group) / len(small group) * len(small group)\n= dominant probability / small probability * len(small_group)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tdominant_group",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tdominant_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==y1_value))]\n\tsmall_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==(1-y1_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tsmall_group",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tsmall_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==(1-y1_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tsmall_probability",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\t\tsmall_group,size",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\t\treplace",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tnew_ids",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tdf_new",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tdominant_group",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tsmall_group",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tsmall_probability",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\t\tsmall_group,size",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\t\treplace",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tnew_ids",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tdf_new",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\ty0_group",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\ty1_group",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\ty1_probability",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), ",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\ty0_ids",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\ty1_ids",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\t\treplace",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\ty1_ids",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\ty0_ids",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\t\treplace",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tdff",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tdff.reset_index(inplace",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n    cand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\treshuffled_ids",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n    cand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n    cand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tdff",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n    cand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n    cand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n    cand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\trng",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\trng = np.random.RandomState(0)\n\telse:\n\t\trng = np.random.RandomState(random_seed)\n\t# --- read in the cleaned image filenames (see chexpert_creation)\n\tdf = pd.read_csv('./penumonia_nofinding_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\trng",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\trng = np.random.RandomState(random_seed)\n\t# --- read in the cleaned image filenames (see chexpert_creation)\n\tdf = pd.read_csv('./penumonia_nofinding_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tdf",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tdf = pd.read_csv('./penumonia_nofinding_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\ttr_val_candidates",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tsize",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tts_candidates",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\ttr_candidates",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tval_candidates",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\ttr_candidates_df",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets\n\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d=0.9, py2d=0.9, py00=0.7, rng=rng)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tval_candidates_df",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets\n\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d=0.9, py2d=0.9, py00=0.7, rng=rng)\n\tsave_created_data(tr_sk_df, experiment_directory=experiment_directory,",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tts_candidates_df",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets\n\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d=0.9, py2d=0.9, py00=0.7, rng=rng)\n\tsave_created_data(tr_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_train')",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\ttr_sk_df",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d=0.9, py2d=0.9, py00=0.7, rng=rng)\n\tsave_created_data(tr_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_train')\n\ttr_usk_df = get_skewed_data(tr_candidates_df, py1d=0.5, py2d=0.5, py00=0.7, rng=rng)\n\tsave_created_data(tr_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_train')\n\t# --- get validation datasets\n\tval_sk_df = get_skewed_data(val_candidates_df, py1d=0.9, py2d=0.9, py00=0.7, rng=rng)\n\tsave_created_data(val_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_valid')",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\ttr_usk_df",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\ttr_usk_df = get_skewed_data(tr_candidates_df, py1d=0.5, py2d=0.5, py00=0.7, rng=rng)\n\tsave_created_data(tr_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_train')\n\t# --- get validation datasets\n\tval_sk_df = get_skewed_data(val_candidates_df, py1d=0.9, py2d=0.9, py00=0.7, rng=rng)\n\tsave_created_data(val_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_valid')\n\tval_usk_df = get_skewed_data(val_candidates_df, py1d=0.5, py2d=0.5, py00=0.7, rng=rng)\n\tsave_created_data(val_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_valid')",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tval_sk_df",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tval_sk_df = get_skewed_data(val_candidates_df, py1d=0.9, py2d=0.9, py00=0.7, rng=rng)\n\tsave_created_data(val_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_valid')\n\tval_usk_df = get_skewed_data(val_candidates_df, py1d=0.5, py2d=0.5, py00=0.7, rng=rng)\n\tsave_created_data(val_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_valid')\n\t# --- get test\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00 = 0.7, rng=rng)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tval_usk_df",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tval_usk_df = get_skewed_data(val_candidates_df, py1d=0.5, py2d=0.5, py00=0.7, rng=rng)\n\tsave_created_data(val_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_valid')\n\t# --- get test\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00 = 0.7, rng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_test')\ndef read_decode_jpg(file_path):",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\tpskew_list",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00 = 0.7, rng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_test')\ndef read_decode_jpg(file_path):\n\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)  # Decode a JPEG-encoded image to a uint8 tensor.\n\treturn img\ndef map_to_image_label(img_dir, label):",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\t\tts_sk_df",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00 = 0.7, rng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_test')\ndef read_decode_jpg(file_path):\n\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)  # Decode a JPEG-encoded image to a uint8 tensor.\n\treturn img\ndef map_to_image_label(img_dir, label):\n    # print(\"@@@@@@@@@@@@@@@@@@@@@image dir: \"+img_dir)\n    img = tf.io.read_file(img_dir)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\timg = tf.io.read_file(file_path)\n\timg = tf.image.decode_jpeg(img, channels=3)  # Decode a JPEG-encoded image to a uint8 tensor.\n\treturn img\ndef map_to_image_label(img_dir, label):\n    # print(\"@@@@@@@@@@@@@@@@@@@@@image dir: \"+img_dir)\n    img = tf.io.read_file(img_dir)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # Resize and rescale the image\n    img_height = 128\n    img_width = 128",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "\timg",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "\timg = tf.image.decode_jpeg(img, channels=3)  # Decode a JPEG-encoded image to a uint8 tensor.\n\treturn img\ndef map_to_image_label(img_dir, label):\n    # print(\"@@@@@@@@@@@@@@@@@@@@@image dir: \"+img_dir)\n    img = tf.io.read_file(img_dir)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # Resize and rescale the image\n    img_height = 128\n    img_width = 128\n    img = tf.image.resize(img, (img_height, img_width))",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "pre_trained_model",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "pre_trained_model = ResNet50(input_shape=(128, 128, 3), \n                             include_top=False, \n                             weights='imagenet')\nlast_layer = pre_trained_model.get_layer('conv5_block3_3_conv')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output \n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "last_layer",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "last_layer = pre_trained_model.get_layer('conv5_block3_3_conv')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output \n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "last_output",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "last_output = last_layer.output \n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1, activation='linear')(x)  \nmodel = Model(pre_trained_model.input, x) ",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "x = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1, activation='linear')(x)  \nmodel = Model(pre_trained_model.input, x) \nmodel.summary()\nparams = {}",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "x = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1, activation='linear')(x)  \nmodel = Model(pre_trained_model.input, x) \nmodel.summary()\nparams = {}\nparams['batch_size'] = 64\nparams['epoch_num'] = 20",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "x = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1, activation='linear')(x)  \nmodel = Model(pre_trained_model.input, x) \nmodel.summary()\nparams = {}\nparams['batch_size'] = 64\nparams['epoch_num'] = 20\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR + 'age_data')",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "x = layers.Dense(1, activation='linear')(x)  \nmodel = Model(pre_trained_model.input, x) \nmodel.summary()\nparams = {}\nparams['batch_size'] = 64\nparams['epoch_num'] = 20\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR + 'age_data')\n# print(\"Finish creating the dataset csv\")\n# # STEP 2: Create training, validation and testing dataset",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "model = Model(pre_trained_model.input, x) \nmodel.summary()\nparams = {}\nparams['batch_size'] = 64\nparams['epoch_num'] = 20\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR + 'age_data')\n# print(\"Finish creating the dataset csv\")\n# # STEP 2: Create training, validation and testing dataset\ndata_dir = MAIN_DIR + 'age_data'",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "params",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "params = {}\nparams['batch_size'] = 64\nparams['epoch_num'] = 20\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR + 'age_data')\n# print(\"Finish creating the dataset csv\")\n# # STEP 2: Create training, validation and testing dataset\ndata_dir = MAIN_DIR + 'age_data'\ntrain_ds, valid_ds, test_ds_dict = load_created_data(data_dir, False, params)\nprint(\"Finish creating the training, validation, and testing dataset\")",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "params['batch_size']",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "params['batch_size'] = 64\nparams['epoch_num'] = 20\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR + 'age_data')\n# print(\"Finish creating the dataset csv\")\n# # STEP 2: Create training, validation and testing dataset\ndata_dir = MAIN_DIR + 'age_data'\ntrain_ds, valid_ds, test_ds_dict = load_created_data(data_dir, False, params)\nprint(\"Finish creating the training, validation, and testing dataset\")\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "params['epoch_num']",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "params['epoch_num'] = 20\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR + 'age_data')\n# print(\"Finish creating the dataset csv\")\n# # STEP 2: Create training, validation and testing dataset\ndata_dir = MAIN_DIR + 'age_data'\ntrain_ds, valid_ds, test_ds_dict = load_created_data(data_dir, False, params)\nprint(\"Finish creating the training, validation, and testing dataset\")\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\nhistory = model.fit(train_ds, epochs=params['epoch_num'], validation_data=valid_ds)",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "data_dir",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "data_dir = MAIN_DIR + 'age_data'\ntrain_ds, valid_ds, test_ds_dict = load_created_data(data_dir, False, params)\nprint(\"Finish creating the training, validation, and testing dataset\")\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\nhistory = model.fit(train_ds, epochs=params['epoch_num'], validation_data=valid_ds)\nmodel.evaluate(test_ds_dict[0.5])\nimport matplotlib.pyplot as plt\n# %matplotlib inline\n# acc = history.history['acc']\n# val_acc = history.history['val_acc']",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "history",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "history = model.fit(train_ds, epochs=params['epoch_num'], validation_data=valid_ds)\nmodel.evaluate(test_ds_dict[0.5])\nimport matplotlib.pyplot as plt\n# %matplotlib inline\n# acc = history.history['acc']\n# val_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(loss))\n# fig1 = plt.gcf()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "loss",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(loss))\n# fig1 = plt.gcf()\n# plt.plot(epochs, acc, 'r', label='Training accuracy')\n# plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n# plt.title('Training and validation accuracy')\n# plt.legend(loc=0)\n# fig1.savefig(accuracy_plot)\n# plt.figure()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "val_loss",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "val_loss = history.history['val_loss']\nepochs = range(len(loss))\n# fig1 = plt.gcf()\n# plt.plot(epochs, acc, 'r', label='Training accuracy')\n# plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n# plt.title('Training and validation accuracy')\n# plt.legend(loc=0)\n# fig1.savefig(accuracy_plot)\n# plt.figure()\n# plt.show()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "epochs = range(len(loss))\n# fig1 = plt.gcf()\n# plt.plot(epochs, acc, 'r', label='Training accuracy')\n# plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n# plt.title('Training and validation accuracy')\n# plt.legend(loc=0)\n# fig1.savefig(accuracy_plot)\n# plt.figure()\n# plt.show()\nfig1 = plt.gcf()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "fig1",
        "kind": 5,
        "importPath": "age_model",
        "description": "age_model",
        "peekOfCode": "fig1 = plt.gcf()\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend(loc=0)\nplt.savefig('age_plot.png')\n# plt.figure()\n# plt.show()",
        "detail": "age_model",
        "documentation": {}
    },
    {
        "label": "PretrainedResNet50",
        "kind": 6,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "class PretrainedResNet50(tf.keras.Model):\n\tdef __init__(self, embedding_dim=10, l2_penalty=0.0, l2_penalty_last_only=False):\n\t\tsuper(PretrainedResNet50, self).__init__()\n\t\tself.embedding_dim = embedding_dim\n\t\tself.resnet = ResNet50(include_top=False, layers=tf.keras.layers,\n\t\t                       weights='imagenet')\n\t\tself.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n\t\tif not l2_penalty_last_only:\n\t\t\tregularizer = tf.keras.regularizers.l2(l2_penalty)\n\t\t\tfor layer in self.resnet.layers:",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "sample_y2_on_y1",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def sample_y2_on_y1(df, y0_value, y1_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==y1_value))]\n\tsmall_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==(1-y1_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "sample_y1_on_main",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "fix_marginal",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "get_skewed_data",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n    cand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n    cand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n    cand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)\n    new_cand_df = cand_df11.append(cand_df10).append(cand_df01).append(cand_df00)\n    new_cand_df.reset_index(inplace=True, drop=True)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "save_created_data",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def save_created_data(df, experiment_directory, filename):\n    IMG_MAIN_DIR = \"/nfs/turbo/coe-rbg/\"\n    df['Path'] = IMG_MAIN_DIR + df['Path']\n    df.drop(['uid', 'patient', 'study'], axis = 1, inplace = True)\n    df.to_csv(f'{experiment_directory}/{filename}.csv', index=False)\ndef create_save_chexpert_lists(experiment_directory, p_tr=.7, random_seed=None):\n\tif random_seed is None:\n\t\trng = np.random.RandomState(0)\n\telse:\n\t\trng = np.random.RandomState(random_seed)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "create_save_chexpert_lists",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def create_save_chexpert_lists(experiment_directory, p_tr=.7, random_seed=None):\n\tif random_seed is None:\n\t\trng = np.random.RandomState(0)\n\telse:\n\t\trng = np.random.RandomState(random_seed)\n\t# --- read in the cleaned image filenames (see chexpert_creation)\n\tdf = pd.read_csv(MAIN_DIR + 'penumonia_nofinding_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "read_decode_jpg",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def read_decode_jpg(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # img = tf.dtypes.cast(img, tf.float64)\n    return img\ndef decode_number(label):\n\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(img_dir, label):",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "decode_number",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def decode_number(label):\n\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(img_dir, label):\n    img = read_decode_jpg(img_dir)\n    # Resize and rescale the image\n    img_height = 128\n    img_width = 128\n    img = tf.image.resize(img, (img_height, img_width))",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "map_to_image_label",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def map_to_image_label(img_dir, label):\n    img = read_decode_jpg(img_dir)\n    # Resize and rescale the image\n    img_height = 128\n    img_width = 128\n    img = tf.image.resize(img, (img_height, img_width))\n    img = img / 255\n    print(f\"Resized image shape: {img.shape}\")\n    # decode number?\n    print(label.shape)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "create_dataset",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def create_dataset(csv_dir, params):\n    df = pd.read_csv(csv_dir)\n    file_paths = df['Path'].values\n    y0 = df['y0'].values   # penumonia or not\n    y1 = df['y1'].values   # sex male = 1, female = 0\n    y2 = df['y2'].values   # Age >= 50 1, < 50 = 0\n    labels = tf.stack([y0, y1, y2], axis = 1)\n    print(f\"labels shape {labels.shape}\")\n    labels = tf.cast(labels, tf.float32)\n    batch_size = params['batch_size']",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "load_created_data",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def load_created_data(data_dir, skew_train, params):\n    skew_str = 'skew' if skew_train == 'True' else 'unskew'\n    train_data = create_dataset(f'{data_dir}/{skew_str}_train.csv', params)\n    valid_data = create_dataset(f'{data_dir}/{skew_str}_valid.csv', params)\n    test_data_dict = {}\n    pskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n    for pskew in pskew_list:\n        test_data = create_dataset(f'{data_dir}/{pskew}_test.csv', params)\n        test_data_dict[pskew] = test_data\n    return train_data, valid_data, test_data_dict",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "hsic",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def hsic(x, y, sigma=1.0):\n    \"\"\" Computes the HSIC between two arbitrary variables x, y for kernels with lengthscale sigma\"\"\"\n    kernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n        amplitude=1.0, length_scale=sigma)\n    kernel_xx = kernel_fxx.matrix(x, x)\n    kernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(\n        amplitude=1.0, length_scale=sigma)\n    kernel_yy = kernel_fyy.matrix(y, y)\n    tK = kernel_xx - tf.linalg.diag_part(kernel_xx)\n    tL = kernel_yy - tf.linalg.diag_part(kernel_yy)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "compute_loss_unweighted",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def compute_loss_unweighted(labels, logits, z_pred, params):\n    # labels: ground truth labels([y0(pnemounia), y1(sex), y2(age)])\n    # logits: predicted label(pnemounia)\n    # z_pred: a learned representation vector\n    y_main = tf.expand_dims(labels[:, 0], axis = -1)\n    individual_losses = tf.keras.losses.binary_crossentropy(y_main, logits, from_logits=True)\n    unweighted_loss = tf.reduce_mean(individual_losses)\n    aux_y = labels[:, 1:]\n    hsic_loss = hsic(z_pred, aux_y, sigma = params['sigma'])\n    return unweighted_loss, hsic_loss",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "auroc",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def auroc(auc_metric, labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\t# auc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\t# auc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric.result()\ndef accuracy(acc, labels, predictions):\n    \"\"\" Computes Accuracy\"\"\"\n    # acc = tf.keras.metrics.Accuracy()\n    # acc.reset_states()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def accuracy(acc, labels, predictions):\n    \"\"\" Computes Accuracy\"\"\"\n    # acc = tf.keras.metrics.Accuracy()\n    # acc.reset_states()\n    acc.update_state(y_true=labels, y_pred=predictions)\n    return acc.result()\ndef update_eval_metrics_dict(acc, acc_metric, labels, predictions):\n\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\teval_metrics_dict = {}\n\teval_metrics_dict['accuracy'] = accuracy(acc,",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "update_eval_metrics_dict",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def update_eval_metrics_dict(acc, acc_metric, labels, predictions):\n\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\teval_metrics_dict = {}\n\teval_metrics_dict['accuracy'] = accuracy(acc,\n\t\tlabels=y_main, predictions=predictions[\"classes\"])\n\teval_metrics_dict[\"auc\"] = auroc(acc_metric, \n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])\n\treturn eval_metrics_dict\n# Training function for one step\ndef train_step(model, optimizer, x, y):",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "train_step",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def train_step(model, optimizer, x, y):\n    with tf.GradientTape() as tape:\n        logits, zpred = model(x)\n        # y_pred: predicted probability\n        ypred = tf.nn.sigmoid(logits)\n        predictions = {\n            \"classes\": tf.cast(tf.math.greater_equal(ypred, .5), dtype=tf.float32),\n            \"logits\": logits,\n            \"probabilities\": ypred,\n            \"embedding\": zpred",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "eval_step",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def eval_step(model, x, y):\n    # logits: prediction, zpred: representation vec\n    logits, zpred = model(x)\n    # y_pred: predicted probability\n    ypred = tf.nn.sigmoid(logits)\n    predictions = {\n        \"classes\": tf.cast(tf.math.greater_equal(ypred, .5), dtype=tf.float32),\n        \"logits\": logits,\n        \"probabilities\": ypred,\n        \"embedding\": zpred",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "train_eval",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def train_eval(params):\n    optimizer = tf.keras.optimizers.Adam(learning_rate=params['lr'])\n    model = PretrainedResNet50(embedding_dim=params['embedding_dim'], l2_penalty=params['l2_penalty'])\n    for epoch in range(params['num_epochs']):\n        print(f\"\\nTraining epoch {epoch}\")\n        for step, (x, y) in enumerate(train_ds):\n            train_step(model, optimizer, x, y)\n        with train_summary_writer.as_default():\n            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "test_step",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def test_step(model, x, y):\n    # logits: prediction, zpred: representation vec\n    logits, zpred = model(x)\n    # y_pred: predicted probability\n    ypred = tf.nn.sigmoid(logits)\n    predictions = {\n        \"classes\": tf.cast(tf.math.greater_equal(ypred, .5), dtype=tf.float32),\n        \"logits\": logits,\n        \"probabilities\": ypred,\n        \"embedding\": zpred",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "def test(model, params):\n    # model = tf.keras.models.load_model(model_dir)\n    pskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n    for pskew in pskew_list:\n        test_ds = test_ds_dict[pskew]\n        for step, (x, y) in enumerate(test_ds):\n            test_step(model, x, y)\n        print(\"\\n*****************************\")\n        print(f\"Test result for pskew={pskew}\")\n        print(f\"loss: {test_loss.result()}\")",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "MAIN_DIR",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "MAIN_DIR = \"/nfs/turbo/coe-rbg/zhengji/single_shortcut/chexpert/\"\n\"\"\" Dataset \"\"\"\n# I. Data Selection: select the data based on the given probability\npath = MAIN_DIR + 'data'\nif not os.path.exists(path):\n    os.mkdir(path)\n\"\"\"\nAssume y_value = 0\nDominant probability: P(y0=y_value, y1=y_value) = len(dominant group) / len(group)\nSmall probability: P(y0=0, y1=1-y_value) = len(small group) / len(group)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "path = MAIN_DIR + 'data'\nif not os.path.exists(path):\n    os.mkdir(path)\n\"\"\"\nAssume y_value = 0\nDominant probability: P(y0=y_value, y1=y_value) = len(dominant group) / len(group)\nSmall probability: P(y0=0, y1=1-y_value) = len(small group) / len(group)\nlen(dominant group) \n= len(dominant group) / len(small group) * len(small group)\n= dominant probability / small probability * len(small_group)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tdominant_group",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tdominant_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==y1_value))]\n\tsmall_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==(1-y1_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tsmall_group",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tsmall_group = df.index[((df.y0==y0_value) & (df.y1==y1_value) & (df.y2==(1-y1_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tsmall_probability",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\t# If the dominant group is smaller than dominant probability*len(group)\n\t# Truncate the size of the small group based on the dominant probability\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\t\tsmall_group,size",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\t\treplace",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\t# If the small group if smaller than small probability*len(group)\n\t# Truncate the size of the large group based on the small probability\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tnew_ids",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tdf_new",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef sample_y1_on_main(df, y_value, dominant_probability, rng):\n\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tdominant_group",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tdominant_group = df.index[((df.y0==y_value) & (df.y1 ==y_value))]\n\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tsmall_group",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tsmall_group = df.index[((df.y0==y_value) & (df.y1 ==(1-y_value)))]\n\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tsmall_probability",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tsmall_probability = 1 - dominant_probability \n\t# CASE I: Smaller group too large, Dominant group too small\n\tif len(dominant_group) < (dominant_probability/small_probability)*len(small_group):\n\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tdominant_id = deepcopy(dominant_group).tolist()\n\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tsmall_id = rng.choice(\n\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\t\tsmall_group,size",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\t\tsmall_group,size = int(\n\t\t\t\t(small_probability/dominant_probability)* len(dominant_group)\n\t\t\t),\n\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\t\treplace",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\t\treplace = False).tolist()\n\t# CASE II: Dominant group too large, smaller group too small\n\telif len(small_group) < (small_probability/dominant_probability)*len(dominant_group):\n\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tsmall_id",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tsmall_id = deepcopy(small_group).tolist()\n\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tdominant_id",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tdominant_id = rng.choice(\n\t\t\tdominant_group, size = int(\n\t\t\t\t(dominant_probability/small_probability)*len(small_group)\n\t\t\t), replace = False).tolist()\n\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tnew_ids",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tnew_ids = small_id + dominant_id\n\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tdf_new",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tdf_new = df.iloc[new_ids]\n\treturn df_new\ndef fix_marginal(df, y0_probability, rng):\n\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\ty0_group",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\ty0_group = df.index[(df.y0 == 0)]\n\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\ty1_group",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\ty1_group = df.index[(df.y0 == 1)]\n\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\ty1_probability",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\ty1_probability = 1 - y0_probability \n\tif len(y0_group) < (y0_probability/y1_probability) * len(y1_group):\n\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), ",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\ty0_ids",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\ty0_ids = deepcopy(y0_group).tolist()\n\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\ty1_ids",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\ty1_ids = rng.choice(\n\t\t\ty1_group, size = int((y1_probability/y0_probability) * len(y0_group)),\n\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\t\treplace",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\t\treplace = False).tolist()\n\telif len(y1_group) < (y1_probability/y0_probability) * len(y0_group):\n\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\ty1_ids",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\ty1_ids = deepcopy(y1_group).tolist()\n\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\ty0_ids",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\ty0_ids = rng.choice(\n\t\t\ty0_group, size = int( (y0_probability/y1_probability)*len(y1_group)), \n\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\t\treplace",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\t\treplace = False\n\t\t).tolist()\n\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tdff",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tdff = df.iloc[y1_ids + y0_ids]\n\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tdff.reset_index(inplace",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tdff.reset_index(inplace = True, drop=True)\n\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n    cand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\treshuffled_ids",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\treshuffled_ids = rng.choice(dff.index, size = len(dff.index), replace=False).tolist()\n\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n    cand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n    cand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tdff",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tdff = dff.iloc[reshuffled_ids].reset_index(drop = True)\n\treturn dff\ndef get_skewed_data(cand_df, py1d=0.9, py2d=0.9, py00=0.7, rng=None):\n    if rng is None:\n        rng = np.random.RandomState(0)\n    # --- Fix the conditional distributions of y2\n    cand_df11 = sample_y2_on_y1(cand_df, 1, 1, py2d, rng)\n    cand_df10 = sample_y2_on_y1(cand_df, 1, 0, py2d, rng)\n    cand_df01 = sample_y2_on_y1(cand_df, 0, 1, py2d, rng)\n    cand_df00 = sample_y2_on_y1(cand_df, 0, 0, py2d, rng)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\trng",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\trng = np.random.RandomState(0)\n\telse:\n\t\trng = np.random.RandomState(random_seed)\n\t# --- read in the cleaned image filenames (see chexpert_creation)\n\tdf = pd.read_csv(MAIN_DIR + 'penumonia_nofinding_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\trng",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\trng = np.random.RandomState(random_seed)\n\t# --- read in the cleaned image filenames (see chexpert_creation)\n\tdf = pd.read_csv(MAIN_DIR + 'penumonia_nofinding_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tdf",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tdf = pd.read_csv(MAIN_DIR + 'penumonia_nofinding_cohort.csv')\n\t# ---- split into train and test patients\n\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\ttr_val_candidates",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\ttr_val_candidates = rng.choice(df.patient.unique(),\n\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tsize",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tsize = int(len(df.patient.unique())*p_tr), replace = False).tolist()\n\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tts_candidates",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tts_candidates = list(set(df.patient.unique()) - set(tr_val_candidates))\n\t# --- split training into training and validation\n\t# TODO: don't hard code the validation percent\n\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\ttr_candidates",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\ttr_candidates = rng.choice(tr_val_candidates,\n\t\tsize=int(0.75 * len(tr_val_candidates)), replace=False).tolist()\n\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tval_candidates",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tval_candidates = list(set(tr_val_candidates) - set(tr_candidates))\n\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\ttr_candidates_df",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\ttr_candidates_df = df[(df.patient.isin(tr_candidates))].reset_index(drop=True)\n\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets\n\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d = 0.9, py2d=0.9, py00 = 0.7, rng=rng)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tval_candidates_df",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tval_candidates_df = df[(df.patient.isin(val_candidates))].reset_index(drop=True)\n\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets\n\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d = 0.9, py2d=0.9, py00 = 0.7, rng=rng)\n\tsave_created_data(tr_sk_df, experiment_directory=experiment_directory,",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tts_candidates_df",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tts_candidates_df = df[(df.patient.isin(ts_candidates))].reset_index(drop=True)\n\t# --- checks\n\tassert len(ts_candidates) + len(tr_candidates) + len(val_candidates) == len(df.patient.unique())\n\tassert len(set(ts_candidates) & set(tr_candidates)) == 0\n\tassert len(set(ts_candidates) & set(val_candidates)) == 0\n\tassert len(set(tr_candidates) & set(val_candidates)) == 0\n\t# --- get train datasets\n\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d = 0.9, py2d=0.9, py00 = 0.7, rng=rng)\n\tsave_created_data(tr_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_train')",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\ttr_sk_df",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\ttr_sk_df = get_skewed_data(tr_candidates_df, py1d = 0.9, py2d=0.9, py00 = 0.7, rng=rng)\n\tsave_created_data(tr_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_train')\n\ttr_usk_df = get_skewed_data(tr_candidates_df, py1d = 0.5, py2d=0.5, py00 = 0.7, rng=rng)\n\tsave_created_data(tr_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_train')\n\t# --- get validation datasets\n\tval_sk_df = get_skewed_data(val_candidates_df, py1d = 0.9, py2d=0.9, py00 = 0.7, rng=rng)\n\tsave_created_data(val_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_valid')",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\ttr_usk_df",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\ttr_usk_df = get_skewed_data(tr_candidates_df, py1d = 0.5, py2d=0.5, py00 = 0.7, rng=rng)\n\tsave_created_data(tr_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_train')\n\t# --- get validation datasets\n\tval_sk_df = get_skewed_data(val_candidates_df, py1d = 0.9, py2d=0.9, py00 = 0.7, rng=rng)\n\tsave_created_data(val_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_valid')\n\tval_usk_df = get_skewed_data(val_candidates_df, py1d = 0.5, py2d=0.5, py00 = 0.7, rng=rng)\n\tsave_created_data(val_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_valid')",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tval_sk_df",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tval_sk_df = get_skewed_data(val_candidates_df, py1d = 0.9, py2d=0.9, py00 = 0.7, rng=rng)\n\tsave_created_data(val_sk_df, experiment_directory=experiment_directory,\n\t\tfilename='skew_valid')\n\tval_usk_df = get_skewed_data(val_candidates_df, py1d = 0.5, py2d=0.5, py00 = 0.7, rng=rng)\n\tsave_created_data(val_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_valid')\n\t# --- get test\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00 = 0.7, rng=rng)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tval_usk_df",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tval_usk_df = get_skewed_data(val_candidates_df, py1d = 0.5, py2d=0.5, py00 = 0.7, rng=rng)\n\tsave_created_data(val_usk_df, experiment_directory=experiment_directory,\n\t\tfilename='unskew_valid')\n\t# --- get test\n\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00 = 0.7, rng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_test')\n# II. Dataset preparation",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tpskew_list",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tpskew_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]\n\tfor pskew in pskew_list:\n\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00 = 0.7, rng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_test')\n# II. Dataset preparation\ndef read_decode_jpg(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # img = tf.dtypes.cast(img, tf.float64)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tts_sk_df",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tts_sk_df = get_skewed_data(ts_candidates_df, py1d=pskew, py2d=pskew, py00 = 0.7, rng=rng)\n\t\tsave_created_data(ts_sk_df, experiment_directory=experiment_directory,\n\t\t\tfilename=f'{pskew}_test')\n# II. Dataset preparation\ndef read_decode_jpg(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # img = tf.dtypes.cast(img, tf.float64)\n    return img\ndef decode_number(label):",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tlabel",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tlabel = tf.expand_dims(label, 0)\n\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(img_dir, label):\n    img = read_decode_jpg(img_dir)\n    # Resize and rescale the image\n    img_height = 128\n    img_width = 128\n    img = tf.image.resize(img, (img_height, img_width))\n    img = img / 255",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\tlabel",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\tlabel = tf.strings.to_number(label)\n\treturn label\ndef map_to_image_label(img_dir, label):\n    img = read_decode_jpg(img_dir)\n    # Resize and rescale the image\n    img_height = 128\n    img_width = 128\n    img = tf.image.resize(img, (img_height, img_width))\n    img = img / 255\n    print(f\"Resized image shape: {img.shape}\")",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tself.embedding_dim",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tself.embedding_dim = embedding_dim\n\t\tself.resnet = ResNet50(include_top=False, layers=tf.keras.layers,\n\t\t                       weights='imagenet')\n\t\tself.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n\t\tif not l2_penalty_last_only:\n\t\t\tregularizer = tf.keras.regularizers.l2(l2_penalty)\n\t\t\tfor layer in self.resnet.layers:\n\t\t\t\tif hasattr(layer, 'kernel'):\n\t\t\t\t\tself.add_loss(lambda layer=layer: regularizer(layer.kernel))\n\t\tif self.embedding_dim != 10:",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tself.resnet",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tself.resnet = ResNet50(include_top=False, layers=tf.keras.layers,\n\t\t                       weights='imagenet')\n\t\tself.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n\t\tif not l2_penalty_last_only:\n\t\t\tregularizer = tf.keras.regularizers.l2(l2_penalty)\n\t\t\tfor layer in self.resnet.layers:\n\t\t\t\tif hasattr(layer, 'kernel'):\n\t\t\t\t\tself.add_loss(lambda layer=layer: regularizer(layer.kernel))\n\t\tif self.embedding_dim != 10:\n\t\t\tself.embedding = tf.keras.layers.Dense(self.embedding_dim,",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tself.avg_pool",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tself.avg_pool = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')\n\t\tif not l2_penalty_last_only:\n\t\t\tregularizer = tf.keras.regularizers.l2(l2_penalty)\n\t\t\tfor layer in self.resnet.layers:\n\t\t\t\tif hasattr(layer, 'kernel'):\n\t\t\t\t\tself.add_loss(lambda layer=layer: regularizer(layer.kernel))\n\t\tif self.embedding_dim != 10:\n\t\t\tself.embedding = tf.keras.layers.Dense(self.embedding_dim,\n\t\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t\tself.dense = tf.keras.layers.Dense(1,",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\t\tregularizer",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\t\tregularizer = tf.keras.regularizers.l2(l2_penalty)\n\t\t\tfor layer in self.resnet.layers:\n\t\t\t\tif hasattr(layer, 'kernel'):\n\t\t\t\t\tself.add_loss(lambda layer=layer: regularizer(layer.kernel))\n\t\tif self.embedding_dim != 10:\n\t\t\tself.embedding = tf.keras.layers.Dense(self.embedding_dim,\n\t\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t\tself.dense = tf.keras.layers.Dense(1,\n\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t@tf.function",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\t\tself.embedding",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\t\tself.embedding = tf.keras.layers.Dense(self.embedding_dim,\n\t\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t\tself.dense = tf.keras.layers.Dense(1,\n\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t@tf.function\n\tdef call(self, inputs):\n\t\tx = self.resnet(inputs)\n\t\tx = self.avg_pool(x)\n\t\tx = self.embedding(x)\n\t\treturn self.dense(x), x",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tself.dense",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tself.dense = tf.keras.layers.Dense(1,\n\t\t\tkernel_regularizer=tf.keras.regularizers.l2(l2_penalty))\n\t@tf.function\n\tdef call(self, inputs):\n\t\tx = self.resnet(inputs)\n\t\tx = self.avg_pool(x)\n\t\tx = self.embedding(x)\n\t\treturn self.dense(x), x\ndef hsic(x, y, sigma=1.0):\n    \"\"\" Computes the HSIC between two arbitrary variables x, y for kernels with lengthscale sigma\"\"\"",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tx",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tx = self.resnet(inputs)\n\t\tx = self.avg_pool(x)\n\t\tx = self.embedding(x)\n\t\treturn self.dense(x), x\ndef hsic(x, y, sigma=1.0):\n    \"\"\" Computes the HSIC between two arbitrary variables x, y for kernels with lengthscale sigma\"\"\"\n    kernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n        amplitude=1.0, length_scale=sigma)\n    kernel_xx = kernel_fxx.matrix(x, x)\n    kernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tx",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tx = self.avg_pool(x)\n\t\tx = self.embedding(x)\n\t\treturn self.dense(x), x\ndef hsic(x, y, sigma=1.0):\n    \"\"\" Computes the HSIC between two arbitrary variables x, y for kernels with lengthscale sigma\"\"\"\n    kernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n        amplitude=1.0, length_scale=sigma)\n    kernel_xx = kernel_fxx.matrix(x, x)\n    kernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(\n        amplitude=1.0, length_scale=sigma)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\t\tx",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\t\tx = self.embedding(x)\n\t\treturn self.dense(x), x\ndef hsic(x, y, sigma=1.0):\n    \"\"\" Computes the HSIC between two arbitrary variables x, y for kernels with lengthscale sigma\"\"\"\n    kernel_fxx = tfp.math.psd_kernels.ExponentiatedQuadratic(\n        amplitude=1.0, length_scale=sigma)\n    kernel_xx = kernel_fxx.matrix(x, x)\n    kernel_fyy = tfp.math.psd_kernels.ExponentiatedQuadratic(\n        amplitude=1.0, length_scale=sigma)\n    kernel_yy = kernel_fyy.matrix(y, y)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "train_loss",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\ntrain_accuracy = tf.keras.metrics.Accuracy('train_accuracy')\ntrain_auroc = tf.keras.metrics.AUC(name='train_auroc')\neval_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\neval_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\neval_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ntest_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\ntest_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ndef auroc(auc_metric, labels, predictions):",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "train_accuracy",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "train_accuracy = tf.keras.metrics.Accuracy('train_accuracy')\ntrain_auroc = tf.keras.metrics.AUC(name='train_auroc')\neval_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\neval_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\neval_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ntest_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\ntest_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ndef auroc(auc_metric, labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "train_auroc",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "train_auroc = tf.keras.metrics.AUC(name='train_auroc')\neval_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\neval_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\neval_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ntest_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\ntest_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ndef auroc(auc_metric, labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\t# auc_metric = tf.keras.metrics.AUC(name=\"auroc\")",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "eval_loss",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "eval_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\neval_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\neval_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ntest_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\ntest_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ndef auroc(auc_metric, labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\t# auc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\t# auc_metric.reset_states()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "eval_accuracy",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "eval_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\neval_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ntest_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\ntest_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ndef auroc(auc_metric, labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\t# auc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\t# auc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "eval_auroc",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "eval_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ntest_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\ntest_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ndef auroc(auc_metric, labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\t# auc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\t# auc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric.result()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "test_loss",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "test_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\ntest_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ndef auroc(auc_metric, labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\t# auc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\t# auc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric.result()\ndef accuracy(acc, labels, predictions):",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "test_accuracy",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "test_accuracy = tf.keras.metrics.Accuracy('eval_accuracy')\ntest_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ndef auroc(auc_metric, labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\t# auc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\t# auc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric.result()\ndef accuracy(acc, labels, predictions):\n    \"\"\" Computes Accuracy\"\"\"",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "test_auroc",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "test_auroc = tf.keras.metrics.AUC(name='eval_auroc')\ndef auroc(auc_metric, labels, predictions):\n\t\"\"\" Computes AUROC \"\"\"\n\t# auc_metric = tf.keras.metrics.AUC(name=\"auroc\")\n\t# auc_metric.reset_states()\n\tauc_metric.update_state(y_true=labels, y_pred=predictions)\n\treturn auc_metric.result()\ndef accuracy(acc, labels, predictions):\n    \"\"\" Computes Accuracy\"\"\"\n    # acc = tf.keras.metrics.Accuracy()",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\ty_main",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\ty_main = tf.expand_dims(labels[:, 0], axis=-1)\n\teval_metrics_dict = {}\n\teval_metrics_dict['accuracy'] = accuracy(acc,\n\t\tlabels=y_main, predictions=predictions[\"classes\"])\n\teval_metrics_dict[\"auc\"] = auroc(acc_metric, \n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])\n\treturn eval_metrics_dict\n# Training function for one step\ndef train_step(model, optimizer, x, y):\n    with tf.GradientTape() as tape:",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\teval_metrics_dict",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\teval_metrics_dict = {}\n\teval_metrics_dict['accuracy'] = accuracy(acc,\n\t\tlabels=y_main, predictions=predictions[\"classes\"])\n\teval_metrics_dict[\"auc\"] = auroc(acc_metric, \n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])\n\treturn eval_metrics_dict\n# Training function for one step\ndef train_step(model, optimizer, x, y):\n    with tf.GradientTape() as tape:\n        logits, zpred = model(x)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\teval_metrics_dict['accuracy']",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\teval_metrics_dict['accuracy'] = accuracy(acc,\n\t\tlabels=y_main, predictions=predictions[\"classes\"])\n\teval_metrics_dict[\"auc\"] = auroc(acc_metric, \n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])\n\treturn eval_metrics_dict\n# Training function for one step\ndef train_step(model, optimizer, x, y):\n    with tf.GradientTape() as tape:\n        logits, zpred = model(x)\n        # y_pred: predicted probability",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "\teval_metrics_dict[\"auc\"]",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "\teval_metrics_dict[\"auc\"] = auroc(acc_metric, \n\t\tlabels=y_main, predictions=predictions[\"probabilities\"])\n\treturn eval_metrics_dict\n# Training function for one step\ndef train_step(model, optimizer, x, y):\n    with tf.GradientTape() as tape:\n        logits, zpred = model(x)\n        # y_pred: predicted probability\n        ypred = tf.nn.sigmoid(logits)\n        predictions = {",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "current_time",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntrain_log_dir = 'logs/gradient_tape/' + current_time + '/train'\neval_log_dir = 'logs/gradient_tape/' + current_time + '/eval'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\neval_summary_writer = tf.summary.create_file_writer(eval_log_dir)\ndef train_eval(params):\n    optimizer = tf.keras.optimizers.Adam(learning_rate=params['lr'])\n    model = PretrainedResNet50(embedding_dim=params['embedding_dim'], l2_penalty=params['l2_penalty'])\n    for epoch in range(params['num_epochs']):\n        print(f\"\\nTraining epoch {epoch}\")",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "train_log_dir",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\neval_log_dir = 'logs/gradient_tape/' + current_time + '/eval'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\neval_summary_writer = tf.summary.create_file_writer(eval_log_dir)\ndef train_eval(params):\n    optimizer = tf.keras.optimizers.Adam(learning_rate=params['lr'])\n    model = PretrainedResNet50(embedding_dim=params['embedding_dim'], l2_penalty=params['l2_penalty'])\n    for epoch in range(params['num_epochs']):\n        print(f\"\\nTraining epoch {epoch}\")\n        for step, (x, y) in enumerate(train_ds):",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "eval_log_dir",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "eval_log_dir = 'logs/gradient_tape/' + current_time + '/eval'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\neval_summary_writer = tf.summary.create_file_writer(eval_log_dir)\ndef train_eval(params):\n    optimizer = tf.keras.optimizers.Adam(learning_rate=params['lr'])\n    model = PretrainedResNet50(embedding_dim=params['embedding_dim'], l2_penalty=params['l2_penalty'])\n    for epoch in range(params['num_epochs']):\n        print(f\"\\nTraining epoch {epoch}\")\n        for step, (x, y) in enumerate(train_ds):\n            train_step(model, optimizer, x, y)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "train_summary_writer",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\neval_summary_writer = tf.summary.create_file_writer(eval_log_dir)\ndef train_eval(params):\n    optimizer = tf.keras.optimizers.Adam(learning_rate=params['lr'])\n    model = PretrainedResNet50(embedding_dim=params['embedding_dim'], l2_penalty=params['l2_penalty'])\n    for epoch in range(params['num_epochs']):\n        print(f\"\\nTraining epoch {epoch}\")\n        for step, (x, y) in enumerate(train_ds):\n            train_step(model, optimizer, x, y)\n        with train_summary_writer.as_default():",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "eval_summary_writer",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "eval_summary_writer = tf.summary.create_file_writer(eval_log_dir)\ndef train_eval(params):\n    optimizer = tf.keras.optimizers.Adam(learning_rate=params['lr'])\n    model = PretrainedResNet50(embedding_dim=params['embedding_dim'], l2_penalty=params['l2_penalty'])\n    for epoch in range(params['num_epochs']):\n        print(f\"\\nTraining epoch {epoch}\")\n        for step, (x, y) in enumerate(train_ds):\n            train_step(model, optimizer, x, y)\n        with train_summary_writer.as_default():\n            tf.summary.scalar('loss', train_loss.result(), step=epoch)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "params",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "params = {}\nparams['embedding_dim'] = 1000\nparams['l2_penalty'] = 0.0\nparams['num_epochs'] = 20\nparams['alpha'] = 1.0  # parameter for HSIC loss\nparams['batch_size'] = 64\nparams['sigma'] = 1.0\nparams['lr'] = 1e-5\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR+'data')",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "params['embedding_dim']",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "params['embedding_dim'] = 1000\nparams['l2_penalty'] = 0.0\nparams['num_epochs'] = 20\nparams['alpha'] = 1.0  # parameter for HSIC loss\nparams['batch_size'] = 64\nparams['sigma'] = 1.0\nparams['lr'] = 1e-5\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR+'data')\n# print(\"Finish creating the dataset csv\")",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "params['l2_penalty']",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "params['l2_penalty'] = 0.0\nparams['num_epochs'] = 20\nparams['alpha'] = 1.0  # parameter for HSIC loss\nparams['batch_size'] = 64\nparams['sigma'] = 1.0\nparams['lr'] = 1e-5\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR+'data')\n# print(\"Finish creating the dataset csv\")\n# STEP 2: Create training, validation and testing dataset",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "params['num_epochs']",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "params['num_epochs'] = 20\nparams['alpha'] = 1.0  # parameter for HSIC loss\nparams['batch_size'] = 64\nparams['sigma'] = 1.0\nparams['lr'] = 1e-5\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR+'data')\n# print(\"Finish creating the dataset csv\")\n# STEP 2: Create training, validation and testing dataset\ndata_dir = MAIN_DIR + 'data'",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "params['alpha']",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "params['alpha'] = 1.0  # parameter for HSIC loss\nparams['batch_size'] = 64\nparams['sigma'] = 1.0\nparams['lr'] = 1e-5\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR+'data')\n# print(\"Finish creating the dataset csv\")\n# STEP 2: Create training, validation and testing dataset\ndata_dir = MAIN_DIR + 'data'\ntrain_ds, valid_ds, test_ds_dict = load_created_data(data_dir, True, params)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "params['batch_size']",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "params['batch_size'] = 64\nparams['sigma'] = 1.0\nparams['lr'] = 1e-5\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR+'data')\n# print(\"Finish creating the dataset csv\")\n# STEP 2: Create training, validation and testing dataset\ndata_dir = MAIN_DIR + 'data'\ntrain_ds, valid_ds, test_ds_dict = load_created_data(data_dir, True, params)\n# train_ds = create_dataset(MAIN_DIR + 'data/unskew_train.csv', params)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "params['sigma']",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "params['sigma'] = 1.0\nparams['lr'] = 1e-5\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR+'data')\n# print(\"Finish creating the dataset csv\")\n# STEP 2: Create training, validation and testing dataset\ndata_dir = MAIN_DIR + 'data'\ntrain_ds, valid_ds, test_ds_dict = load_created_data(data_dir, True, params)\n# train_ds = create_dataset(MAIN_DIR + 'data/unskew_train.csv', params)\n# valid_ds = create_dataset(MAIN_DIR + 'data/unskew_valid.csv', params)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "params['lr']",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "params['lr'] = 1e-5\n# STEP 1: Create the dataset csv\n# create_save_chexpert_lists(MAIN_DIR+'data')\n# print(\"Finish creating the dataset csv\")\n# STEP 2: Create training, validation and testing dataset\ndata_dir = MAIN_DIR + 'data'\ntrain_ds, valid_ds, test_ds_dict = load_created_data(data_dir, True, params)\n# train_ds = create_dataset(MAIN_DIR + 'data/unskew_train.csv', params)\n# valid_ds = create_dataset(MAIN_DIR + 'data/unskew_valid.csv', params)\nprint(\"Finish creating the training, validation, and testing dataset\")",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "data_dir",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "data_dir = MAIN_DIR + 'data'\ntrain_ds, valid_ds, test_ds_dict = load_created_data(data_dir, True, params)\n# train_ds = create_dataset(MAIN_DIR + 'data/unskew_train.csv', params)\n# valid_ds = create_dataset(MAIN_DIR + 'data/unskew_valid.csv', params)\nprint(\"Finish creating the training, validation, and testing dataset\")\n# # STEP 3: Training and evaluating\nmodel = train_eval(params)\n# # STEP 4: Testing\n# model_dir = MAIN_DIR + 'model.h5'\ntest(model, params)",
        "detail": "main_aux",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "main_aux",
        "description": "main_aux",
        "peekOfCode": "model = train_eval(params)\n# # STEP 4: Testing\n# model_dir = MAIN_DIR + 'model.h5'\ntest(model, params)",
        "detail": "main_aux",
        "documentation": {}
    }
]